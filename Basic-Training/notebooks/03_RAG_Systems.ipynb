{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Building RAG Systems\n",
    "## Retrieval-Augmented Generation\n",
    "\n",
    "**Duration**: 90 minutes\n",
    "\n",
    "In this session, you'll learn how to build a complete RAG (Retrieval-Augmented Generation) system that allows LLMs to answer questions based on your own documents.\n",
    "\n",
    "### What You'll Build:\n",
    "- Document loading and processing pipeline\n",
    "- Intelligent text chunking system\n",
    "- Embeddings generation with Sentence Transformers\n",
    "- Vector database storage with ChromaDB\n",
    "- Semantic search functionality\n",
    "- Complete end-to-end RAG application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q openai\n",
    "!pip install -q langchain langchain-community\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q chromadb\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q numpy\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API keys (from Colab Secrets)\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API keys loaded\")\n",
    "except:\n",
    "    from getpass import getpass\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter OpenAI API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Sample Documents\n",
    "\n",
    "We'll create a small knowledge base of company policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Sample documents (company knowledge base)\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "Product Return Policy\n",
    "\n",
    "Our company offers a 30-day return policy for all products.\n",
    "To be eligible for a return, items must be unused and in their\n",
    "original packaging. Customers can initiate a return by contacting\n",
    "customer service at support@example.com or calling 1-800-RETURNS.\n",
    "\n",
    "Refunds are processed within 5-7 business days after we receive\n",
    "the returned item. The refund will be issued to the original\n",
    "payment method. Shipping costs are non-refundable unless the\n",
    "return is due to our error.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"return_policy.txt\", \"department\": \"customer_service\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "Shipping Information\n",
    "\n",
    "We offer three shipping options:\n",
    "- Standard Shipping: 5-7 business days ($5.99)\n",
    "- Express Shipping: 2-3 business days ($12.99)\n",
    "- Overnight Shipping: 1 business day ($24.99)\n",
    "\n",
    "All orders over $50 qualify for free standard shipping.\n",
    "International shipping is available to select countries.\n",
    "Tracking information is provided via email once the order ships.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"shipping_info.txt\", \"department\": \"logistics\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "Customer Support Hours\n",
    "\n",
    "Our customer support team is available:\n",
    "- Monday to Friday: 9 AM - 9 PM EST\n",
    "- Saturday: 10 AM - 6 PM EST\n",
    "- Sunday: 12 PM - 5 PM EST\n",
    "\n",
    "Contact methods:\n",
    "- Phone: 1-800-SUPPORT\n",
    "- Email: support@example.com\n",
    "- Live Chat: Available on our website during business hours\n",
    "\n",
    "Average response time for emails is 24 hours on business days.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"support_hours.txt\", \"department\": \"customer_service\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "Product Warranty\n",
    "\n",
    "All products come with a standard 1-year manufacturer's warranty\n",
    "covering defects in materials and workmanship. Extended warranty\n",
    "plans are available for purchase at checkout.\n",
    "\n",
    "Warranty claims can be submitted through our website or by\n",
    "contacting customer service. Proof of purchase is required for\n",
    "all warranty claims. The warranty does not cover damage from\n",
    "misuse, accidents, or normal wear and tear.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"warranty_info.txt\", \"department\": \"product\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} sample documents\")\n",
    "print(\"\\nDocument sources:\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc['metadata']['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Chunking\n",
    "\n",
    "We'll split documents into smaller chunks for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(documents, chunk_size=400, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into chunks using intelligent text splitter\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split on paragraphs, then sentences\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc['content'])\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                'content': chunk,\n",
    "                'metadata': {\n",
    "                    **doc['metadata'],\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(chunks)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Chunk all documents\n",
    "chunked_docs = chunk_documents(documents, chunk_size=400, chunk_overlap=100)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunked_docs)} chunks from {len(documents)} documents\")\n",
    "print(\"\\nChunk distribution:\")\n",
    "\n",
    "for doc in documents:\n",
    "    source = doc['metadata']['source']\n",
    "    count = sum(1 for c in chunked_docs if c['metadata']['source'] == source)\n",
    "    print(f\"  - {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample chunk\n",
    "print(\"Example Chunk:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Source: {chunked_docs[0]['metadata']['source']}\")\n",
    "print(f\"Chunk {chunked_docs[0]['metadata']['chunk_index'] + 1} of {chunked_docs[0]['metadata']['total_chunks']}\")\n",
    "print(\"-\" * 60)\n",
    "print(chunked_docs[0]['content'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Generate Embeddings\n",
    "\n",
    "Convert text chunks into numerical vectors for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"‚úÖ Loaded: {model_name}\")\n",
    "print(f\"   Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "chunk_texts = [chunk['content'] for chunk in chunked_docs]\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = embedding_model.encode(\n",
    "    chunk_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"   Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
    "    emb1 = embedding_model.encode([text1])\n",
    "    emb2 = embedding_model.encode([text2])\n",
    "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Test examples\n",
    "examples = [\n",
    "    (\"What is the return policy?\", \"How do I return a product?\"),\n",
    "    (\"What is the return policy?\", \"What are shipping options?\"),\n",
    "]\n",
    "\n",
    "print(\"Semantic Similarity Examples:\")\n",
    "print(\"-\" * 60)\n",
    "for text1, text2 in examples:\n",
    "    sim = calculate_similarity(text1, text2)\n",
    "    print(f\"Text 1: {text1}\")\n",
    "    print(f\"Text 2: {text2}\")\n",
    "    print(f\"Similarity: {sim:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Store in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"company_knowledge_base\"\n",
    "\n",
    "# Delete if exists (for fresh start)\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\": \"Company documentation for RAG\"}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to collection\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunked_docs))]\n",
    "documents = [chunk['content'] for chunk in chunked_docs]\n",
    "metadatas = [chunk['metadata'] for chunk in chunked_docs]\n",
    "embeddings_list = [emb.tolist() for emb in embeddings]\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=documents,\n",
    "    embeddings=embeddings_list,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Added {collection.count()} documents to ChromaDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_knowledge_base(query, n_results=3):\n",
    "    \"\"\"Search the knowledge base for relevant chunks\"\"\"\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Search vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test query\n",
    "query = \"What is the return policy?\"\n",
    "results = search_knowledge_base(query, n_results=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nTop 3 Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(len(results['documents'][0])):\n",
    "    print(f\"\\n[Result {i+1}]\")\n",
    "    print(f\"Source: {results['metadatas'][0][i]['source']}\")\n",
    "    print(f\"Distance: {results['distances'][0][i]:.4f}\")\n",
    "    print(f\"Content: {results['documents'][0][i][:150]}...\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple queries\n",
    "test_queries = [\n",
    "    \"How long does shipping take?\",\n",
    "    \"What are customer support hours?\",\n",
    "    \"How do warranties work?\",\n",
    "    \"Can I get free shipping?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîç Query: {query}\")\n",
    "    results = search_knowledge_base(query, n_results=2)\n",
    "    \n",
    "    for i in range(len(results['documents'][0])):\n",
    "        print(f\"  ‚úì {results['metadatas'][0][i]['source']}\")\n",
    "        print(f\"    {results['documents'][0][i][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Build Complete RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "def rag_query(question, n_results=3):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline:\n",
    "    1. Retrieve relevant documents\n",
    "    2. Format context\n",
    "    3. Generate answer with LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve\n",
    "    search_results = search_knowledge_base(question, n_results=n_results)\n",
    "    \n",
    "    # Step 2: Format context\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(search_results['documents'][0]):\n",
    "        source = search_results['metadatas'][0][i]['source']\n",
    "        context_parts.append(f\"[Source: {source}]\\n{doc}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    prompt = f\"\"\"Answer the question based on the provided context.\n",
    "If the answer is not in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful customer service assistant. Answer questions based only on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': [meta['source'] for meta in search_results['metadatas'][0]],\n",
    "        'context_chunks': search_results['documents'][0]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ RAG system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system\n",
    "question = \"What is your return policy and how long do I have?\"\n",
    "result = rag_query(question)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nü§ñ Answer:\\n{result['answer']}\")\n",
    "print(f\"\\nüìö Sources: {', '.join(result['sources'])}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more questions\n",
    "questions = [\n",
    "    \"How much does overnight shipping cost?\",\n",
    "    \"What days is customer support available?\",\n",
    "    \"How long is the warranty period?\",\n",
    "    \"What happens if I want to return something after 30 days?\"  # Edge case\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag_query(q)\n",
    "    print(f\"\\n‚ùì {q}\")\n",
    "    print(f\"üí° {result['answer']}\")\n",
    "    print(f\"üìÑ Sources: {', '.join(result['sources'])}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"How long do I have to return a product?\",\n",
    "        \"expected_answer_contains\": [\"30-day\", \"30 day\"],\n",
    "        \"expected_source\": \"return_policy.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the customer service hours on Saturday?\",\n",
    "        \"expected_answer_contains\": [\"10 AM\", \"6 PM\"],\n",
    "        \"expected_source\": \"support_hours.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much is overnight shipping?\",\n",
    "        \"expected_answer_contains\": [\"24.99\", \"$24.99\"],\n",
    "        \"expected_source\": \"shipping_info.txt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "for test in test_cases:\n",
    "    result = rag_query(test['question'])\n",
    "    \n",
    "    # Check answer correctness\n",
    "    answer_correct = any(\n",
    "        exp.lower() in result['answer'].lower()\n",
    "        for exp in test['expected_answer_contains']\n",
    "    )\n",
    "    \n",
    "    # Check source correctness\n",
    "    source_correct = test['expected_source'] in result['sources']\n",
    "    \n",
    "    results.append({\n",
    "        'question': test['question'],\n",
    "        'answer_correct': answer_correct,\n",
    "        'source_correct': source_correct\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "answer_accuracy = sum(r['answer_correct'] for r in results) / len(results)\n",
    "source_accuracy = sum(r['source_correct'] for r in results) / len(results)\n",
    "\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Answer Accuracy: {answer_accuracy:.1%}\")\n",
    "print(f\"Source Accuracy: {source_accuracy:.1%}\\n\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Test {i+1}: {result['question']}\")\n",
    "    print(f\"  Answer: {'‚úÖ' if result['answer_correct'] else '‚ùå'}\")\n",
    "    print(f\"  Source: {'‚úÖ' if result['source_correct'] else '‚ùå'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Complete! ‚úÖ\n",
    "\n",
    "You've successfully built a complete RAG system!\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ Document processing and chunking\n",
    "- ‚úÖ Generating embeddings with Sentence Transformers\n",
    "- ‚úÖ Storing vectors in ChromaDB\n",
    "- ‚úÖ Semantic search and retrieval\n",
    "- ‚úÖ Building end-to-end RAG pipeline\n",
    "- ‚úÖ Evaluating RAG performance\n",
    "\n",
    "### Next Steps:\n",
    "1. Try with your own documents\n",
    "2. Experiment with different chunk sizes\n",
    "3. Test different embedding models\n",
    "4. Implement reranking\n",
    "\n",
    "**Next Session**: Function Calling & Tool Use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
