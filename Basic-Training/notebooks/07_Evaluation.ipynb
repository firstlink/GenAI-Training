{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7: Evaluation & Testing\n",
    "\n",
    "**Duration**: 75 minutes  \n",
    "**Difficulty**: Intermediate\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- üéØ Create test datasets\n",
    "- üéØ Define quality metrics\n",
    "- üéØ Build evaluation frameworks\n",
    "- üéØ Implement automated testing\n",
    "- üéØ A/B test different approaches\n",
    "- üéØ Set up monitoring dashboards\n",
    "\n",
    "## üìö Why Evaluate?\n",
    "\n",
    "**Problems without evaluation**:\n",
    "- Don't know if system is improving\n",
    "- Can't compare approaches\n",
    "- Don't catch regressions\n",
    "- No data for decisions\n",
    "\n",
    "**Benefits of evaluation**:\n",
    "- Measure quality objectively\n",
    "- Track improvements over time\n",
    "- Compare models/prompts\n",
    "- Catch bugs early"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv pandas matplotlib -q\n",
    "\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set up API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    from getpass import getpass\n",
    "    if 'OPENAI_API_KEY' not in os.environ:\n",
    "        os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
    "    print(\"‚úÖ API key loaded\")\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"\\nüöÄ Ready to build evaluation systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating Test Datasets\n",
    "\n",
    "A good test dataset is the foundation of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a golden test dataset\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"category\": \"order_status\",\n",
    "        \"query\": \"Where is my order ORD-12345?\",\n",
    "        \"expected_function\": \"get_order_status\",\n",
    "        \"expected_args\": {\"order_id\": \"ORD-12345\"},\n",
    "        \"expected_contains\": [\"shipped\", \"tracking\"],\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"category\": \"refund\",\n",
    "        \"query\": \"I want a refund for my broken product\",\n",
    "        \"expected_function\": \"create_support_ticket\",\n",
    "        \"expected_contains\": [\"ticket\", \"refund\"],\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"category\": \"policy\",\n",
    "        \"query\": \"What is your return policy?\",\n",
    "        \"expected_function\": \"search_knowledge_base\",\n",
    "        \"expected_contains\": [\"30 days\", \"return\"],\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"category\": \"technical\",\n",
    "        \"query\": \"My laptop won't turn on and the light is blinking\",\n",
    "        \"expected_function\": \"create_support_ticket\",\n",
    "        \"expected_contains\": [\"troubleshooting\", \"technical\"],\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"category\": \"account\",\n",
    "        \"query\": \"Check my rewards points for john@email.com\",\n",
    "        \"expected_function\": \"check_account_info\",\n",
    "        \"expected_args\": {\"customer_email\": \"john@email.com\"},\n",
    "        \"expected_contains\": [\"points\", \"account\"],\n",
    "        \"difficulty\": \"easy\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created test dataset with {len(test_cases)} test cases\\n\")\n",
    "\n",
    "# Display test cases\n",
    "df = pd.DataFrame(test_cases)\n",
    "print(df[['id', 'category', 'query', 'difficulty']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Automated Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple mock system for testing\n",
    "\n",
    "def mock_rag_system(query: str) -> str:\n",
    "    \"\"\"Mock RAG system for testing\"\"\"\n",
    "    \n",
    "    # Simple keyword-based responses\n",
    "    if \"return policy\" in query.lower():\n",
    "        return \"Our return policy allows returns within 30 days of purchase.\"\n",
    "    elif \"order\" in query.lower() and \"ORD\" in query:\n",
    "        return \"Your order has been shipped. Tracking number: 1Z999AA.\"\n",
    "    elif \"refund\" in query.lower():\n",
    "        return \"We've created ticket TKT-12345 for your refund request.\"\n",
    "    elif \"laptop\" in query.lower() and \"won't turn on\" in query.lower():\n",
    "        return \"Let's troubleshoot your laptop. Try holding the power button for 10 seconds.\"\n",
    "    elif \"rewards\" in query.lower() or \"points\" in query.lower():\n",
    "        return \"Your account has 450 rewards points.\"\n",
    "    else:\n",
    "        return \"I'm not sure how to help with that. Let me connect you with support.\"\n",
    "\n",
    "# Test it\n",
    "print(\"Testing mock system:\\n\")\n",
    "test_query = \"What is your return policy?\"\n",
    "response = mock_rag_system(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system quality\"\"\"\n",
    "    \n",
    "    def __init__(self, system):\n",
    "        self.system = system\n",
    "    \n",
    "    def evaluate_responses(self, test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"Evaluate system responses against test cases\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for test in test_cases:\n",
    "            # Get system response\n",
    "            response = self.system(test['query'])\n",
    "            \n",
    "            # Check if expected keywords are in response\n",
    "            contains_expected = all(\n",
    "                keyword.lower() in response.lower()\n",
    "                for keyword in test.get('expected_contains', [])\n",
    "            )\n",
    "            \n",
    "            # Calculate score\n",
    "            score = 1.0 if contains_expected else 0.0\n",
    "            \n",
    "            results.append({\n",
    "                \"test_id\": test['id'],\n",
    "                \"query\": test['query'],\n",
    "                \"response\": response,\n",
    "                \"contains_expected\": contains_expected,\n",
    "                \"score\": score,\n",
    "                \"category\": test['category'],\n",
    "                \"difficulty\": test['difficulty']\n",
    "            })\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        total_score = sum(r['score'] for r in results)\n",
    "        accuracy = total_score / len(results) if results else 0\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"total_tests\": len(results),\n",
    "            \"passed\": int(total_score),\n",
    "            \"failed\": len(results) - int(total_score),\n",
    "            \"details\": results\n",
    "        }\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = RAGEvaluator(mock_rag_system)\n",
    "eval_results = evaluator.evaluate_responses(test_cases)\n",
    "\n",
    "print(\"üìä Evaluation Results:\\n\")\n",
    "print(f\"Accuracy: {eval_results['accuracy']:.1%}\")\n",
    "print(f\"Passed: {eval_results['passed']}/{eval_results['total_tests']}\")\n",
    "print(f\"Failed: {eval_results['failed']}/{eval_results['total_tests']}\")\n",
    "\n",
    "# Show failures\n",
    "print(\"\\n‚ùå Failed Tests:\")\n",
    "for result in eval_results['details']:\n",
    "    if result['score'] == 0:\n",
    "        print(f\"  - Test {result['test_id']}: {result['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LLM-as-Judge\n",
    "\n",
    "Use LLMs to evaluate other LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_evaluate(query: str, response: str, expected_elements: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Use GPT-4 to evaluate a response\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = f\"\"\"Evaluate this customer support response.\n",
    "\n",
    "Query: {query}\n",
    "Response: {response}\n",
    "Expected elements: {', '.join(expected_elements)}\n",
    "\n",
    "Rate from 1-5 on:\n",
    "1. Accuracy (Does it answer correctly?)\n",
    "2. Helpfulness (Is it useful to the customer?)\n",
    "3. Tone (Is it professional and empathetic?)\n",
    "4. Completeness (Does it address all aspects?)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"accuracy\": 1-5,\n",
    "  \"helpfulness\": 1-5,\n",
    "  \"tone\": 1-5,\n",
    "  \"completeness\": 1-5,\n",
    "  \"overall_score\": 1-5,\n",
    "  \"reasoning\": \"brief explanation\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    return json.loads(result.choices[0].message.content)\n",
    "\n",
    "# Test LLM-as-judge\n",
    "test_case = test_cases[0]\n",
    "response = mock_rag_system(test_case['query'])\n",
    "\n",
    "print(f\"Query: {test_case['query']}\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "evaluation = llm_evaluate(\n",
    "    test_case['query'],\n",
    "    response,\n",
    "    test_case['expected_contains']\n",
    ")\n",
    "\n",
    "print(\"ü§ñ LLM Evaluation:\")\n",
    "print(json.dumps(evaluation, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM-as-judge on all test cases\n",
    "\n",
    "print(\"Running LLM-as-Judge evaluation...\\n\")\n",
    "\n",
    "llm_eval_results = []\n",
    "\n",
    "for test in test_cases:\n",
    "    response = mock_rag_system(test['query'])\n",
    "    \n",
    "    evaluation = llm_evaluate(\n",
    "        test['query'],\n",
    "        response,\n",
    "        test.get('expected_contains', [])\n",
    "    )\n",
    "    \n",
    "    llm_eval_results.append({\n",
    "        \"test_id\": test['id'],\n",
    "        \"category\": test['category'],\n",
    "        **evaluation\n",
    "    })\n",
    "    \n",
    "    print(f\"Test {test['id']}: Overall Score = {evaluation['overall_score']}/5\")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_accuracy = sum(r['accuracy'] for r in llm_eval_results) / len(llm_eval_results)\n",
    "avg_overall = sum(r['overall_score'] for r in llm_eval_results) / len(llm_eval_results)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.1f}/5\")\n",
    "print(f\"Average Overall: {avg_overall:.1f}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsDashboard:\n",
    "    \"\"\"Track system metrics over time\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"total_queries\": 0,\n",
    "            \"successful_queries\": 0,\n",
    "            \"average_latency\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"error_count\": 0\n",
    "        }\n",
    "        self.query_log = []\n",
    "    \n",
    "    def track_query(self, query: str, response: str, latency: float, cost: float, error: str = None):\n",
    "        \"\"\"Track a single query\"\"\"\n",
    "        \n",
    "        self.metrics[\"total_queries\"] += 1\n",
    "        \n",
    "        if not error:\n",
    "            self.metrics[\"successful_queries\"] += 1\n",
    "        else:\n",
    "            self.metrics[\"error_count\"] += 1\n",
    "        \n",
    "        # Update averages\n",
    "        n = self.metrics[\"total_queries\"]\n",
    "        self.metrics[\"average_latency\"] = (\n",
    "            (self.metrics[\"average_latency\"] * (n-1) + latency) / n\n",
    "        )\n",
    "        self.metrics[\"total_cost\"] += cost\n",
    "        \n",
    "        # Log query\n",
    "        self.query_log.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"latency\": latency,\n",
    "            \"cost\": cost,\n",
    "            \"error\": error,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get metrics summary\"\"\"\n",
    "        total = self.metrics[\"total_queries\"]\n",
    "        \n",
    "        return {\n",
    "            \"success_rate\": self.metrics[\"successful_queries\"] / total if total > 0 else 0,\n",
    "            \"avg_latency_ms\": self.metrics[\"average_latency\"] * 1000,\n",
    "            \"total_cost_usd\": self.metrics[\"total_cost\"],\n",
    "            \"error_rate\": self.metrics[\"error_count\"] / total if total > 0 else 0,\n",
    "            \"total_queries\": total\n",
    "        }\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Visualize metrics\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle('System Metrics Dashboard', fontsize=16)\n",
    "        \n",
    "        # Success rate\n",
    "        axes[0, 0].bar(['Success', 'Error'], \n",
    "                       [summary['success_rate'], summary['error_rate']])\n",
    "        axes[0, 0].set_title('Success Rate')\n",
    "        axes[0, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # Latency\n",
    "        axes[0, 1].bar(['Avg Latency'], [summary['avg_latency_ms']])\n",
    "        axes[0, 1].set_title('Average Latency (ms)')\n",
    "        \n",
    "        # Cost\n",
    "        axes[1, 0].bar(['Total Cost'], [summary['total_cost_usd']])\n",
    "        axes[1, 0].set_title('Total Cost (USD)')\n",
    "        \n",
    "        # Query count\n",
    "        axes[1, 1].bar(['Total Queries'], [summary['total_queries']])\n",
    "        axes[1, 1].set_title('Total Queries')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test metrics dashboard\n",
    "dashboard = MetricsDashboard()\n",
    "\n",
    "# Simulate some queries\n",
    "for test in test_cases:\n",
    "    start = time.time()\n",
    "    response = mock_rag_system(test['query'])\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    dashboard.track_query(\n",
    "        query=test['query'],\n",
    "        response=response,\n",
    "        latency=latency,\n",
    "        cost=0.002  # Mock cost\n",
    "    )\n",
    "\n",
    "# Display summary\n",
    "summary = dashboard.get_summary()\n",
    "print(\"üìä Metrics Summary:\\n\")\n",
    "print(f\"Success Rate: {summary['success_rate']:.1%}\")\n",
    "print(f\"Average Latency: {summary['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"Total Cost: ${summary['total_cost_usd']:.4f}\")\n",
    "print(f\"Error Rate: {summary['error_rate']:.1%}\")\n",
    "print(f\"Total Queries: {summary['total_queries']}\")\n",
    "\n",
    "# Plot\n",
    "dashboard.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTest:\n",
    "    \"\"\"A/B test two different systems\"\"\"\n",
    "    \n",
    "    def __init__(self, version_a, version_b):\n",
    "        self.version_a = version_a\n",
    "        self.version_b = version_b\n",
    "        self.results = {\"a\": [], \"b\": []}\n",
    "    \n",
    "    def run_test(self, test_cases: List[Dict], evaluator):\n",
    "        \"\"\"Run A/B test\"\"\"\n",
    "        \n",
    "        for test in test_cases:\n",
    "            # Randomly assign to A or B\n",
    "            version = random.choice([\"a\", \"b\"])\n",
    "            \n",
    "            if version == \"a\":\n",
    "                response = self.version_a(test['query'])\n",
    "            else:\n",
    "                response = self.version_b(test['query'])\n",
    "            \n",
    "            # Evaluate response\n",
    "            score = evaluator(test, response)\n",
    "            \n",
    "            self.results[version].append({\n",
    "                \"test_id\": test['id'],\n",
    "                \"score\": score,\n",
    "                \"query\": test['query']\n",
    "            })\n",
    "    \n",
    "    def analyze(self) -> Dict:\n",
    "        \"\"\"Analyze A/B test results\"\"\"\n",
    "        \n",
    "        avg_a = sum(r['score'] for r in self.results[\"a\"]) / len(self.results[\"a\"]) if self.results[\"a\"] else 0\n",
    "        avg_b = sum(r['score'] for r in self.results[\"b\"]) / len(self.results[\"b\"]) if self.results[\"b\"] else 0\n",
    "        \n",
    "        return {\n",
    "            \"version_a_score\": avg_a,\n",
    "            \"version_b_score\": avg_b,\n",
    "            \"version_a_count\": len(self.results[\"a\"]),\n",
    "            \"version_b_count\": len(self.results[\"b\"]),\n",
    "            \"winner\": \"A\" if avg_a > avg_b else \"B\" if avg_b > avg_a else \"Tie\",\n",
    "            \"improvement\": abs(avg_a - avg_b)\n",
    "        }\n",
    "\n",
    "# Create two versions\n",
    "def version_a(query: str) -> str:\n",
    "    \"\"\"Original version\"\"\"\n",
    "    return mock_rag_system(query)\n",
    "\n",
    "def version_b(query: str) -> str:\n",
    "    \"\"\"Improved version with more empathy\"\"\"\n",
    "    response = mock_rag_system(query)\n",
    "    return f\"Thank you for reaching out! {response} Is there anything else I can help you with?\"\n",
    "\n",
    "# Simple evaluator\n",
    "def simple_evaluator(test: Dict, response: str) -> float:\n",
    "    \"\"\"Simple keyword-based scoring\"\"\"\n",
    "    score = 0.0\n",
    "    for keyword in test.get('expected_contains', []):\n",
    "        if keyword.lower() in response.lower():\n",
    "            score += 1.0\n",
    "    return score / len(test.get('expected_contains', [1]))\n",
    "\n",
    "# Run A/B test\n",
    "ab_test = ABTest(version_a, version_b)\n",
    "ab_test.run_test(test_cases, simple_evaluator)\n",
    "\n",
    "# Analyze results\n",
    "results = ab_test.analyze()\n",
    "\n",
    "print(\"üî¨ A/B Test Results:\\n\")\n",
    "print(f\"Version A Score: {results['version_a_score']:.2f} ({results['version_a_count']} tests)\")\n",
    "print(f\"Version B Score: {results['version_b_score']:.2f} ({results['version_b_count']} tests)\")\n",
    "print(f\"\\nüèÜ Winner: Version {results['winner']}\")\n",
    "print(f\"Improvement: {results['improvement']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Expand Test Dataset\n",
    "\n",
    "Create 20 more test cases covering edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "expanded_test_cases = [\n",
    "    # TODO: Add 20 more test cases\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Response Time Monitoring\n",
    "\n",
    "Track and visualize response times over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def track_response_times():\n",
    "    \"\"\"Track and plot response times\"\"\"\n",
    "    # TODO: Implement response time tracking\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Build Regression Test Suite\n",
    "\n",
    "Create tests that run automatically to catch regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "class RegressionTestSuite:\n",
    "    \"\"\"Automated regression testing\"\"\"\n",
    "    # TODO: Implement regression testing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Session 7 Complete!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "‚úÖ Evaluation enables improvement  \n",
    "‚úÖ Create comprehensive test datasets  \n",
    "‚úÖ Use multiple metrics  \n",
    "‚úÖ Automate evaluation  \n",
    "‚úÖ A/B test changes  \n",
    "‚úÖ LLM-as-judge for quality assessment  \n",
    "‚úÖ Metrics dashboards for monitoring\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In **Session 8: Production Deployment**, you'll learn:\n",
    "- FastAPI application structure\n",
    "- Redis caching\n",
    "- Monitoring and logging\n",
    "- Security and rate limiting\n",
    "- Docker deployment\n",
    "- Cloud deployment options\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to**: [Session 8: Production ‚Üí](08_Production.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
