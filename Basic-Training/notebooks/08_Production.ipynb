{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8: Production Deployment\n",
    "\n",
    "**Duration**: 90 minutes  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- üéØ Build production-ready FastAPI application\n",
    "- üéØ Implement caching with Redis\n",
    "- üéØ Add monitoring and logging\n",
    "- üéØ Set up security and guardrails\n",
    "- üéØ Deploy to cloud (Railway/Render)\n",
    "- üéØ Optimize performance and costs\n",
    "\n",
    "## üìö What You'll Build\n",
    "\n",
    "**SupportGenie v1.0 - Production Ready!**\n",
    "\n",
    "Final version with:\n",
    "- FastAPI REST API\n",
    "- Redis caching\n",
    "- Monitoring dashboard\n",
    "- Rate limiting\n",
    "- Security features\n",
    "- Cloud deployment\n",
    "- **PRODUCTION READY!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "\n",
    "**Note**: This notebook demonstrates production concepts. Some features (like Redis) require installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install fastapi uvicorn pydantic python-dotenv -q\n",
    "\n",
    "print(\"‚úÖ Packages installed!\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: Redis caching requires Redis server (see documentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, List\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    from getpass import getpass\n",
    "    if 'OPENAI_API_KEY' not in os.environ:\n",
    "        os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
    "    print(\"‚úÖ API key loaded\")\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"\\nüöÄ Ready to build production API!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: FastAPI Application Structure\n",
    "\n",
    "Build a production-ready REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request/response models\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    \"\"\"Request model for chat endpoint\"\"\"\n",
    "    message: str\n",
    "    customer_id: Optional[str] = None\n",
    "    session_id: Optional[str] = None\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"message\": \"What's the status of order ORD-12345?\",\n",
    "                \"customer_id\": \"CUST-001\",\n",
    "                \"session_id\": \"sess_abc123\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    \"\"\"Response model for chat endpoint\"\"\"\n",
    "    response: str\n",
    "    sources: List[str] = []\n",
    "    confidence: float = 1.0\n",
    "    processing_time_ms: float\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"response\": \"Your order ORD-12345 has been shipped.\",\n",
    "                \"sources\": [\"order_database\"],\n",
    "                \"confidence\": 0.95,\n",
    "                \"processing_time_ms\": 245.3\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Request/Response models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI app\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"SupportGenie API\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Production AI Customer Support API\"\n",
    ")\n",
    "\n",
    "# Mock agent for demonstration\n",
    "class MockSupportAgent:\n",
    "    \"\"\"Mock agent for demo purposes\"\"\"\n",
    "    \n",
    "    def handle_query(self, query: str, customer_id: str = None) -> dict:\n",
    "        \"\"\"Process query\"\"\"\n",
    "        # Simple mock response\n",
    "        return {\n",
    "            \"answer\": f\"I understand you're asking about: {query}. Let me help you with that.\",\n",
    "            \"sources\": [\"knowledge_base\"],\n",
    "            \"confidence\": 0.85\n",
    "        }\n",
    "\n",
    "# Initialize agent\n",
    "agent = MockSupportAgent()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint\"\"\"\n",
    "    return {\n",
    "        \"message\": \"SupportGenie API v1.0\",\n",
    "        \"status\": \"operational\",\n",
    "        \"endpoints\": [\"/chat\", \"/health\", \"/metrics\"]\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.post(\"/chat\", response_model=QueryResponse)\n",
    "async def chat(request: QueryRequest):\n",
    "    \"\"\"Main chat endpoint\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process query\n",
    "        response = agent.handle_query(\n",
    "            query=request.message,\n",
    "            customer_id=request.customer_id\n",
    "        )\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return QueryResponse(\n",
    "            response=response['answer'],\n",
    "            sources=response.get('sources', []),\n",
    "            confidence=response.get('confidence', 1.0),\n",
    "            processing_time_ms=processing_time\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print(\"‚úÖ FastAPI app created\")\n",
    "print(\"\\nüìù To run the server:\")\n",
    "print(\"   uvicorn main:app --host 0.0.0.0 --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Caching with Redis (Mock)\n",
    "\n",
    "Implement response caching to reduce costs and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockCache:\n",
    "    \"\"\"Mock cache for demonstration (use Redis in production)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.ttl = 3600  # 1 hour\n",
    "    \n",
    "    def get_cache_key(self, query: str, customer_id: str = None) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        data = f\"{query}:{customer_id}\"\n",
    "        return hashlib.md5(data.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, query: str, customer_id: str = None) -> Optional[dict]:\n",
    "        \"\"\"Get cached response\"\"\"\n",
    "        key = self.get_cache_key(query, customer_id)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            \n",
    "            # Check if expired\n",
    "            if time.time() - entry['timestamp'] < self.ttl:\n",
    "                print(f\"  ‚úÖ Cache HIT for: {query[:50]}...\")\n",
    "                return entry['data']\n",
    "            else:\n",
    "                # Expired\n",
    "                del self.cache[key]\n",
    "        \n",
    "        print(f\"  ‚ùå Cache MISS for: {query[:50]}...\")\n",
    "        return None\n",
    "    \n",
    "    def set(self, query: str, response: dict, customer_id: str = None):\n",
    "        \"\"\"Cache response\"\"\"\n",
    "        key = self.get_cache_key(query, customer_id)\n",
    "        self.cache[key] = {\n",
    "            'data': response,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        print(f\"  üíæ Cached response for: {query[:50]}...\")\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        print(\"  üóëÔ∏è  Cache cleared\")\n",
    "\n",
    "# Test cache\n",
    "cache = MockCache()\n",
    "\n",
    "# First call - cache miss\n",
    "result = cache.get(\"What's your return policy?\")\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Set cache\n",
    "cache.set(\"What's your return policy?\", {\"answer\": \"30 days return policy\"})\n",
    "\n",
    "# Second call - cache hit\n",
    "result = cache.get(\"What's your return policy?\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logging and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MetricsLogger:\n",
    "    \"\"\"Track API metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "    \n",
    "    def log_request(self, \n",
    "                    query: str, \n",
    "                    response: str, \n",
    "                    latency: float, \n",
    "                    cost: float, \n",
    "                    customer_id: str = None,\n",
    "                    cached: bool = False):\n",
    "        \"\"\"Log API request metrics\"\"\"\n",
    "        \n",
    "        metric = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"customer_id\": customer_id,\n",
    "            \"query_length\": len(query),\n",
    "            \"response_length\": len(response),\n",
    "            \"latency_ms\": latency,\n",
    "            \"cost_usd\": cost,\n",
    "            \"cached\": cached,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        self.metrics.append(metric)\n",
    "        logger.info(f\"Request processed: {json.dumps(metric)}\")\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Get metrics summary\"\"\"\n",
    "        if not self.metrics:\n",
    "            return {}\n",
    "        \n",
    "        total_requests = len(self.metrics)\n",
    "        cached_requests = sum(1 for m in self.metrics if m['cached'])\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": total_requests,\n",
    "            \"cached_requests\": cached_requests,\n",
    "            \"cache_hit_rate\": cached_requests / total_requests,\n",
    "            \"avg_latency_ms\": sum(m['latency_ms'] for m in self.metrics) / total_requests,\n",
    "            \"total_cost_usd\": sum(m['cost_usd'] for m in self.metrics)\n",
    "        }\n",
    "\n",
    "# Test metrics logger\n",
    "metrics = MetricsLogger()\n",
    "\n",
    "# Log some requests\n",
    "metrics.log_request(\n",
    "    query=\"What's the return policy?\",\n",
    "    response=\"30 days return policy\",\n",
    "    latency=245.5,\n",
    "    cost=0.002,\n",
    "    cached=False\n",
    ")\n",
    "\n",
    "metrics.log_request(\n",
    "    query=\"What's the return policy?\",\n",
    "    response=\"30 days return policy\",\n",
    "    latency=5.2,\n",
    "    cost=0.0,\n",
    "    cached=True\n",
    ")\n",
    "\n",
    "# Get summary\n",
    "summary = metrics.get_summary()\n",
    "print(\"\\nüìä Metrics Summary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Security and Input Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityValidator:\n",
    "    \"\"\"Security and input validation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_input(text: str, max_length: int = 1000) -> str:\n",
    "        \"\"\"Sanitize user input\"\"\"\n",
    "        \n",
    "        # Remove leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Limit length\n",
    "        if len(text) > max_length:\n",
    "            raise ValueError(f\"Input too long (max {max_length} characters)\")\n",
    "        \n",
    "        # Check for malicious patterns\n",
    "        dangerous_patterns = [\n",
    "            '<script>',\n",
    "            'javascript:',\n",
    "            'onerror=',\n",
    "            '<?php',\n",
    "            '<iframe>'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for pattern in dangerous_patterns:\n",
    "            if pattern in text_lower:\n",
    "                raise ValueError(f\"Input contains prohibited content: {pattern}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_api_key(api_key: str) -> bool:\n",
    "        \"\"\"Validate API key (mock)\"\"\"\n",
    "        # In production: check against database\n",
    "        valid_keys = [\"test-key-123\", \"prod-key-456\"]\n",
    "        return api_key in valid_keys\n",
    "\n",
    "# Test security validator\n",
    "validator = SecurityValidator()\n",
    "\n",
    "# Test valid input\n",
    "try:\n",
    "    clean_text = validator.sanitize_input(\"What's your return policy?\")\n",
    "    print(f\"‚úÖ Valid input: {clean_text}\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Invalid input: {e}\")\n",
    "\n",
    "# Test malicious input\n",
    "try:\n",
    "    clean_text = validator.sanitize_input(\"<script>alert('xss')</script>\")\n",
    "    print(f\"‚úÖ Valid input: {clean_text}\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Blocked malicious input: {e}\")\n",
    "\n",
    "# Test API key\n",
    "print(f\"\\nValid key: {validator.validate_api_key('test-key-123')}\")\n",
    "print(f\"Invalid key: {validator.validate_api_key('invalid-key')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Rate Limiting (Conceptual)\n",
    "\n",
    "In production, use `slowapi` or similar libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRateLimiter:\n",
    "    \"\"\"Simple in-memory rate limiter\"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests: int = 10, window_seconds: int = 60):\n",
    "        self.max_requests = max_requests\n",
    "        self.window_seconds = window_seconds\n",
    "        self.requests = {}  # user_id -> [timestamps]\n",
    "    \n",
    "    def is_allowed(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if request is allowed\"\"\"\n",
    "        \n",
    "        now = time.time()\n",
    "        \n",
    "        # Get user's request history\n",
    "        if user_id not in self.requests:\n",
    "            self.requests[user_id] = []\n",
    "        \n",
    "        # Remove old requests outside the window\n",
    "        self.requests[user_id] = [\n",
    "            ts for ts in self.requests[user_id]\n",
    "            if now - ts < self.window_seconds\n",
    "        ]\n",
    "        \n",
    "        # Check if under limit\n",
    "        if len(self.requests[user_id]) < self.max_requests:\n",
    "            self.requests[user_id].append(now)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_remaining(self, user_id: str) -> int:\n",
    "        \"\"\"Get remaining requests\"\"\"\n",
    "        if user_id not in self.requests:\n",
    "            return self.max_requests\n",
    "        \n",
    "        now = time.time()\n",
    "        recent = [\n",
    "            ts for ts in self.requests[user_id]\n",
    "            if now - ts < self.window_seconds\n",
    "        ]\n",
    "        \n",
    "        return max(0, self.max_requests - len(recent))\n",
    "\n",
    "# Test rate limiter\n",
    "limiter = SimpleRateLimiter(max_requests=3, window_seconds=60)\n",
    "\n",
    "user_id = \"user-123\"\n",
    "\n",
    "print(\"Testing rate limiter:\\n\")\n",
    "for i in range(5):\n",
    "    allowed = limiter.is_allowed(user_id)\n",
    "    remaining = limiter.get_remaining(user_id)\n",
    "    \n",
    "    status = \"‚úÖ ALLOWED\" if allowed else \"‚ùå BLOCKED\"\n",
    "    print(f\"Request {i+1}: {status} (Remaining: {remaining})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cost Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostOptimizer:\n",
    "    \"\"\"Optimize costs by selecting appropriate models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_costs = {\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},  # per 1K tokens\n",
    "            \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06}\n",
    "        }\n",
    "    \n",
    "    def assess_complexity(self, query: str) -> str:\n",
    "        \"\"\"Assess query complexity\"\"\"\n",
    "        \n",
    "        # Simple heuristics\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Complex queries\n",
    "        complex_keywords = ['explain', 'analyze', 'compare', 'why', 'how does']\n",
    "        if any(kw in query_lower for kw in complex_keywords):\n",
    "            return \"complex\"\n",
    "        \n",
    "        # Moderate queries\n",
    "        moderate_keywords = ['help', 'troubleshoot', 'fix']\n",
    "        if any(kw in query_lower for kw in moderate_keywords):\n",
    "            return \"moderate\"\n",
    "        \n",
    "        # Simple queries\n",
    "        return \"simple\"\n",
    "    \n",
    "    def select_model(self, query: str) -> str:\n",
    "        \"\"\"Select optimal model based on complexity\"\"\"\n",
    "        \n",
    "        complexity = self.assess_complexity(query)\n",
    "        \n",
    "        if complexity == \"simple\":\n",
    "            return \"gpt-3.5-turbo\"  # Cheapest\n",
    "        elif complexity == \"moderate\":\n",
    "            return \"gpt-4-turbo\"    # Balanced\n",
    "        else:\n",
    "            return \"gpt-4\"          # Most capable\n",
    "    \n",
    "    def estimate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Estimate cost for a request\"\"\"\n",
    "        \n",
    "        costs = self.model_costs[model]\n",
    "        \n",
    "        input_cost = (input_tokens / 1000) * costs['input']\n",
    "        output_cost = (output_tokens / 1000) * costs['output']\n",
    "        \n",
    "        return input_cost + output_cost\n",
    "\n",
    "# Test cost optimizer\n",
    "optimizer = CostOptimizer()\n",
    "\n",
    "test_queries = [\n",
    "    \"Where is my order?\",\n",
    "    \"Help me troubleshoot my laptop\",\n",
    "    \"Explain how the warranty works and compare it to the extended warranty\"\n",
    "]\n",
    "\n",
    "print(\"Cost Optimization:\\n\")\n",
    "for query in test_queries:\n",
    "    complexity = optimizer.assess_complexity(query)\n",
    "    model = optimizer.select_model(query)\n",
    "    cost = optimizer.estimate_cost(model, input_tokens=100, output_tokens=200)\n",
    "    \n",
    "    print(f\"Query: {query[:50]}...\")\n",
    "    print(f\"  Complexity: {complexity}\")\n",
    "    print(f\"  Model: {model}\")\n",
    "    print(f\"  Est. Cost: ${cost:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Monitoring Dashboard (Simple HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dashboard_html(metrics_summary: dict) -> str:\n",
    "    \"\"\"Generate simple HTML dashboard\"\"\"\n",
    "    \n",
    "    total_requests = metrics_summary.get('total_requests', 0)\n",
    "    avg_latency = metrics_summary.get('avg_latency_ms', 0)\n",
    "    total_cost = metrics_summary.get('total_cost_usd', 0)\n",
    "    cache_hit_rate = metrics_summary.get('cache_hit_rate', 0)\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>SupportGenie Dashboard</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 40px;\n",
    "                background-color: #f5f5f5;\n",
    "            }}\n",
    "            .dashboard {{\n",
    "                background: white;\n",
    "                padding: 30px;\n",
    "                border-radius: 10px;\n",
    "                box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "            }}\n",
    "            h1 {{\n",
    "                color: #333;\n",
    "            }}\n",
    "            .metric {{\n",
    "                display: inline-block;\n",
    "                margin: 20px;\n",
    "                padding: 20px;\n",
    "                background: #f8f9fa;\n",
    "                border-radius: 5px;\n",
    "                min-width: 200px;\n",
    "            }}\n",
    "            .metric-value {{\n",
    "                font-size: 2em;\n",
    "                font-weight: bold;\n",
    "                color: #007bff;\n",
    "            }}\n",
    "            .metric-label {{\n",
    "                color: #666;\n",
    "                margin-top: 5px;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"dashboard\">\n",
    "            <h1>ü§ñ SupportGenie Metrics Dashboard</h1>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{total_requests}</div>\n",
    "                <div class=\"metric-label\">Total Requests</div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{avg_latency:.1f}ms</div>\n",
    "                <div class=\"metric-label\">Avg Latency</div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">${total_cost:.4f}</div>\n",
    "                <div class=\"metric-label\">Total Cost</div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{cache_hit_rate:.0%}</div>\n",
    "                <div class=\"metric-label\">Cache Hit Rate</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "# Generate dashboard\n",
    "dashboard_html = generate_dashboard_html(metrics.get_summary())\n",
    "\n",
    "# Save to file\n",
    "with open('dashboard.html', 'w') as f:\n",
    "    f.write(dashboard_html)\n",
    "\n",
    "print(\"‚úÖ Dashboard generated: dashboard.html\")\n",
    "print(\"\\nPreview:\")\n",
    "print(dashboard_html[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Deployment Configuration\n",
    "\n",
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_content = \"\"\"\n",
    "# Dockerfile for SupportGenie API\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì¶ Dockerfile:\")\n",
    "print(dockerfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_compose_content = \"\"\"\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "      - REDIS_URL=redis://redis:6379\n",
    "    depends_on:\n",
    "      - redis\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"üê≥ Docker Compose:\")\n",
    "print(docker_compose_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_content = \"\"\"\n",
    "fastapi>=0.104.0\n",
    "uvicorn>=0.24.0\n",
    "pydantic>=2.0.0\n",
    "openai>=1.0.0\n",
    "redis>=5.0.0\n",
    "python-dotenv>=1.0.0\n",
    "slowapi>=0.1.9\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã requirements.txt:\")\n",
    "print(requirements_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Session 8 Complete! üéì COURSE COMPLETE!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "‚úÖ FastAPI for production APIs  \n",
    "‚úÖ Redis for caching  \n",
    "‚úÖ Proper logging and monitoring  \n",
    "‚úÖ Security with authentication  \n",
    "‚úÖ Rate limiting  \n",
    "‚úÖ Docker for containerization  \n",
    "‚úÖ Cost optimization strategies  \n",
    "‚úÖ Production deployment best practices\n",
    "\n",
    "---\n",
    "\n",
    "## üéì CONGRATULATIONS!\n",
    "\n",
    "**You've completed the entire Gen AI Production Course!**\n",
    "\n",
    "### Your Journey:\n",
    "\n",
    "1. ‚úÖ **LLM Fundamentals** - API usage, tokens, costs\n",
    "2. ‚úÖ **Prompt Engineering** - Advanced prompting techniques\n",
    "3. ‚úÖ **RAG Systems** - Document retrieval and generation\n",
    "4. ‚úÖ **Function Calling** - Tool use and actions\n",
    "5. ‚úÖ **AI Agents** - Autonomous systems with memory\n",
    "6. ‚úÖ **Multi-Agent** - Orchestration and specialization\n",
    "7. ‚úÖ **Evaluation** - Testing and quality assurance\n",
    "8. ‚úÖ **Production** - Deployment and optimization\n",
    "\n",
    "### Your Capstone: SupportGenie\n",
    "\n",
    "You built a complete AI customer support platform:\n",
    "- From simple chatbot ‚Üí Production-ready system\n",
    "- Multi-agent architecture\n",
    "- RAG-powered knowledge base\n",
    "- Full evaluation framework\n",
    "- Cloud deployment ready\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Deploy your project** - Put SupportGenie in production\n",
    "2. **Build your portfolio** - Showcase your skills\n",
    "3. **Explore advanced topics** - Fine-tuning, observability\n",
    "4. **Apply to real projects** - You're ready!\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ You're now ready to build production Gen AI applications!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
