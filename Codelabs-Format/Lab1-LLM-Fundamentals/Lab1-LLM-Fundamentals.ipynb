{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: LLM Fundamentals & API Usage\n",
    "\n",
    "## ğŸ¯ What You'll Build\n",
    "\n",
    "**SupportGenie v0.1** - A professional AI chatbot with:\n",
    "- âœ… Multi-provider API support (OpenAI, Claude, Gemini)\n",
    "- âœ… Streaming responses for better UX\n",
    "- âœ… Token tracking and cost calculation\n",
    "- âœ… Professional conversation handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Lab Overview\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Duration** | 60 minutes |\n",
    "| **Difficulty** | Beginner |\n",
    "| **Prerequisites** | Basic Python, API key |\n",
    "| **Environment** | Jupyter Notebook or Python script |\n",
    "\n",
    "### What You'll Learn\n",
    "- ğŸ¯ How Large Language Models work (high-level)\n",
    "- ğŸ¯ Make your first API calls to OpenAI, Claude, and Gemini\n",
    "- ğŸ¯ Understand tokens and how to count them\n",
    "- ğŸ¯ Master key parameters: temperature, max_tokens, top_p\n",
    "- ğŸ¯ Implement streaming responses\n",
    "- ğŸ¯ Calculate and optimize API costs\n",
    "- ğŸ¯ Build your first AI chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Welcome to the Exciting World of AI! ğŸš€\n",
    "\n",
    "Forget simple \"Hello World\" programs. In this lab, you'll build a **production-ready AI chatbot** that can:\n",
    "- Answer questions naturally\n",
    "- Stream responses in real-time\n",
    "- Track costs automatically\n",
    "- Handle errors gracefully\n",
    "\n",
    "**Duration:** 5 minutes\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Large Language Models are transforming every industry:\n",
    "- ğŸ’¼ Customer support automation\n",
    "- ğŸ“Š Data analysis and insights\n",
    "- ğŸ¤– Personal AI assistants\n",
    "- ğŸ“ Content generation\n",
    "\n",
    "By the end of this lab, you'll understand the fundamentals and have a working chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Your Environment\n",
    "\n",
    "**Duration:** 10 minutes\n",
    "\n",
    "### ğŸ› ï¸ Step 1: Install Required Packages\n",
    "\n",
    "Run the cell below to install all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai anthropic google-generativeai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What these do:**\n",
    "- `openai` - Access GPT models\n",
    "- `anthropic` - Access Claude models\n",
    "- `google-generativeai` - Access Gemini models\n",
    "- `tiktoken` - Count tokens accurately\n",
    "- `python-dotenv` - Manage API keys securely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”‘ Step 2: Get Your API Keys\n",
    "\n",
    "You'll need at least ONE of these:\n",
    "\n",
    "#### OpenAI API Key\n",
    "1. Go to [platform.openai.com](https://platform.openai.com)\n",
    "2. Sign up or log in\n",
    "3. Navigate to API Keys\n",
    "4. Create new secret key\n",
    "5. Copy and save it securely\n",
    "\n",
    "#### Anthropic API Key (Claude)\n",
    "1. Go to [console.anthropic.com](https://console.anthropic.com)\n",
    "2. Sign up or log in\n",
    "3. Navigate to API Keys\n",
    "4. Create new key\n",
    "5. Copy and save it securely\n",
    "\n",
    "#### Google API Key (Gemini)\n",
    "1. Go to [makersuite.google.com](https://makersuite.google.com)\n",
    "2. Get API key\n",
    "3. Copy and save it securely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš¨ CRITICAL: Secure Your API Keys\n",
    "\n",
    "**NEVER** hardcode API keys in your code!\n",
    "\n",
    "Create a `.env` file in the same directory:\n",
    "\n",
    "```bash\n",
    "# .env file (DO NOT commit to Git!)\n",
    "OPENAI_API_KEY=sk-your-openai-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "Add to `.gitignore`:\n",
    "```bash\n",
    ".env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Step 3: Verify Installation\n",
    "\n",
    "Run this test to verify everything is set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test setup\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if keys are loaded\n",
    "print(\"âœ… Packages imported successfully!\")\n",
    "print(f\"OpenAI key loaded: {bool(os.getenv('OPENAI_API_KEY'))}\")\n",
    "print(f\"Anthropic key loaded: {bool(os.getenv('ANTHROPIC_API_KEY'))}\")\n",
    "print(f\"Google key loaded: {bool(os.getenv('GOOGLE_API_KEY'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How LLMs Work\n",
    "\n",
    "**Duration:** 10 minutes\n",
    "\n",
    "### What is a Large Language Model?\n",
    "\n",
    "ğŸ§  **Simple Definition:** An LLM is a neural network trained on massive amounts of text to predict the next word in a sequence.\n",
    "\n",
    "Think of it like an **incredibly sophisticated autocomplete**.\n",
    "\n",
    "### The Training Process (Simplified)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 1: PRE-TRAINING                              â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚\n",
    "â”‚  Input: Billions of words from books, websites    â”‚\n",
    "â”‚  Task: Predict the next word                       â”‚\n",
    "â”‚  Result: Model learns grammar, facts, reasoning    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 2: FINE-TUNING                               â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚\n",
    "â”‚  Input: High-quality instruction-response pairs    â”‚\n",
    "ï¿½ï¿½ï¿½  Task: Follow instructions accurately              â”‚\n",
    "â”‚  Result: Model becomes a helpful assistant         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 3: ALIGNMENT (RLHF)                          â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚\n",
    "â”‚  Input: Human feedback on responses                â”‚\n",
    "â”‚  Task: Be helpful, harmless, honest                â”‚\n",
    "â”‚  Result: Safe, reliable AI assistant               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### How Text Generation Works\n",
    "\n",
    "**Example:** User types \"The weather today is\"\n",
    "\n",
    "```python\n",
    "# LLM Process:\n",
    "# 1. Tokenize input\n",
    "tokens = [\"The\", \"weather\", \"today\", \"is\"]\n",
    "\n",
    "# 2. Predict next token probabilities\n",
    "probabilities = {\n",
    "    \"sunny\": 30%,\n",
    "    \"nice\": 25%,\n",
    "    \"cloudy\": 20%,\n",
    "    \"rainy\": 15%,\n",
    "    \"...\": 10%\n",
    "}\n",
    "\n",
    "# 3. Sample based on temperature (more on this later!)\n",
    "# 4. Add chosen token to sequence\n",
    "# 5. Repeat until done\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Key Insight\n",
    "\n",
    "**LLMs don't \"know\" facts** - they predict statistically likely continuations based on their training data.\n",
    "\n",
    "This is why they can:\n",
    "- âœ… Write convincingly\n",
    "- âœ… Solve complex problems\n",
    "- âŒ Sometimes \"hallucinate\" (make up facts)\n",
    "- âŒ Get math wrong occasionally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Tokens\n",
    "\n",
    "**Duration:** 8 minutes\n",
    "\n",
    "### What Are Tokens?\n",
    "\n",
    "**Tokens** are the basic units that LLMs process. They're **not quite words, not quite characters**.\n",
    "\n",
    "ğŸ¯ **Rule of Thumb:**\n",
    "- 1 token â‰ˆ **4 characters** in English\n",
    "- 1 token â‰ˆ **Â¾ of a word**\n",
    "- 100 tokens â‰ˆ **75 words**\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "Text: \"Hello, world!\"\n",
    "Tokens: [\"Hello\", \",\", \" world\", \"!\"]\n",
    "Count: 4 tokens\n",
    "\n",
    "Text: \"ChatGPT is amazing\"\n",
    "Tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" amazing\"]\n",
    "Count: 5 tokens\n",
    "\n",
    "Text: \"OpenAI\"\n",
    "Tokens: [\"Open\", \"AI\"]\n",
    "Count: 2 tokens\n",
    "```\n",
    "\n",
    "### Why Tokens Matter\n",
    "\n",
    "#### 1. **Context Limits**\n",
    "Models have maximum token limits:\n",
    "- GPT-3.5-turbo: 4,096 tokens (4K)\n",
    "- GPT-4: 8,192 tokens (8K)\n",
    "- GPT-4-turbo: 128,000 tokens (128K)\n",
    "- Claude 3: 200,000 tokens (200K)\n",
    "\n",
    "#### 2. **Cost**\n",
    "APIs charge **per token** (input + output)\n",
    "\n",
    "#### 3. **Performance**\n",
    "More tokens = slower response time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Hands-On: Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Count tokens in text for a specific model\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Test it out!\n",
    "examples = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"OpenAI GPT-4 is a large language model\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    token_count = count_tokens(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens: {token_count}\")\n",
    "    print(f\"Characters: {len(text)}\")\n",
    "    print(f\"Ratio: {len(text)/token_count:.2f} chars/token\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Exercise: Count Tokens\n",
    "\n",
    "Count tokens in this sentence:\n",
    "```\n",
    "\"Large Language Models are transforming artificial intelligence applications worldwide.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "sentence = \"Large Language Models are transforming artificial intelligence applications worldwide.\"\n",
    "result = count_tokens(sentence)\n",
    "print(f\"Token count: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your First API Call\n",
    "\n",
    "**Duration:** 10 minutes\n",
    "\n",
    "Let's make your first call to an LLM! We'll start with OpenAI's GPT.\n",
    "\n",
    "### ğŸš€ OpenAI API - Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Make API call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Extract answer\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "# Check token usage\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")\n",
    "print(f\"  - Prompt: {response.usage.prompt_tokens}\")\n",
    "print(f\"  - Completion: {response.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Understanding Message Roles\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a customer support agent for TechStore.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I return a product?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"To return a product, please contact...\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How long does it take?\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "**Role Purposes:**\n",
    "- ğŸ­ **system**: Sets the AI's behavior, tone, and constraints\n",
    "- ğŸ‘¤ **user**: The human's messages\n",
    "- ğŸ¤– **assistant**: The AI's previous responses (for conversation history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Try It: Anthropic Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "\n",
    "client_claude = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "message = client_claude.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=100,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Answer: {message.content[0].text}\")\n",
    "print(f\"\\nTokens used:\")\n",
    "print(f\"  - Input: {message.usage.input_tokens}\")\n",
    "print(f\"  - Output: {message.usage.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Exercise: Ask Both Models\n",
    "\n",
    "Modify the code to ask: \"Explain quantum computing in one sentence\"\n",
    "\n",
    "Run it with both OpenAI and Claude. Compare the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - try asking both models the same question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mastering Parameters\n",
    "\n",
    "**Duration:** 12 minutes\n",
    "\n",
    "Time to learn the \"dials and knobs\" that control LLM behavior!\n",
    "\n",
    "### ğŸŒ¡ï¸ Temperature (0.0 - 2.0)\n",
    "\n",
    "Controls **randomness/creativity** of responses.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Temperature Effect on Token Selection          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                  â”‚\n",
    "â”‚  Original probabilities: [60%, 25%, 10%, 5%]    â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚  Temperature = 0.1 (Sharp):                     â”‚\n",
    "â”‚  â†’ [95%, 4%, 0.8%, 0.2%] â† Heavily favor top   â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚  Temperature = 1.0 (Normal):                    â”‚\n",
    "â”‚  â†’ [60%, 25%, 10%, 5%] â† Unchanged              â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚  Temperature = 2.0 (Flat):                      â”‚\n",
    "â”‚  â†’ [40%, 30%, 20%, 10%] â† More balanced         â”‚\n",
    "â”‚                                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Hands-On: Temperature Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "prompt = \"Name a color\"\n",
    "\n",
    "temperatures = [0.0, 0.7, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print('='*50)\n",
    "\n",
    "    # Run 5 times to see variation\n",
    "    for i in range(5):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(f\"  {i+1}. {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š When to Use Each Temperature\n",
    "\n",
    "| Range | Use Case | Example |\n",
    "|-------|----------|---------||\n",
    "| **0.0** | Deterministic, consistent | Math problems, code generation, JSON parsing |\n",
    "| **0.0-0.3** | Mostly factual | Data extraction, summarization, translation |\n",
    "| **0.3-0.7** | Balanced | Customer support, chatbots, Q&A |\n",
    "| **0.7-1.0** | Creative but coherent | Content writing, email drafting |\n",
    "| **1.0-1.5** | High creativity | Creative writing, brainstorming, marketing |\n",
    "| **1.5-2.0** | Maximum creativity | Experimental, poetry (may lose coherence) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ max_tokens\n",
    "\n",
    "Limits the **maximum response length** (output only).\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Input tokens + max_tokens â‰¤ Context Window\n",
    "\n",
    "Example:\n",
    "- Prompt: 500 tokens\n",
    "- max_tokens: 1000\n",
    "- Total needed: 1500 tokens\n",
    "- GPT-3.5-turbo (4K): âœ… Fits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Hands-On: max_tokens Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\n\n# Initialize client\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\nresponse_lengths = [50, 200, 500]\n\nfor max_tok in response_lengths:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n        max_tokens=max_tok\n    )\n\n    content = response.choices[0].message.content\n    finish_reason = response.choices[0].finish_reason\n\n    print(f\"\\nmax_tokens: {max_tok}\")\n    print(f\"Finish reason: {finish_reason}\")\n    print(f\"Actual tokens: {response.usage.completion_tokens}\")\n    print(f\"Response: {content[:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**finish_reason values:**\n",
    "- `\"stop\"` - Natural completion\n",
    "- `\"length\"` - Hit max_tokens limit (truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ top_p (Nucleus Sampling)\n",
    "\n",
    "Alternative to temperature. Limits tokens to cumulative probability threshold.\n",
    "\n",
    "```\n",
    "Token probabilities (sorted):\n",
    "Token A: 40%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Token B: 30%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Token C: 15%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Token D: 10%  â–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Token E: 3%   â–ˆ\n",
    "Token F: 2%   â–ˆ\n",
    "\n",
    "top_p = 0.5:  Only A and B (40% + 30% = 70% â‰¥ 50%)\n",
    "top_p = 0.9:  A, B, C, and D (95% â‰¥ 90%)\n",
    "```\n",
    "\n",
    "**ğŸ¯ Pro Tip:** Use temperature **OR** top_p, not both!\n",
    "- Production apps often use `top_p=0.9`\n",
    "- Provides better quality control than temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming Responses\n",
    "\n",
    "**Duration:** 8 minutes\n",
    "\n",
    "### Why Stream?\n",
    "\n",
    "âŒ **Problem:** Long responses take 10+ seconds. User sees nothing until complete.\n",
    "âœ… **Solution:** Stream tokens as they're generated. Better UX!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Hands-On: Implement Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def stream_chat(message):\n",
    "    \"\"\"Stream a chat response token by token\"\"\"\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        stream=True,  # â† Enable streaming!\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        # Check if chunk has content\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)  # â† Print immediately\n",
    "            full_response += content\n",
    "\n",
    "    print()  # New line at end\n",
    "    return full_response\n",
    "\n",
    "# Test it!\n",
    "response = stream_chat(\"Tell me a short story about a robot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost Calculation\n",
    "\n",
    "**Duration:** 5 minutes\n",
    "\n",
    "Understanding costs is crucial for production applications!\n",
    "\n",
    "### ğŸ’° Pricing (December 2024)\n",
    "\n",
    "| Model | Input (per 1M tokens) | Output (per 1M tokens) |\n",
    "|-------|----------------------|------------------------|\n",
    "| GPT-3.5-turbo | $0.50 | $1.50 |\n",
    "| GPT-4-turbo | $10.00 | $30.00 |\n",
    "| GPT-4 | $30.00 | $60.00 |\n",
    "| Claude Haiku | $0.25 | $1.25 |\n",
    "| Claude Sonnet | $3.00 | $15.00 |\n",
    "| Claude Opus | $15.00 | $75.00 |\n",
    "| Gemini Pro | $0.125 | $0.375 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Hands-On: Cost Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_cost(prompt, response, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Calculate cost of an API call\"\"\"\n",
    "\n",
    "    # Pricing per 1M tokens\n",
    "    pricing = {\n",
    "        \"gpt-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50},\n",
    "        \"gpt-4-turbo\": {\"input\": 10.00, \"output\": 30.00},\n",
    "        \"gpt-4\": {\"input\": 30.00, \"output\": 60.00},\n",
    "    }\n",
    "\n",
    "    # Count tokens\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    input_tokens = len(encoding.encode(prompt))\n",
    "    output_tokens = len(encoding.encode(response))\n",
    "\n",
    "    # Calculate cost\n",
    "    input_cost = (input_tokens / 1_000_000) * pricing[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * pricing[model][\"output\"]\n",
    "\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": input_tokens + output_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": input_cost + output_cost\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "prompt = \"Explain machine learning in simple terms\"\n",
    "response = \"Machine learning is a way for computers to learn from data...\"\n",
    "\n",
    "cost_info = calculate_cost(prompt, response)\n",
    "print(f\"Input tokens: {cost_info['input_tokens']}\")\n",
    "print(f\"Output tokens: {cost_info['output_tokens']}\")\n",
    "print(f\"Total cost: ${cost_info['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building Your Chatbot\n",
    "\n",
    "**Duration:** 10 minutes\n",
    "\n",
    "Now let's combine everything into a reusable chatbot class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class SimpleChatbot:\n",
    "    def __init__(self, api_key, model=\"gpt-3.5-turbo\"):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.conversation_history = []\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "\n",
    "    def set_system_message(self, message):\n",
    "        \"\"\"Set the system prompt\"\"\"\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": message}\n",
    "        ]\n",
    "\n",
    "    def chat(self, user_message, stream=False):\n",
    "        \"\"\"Send a message and get response\"\"\"\n",
    "\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "\n",
    "        if stream:\n",
    "            return self._stream_response()\n",
    "        else:\n",
    "            return self._complete_response()\n",
    "\n",
    "    def _complete_response(self):\n",
    "        \"\"\"Get complete response at once\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.conversation_history,\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        assistant_message = response.choices[0].message.content\n",
    "\n",
    "        # Add to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "\n",
    "        # Track usage\n",
    "        self.total_tokens += response.usage.total_tokens\n",
    "        self.total_cost += self._calculate_call_cost(response.usage)\n",
    "\n",
    "        return assistant_message\n",
    "\n",
    "    def _stream_response(self):\n",
    "        \"\"\"Stream response token by token\"\"\"\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.conversation_history,\n",
    "            temperature=0.7,\n",
    "            max_tokens=500,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "\n",
    "        print()  # New line\n",
    "\n",
    "        # Add to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "\n",
    "        return full_response\n",
    "\n",
    "    def _calculate_call_cost(self, usage):\n",
    "        \"\"\"Calculate cost for this call\"\"\"\n",
    "        pricing = {\"gpt-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50}}\n",
    "\n",
    "        input_cost = (usage.prompt_tokens / 1_000_000) * pricing[self.model][\"input\"]\n",
    "        output_cost = (usage.completion_tokens / 1_000_000) * pricing[self.model][\"output\"]\n",
    "\n",
    "        return input_cost + output_cost\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get usage statistics\"\"\"\n",
    "        return {\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"total_cost\": f\"${self.total_cost:.6f}\",\n",
    "            \"messages\": len([m for m in self.conversation_history if m['role'] == 'user'])\n",
    "        }\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"Reset conversation\"\"\"\n",
    "        system_msg = [m for m in self.conversation_history if m['role'] == 'system']\n",
    "        self.conversation_history = system_msg\n",
    "\n",
    "\n",
    "# Test it!\n",
    "bot = SimpleChatbot(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "bot.set_system_message(\"You are a helpful assistant. Keep responses under 100 words.\")\n",
    "\n",
    "print(\"Chatbot ready! Try sending a message.\\n\")\n",
    "\n",
    "# Example usage\n",
    "response = bot.chat(\"What is Python?\", stream=False)\n",
    "print(f\"Assistant: {response}\")\n",
    "print(f\"\\nStats: {bot.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Capstone: SupportGenie v0.1\n",
    "\n",
    "**Duration:** 15 minutes\n",
    "\n",
    "Let's build a professional customer support chatbot!\n",
    "\n",
    "### ğŸ¯ Project Requirements\n",
    "\n",
    "Build **SupportGenie** with:\n",
    "- âœ… Professional, empathetic tone\n",
    "- âœ… Streaming responses\n",
    "- âœ… Token and cost tracking\n",
    "- âœ… Session statistics\n",
    "- âœ… Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass SupportGenieV1:\n    \"\"\"\n    SupportGenie - AI Customer Support Assistant\n    Version 0.1: Basic chatbot with professional tone\n    \"\"\"\n\n    def __init__(self, api_key):\n        self.client = OpenAI(api_key=api_key)\n        self.model = \"gpt-3.5-turbo\"\n        self.conversation_history = []\n        self.total_tokens = 0\n        self.total_cost = 0.0\n\n        # Set professional customer service personality\n        self.set_system_message(\"\"\"\nYou are SupportGenie, an AI customer support assistant for TechStore.\n\nGuidelines:\n- Be professional, empathetic, and helpful\n- Keep responses under 100 words\n- If you don't know something, be honest\n- Always offer to escalate to a human agent if needed\n- Use a friendly but professional tone\n\nResponse Format:\n1. Acknowledge the customer's concern\n2. Provide helpful information or solution\n3. Ask if there's anything else you can help with\n        \"\"\")\n\n    def set_system_message(self, message):\n        \"\"\"Set the system prompt\"\"\"\n        self.conversation_history = [\n            {\"role\": \"system\", \"content\": message}\n        ]\n\n    def chat(self, user_message, stream=True):\n        \"\"\"Send a message and get response\"\"\"\n\n        # Add user message to history\n        self.conversation_history.append({\n            \"role\": \"user\",\n            \"content\": user_message\n        })\n\n        # Stream response\n        stream_obj = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.conversation_history,\n            temperature=0.7,\n            max_tokens=500,\n            stream=True,\n            stream_options={\"include_usage\": True}  # Enable usage tracking for streaming\n        )\n\n        full_response = \"\"\n        usage_data = None\n\n        for chunk in stream_obj:\n            if chunk.choices[0].delta.content:\n                content = chunk.choices[0].delta.content\n                print(content, end=\"\", flush=True)\n                full_response += content\n\n            # Capture usage data from final chunk\n            if hasattr(chunk, 'usage') and chunk.usage is not None:\n                usage_data = chunk.usage\n\n        print()  # New line\n\n        # Add to history\n        self.conversation_history.append({\n            \"role\": \"assistant\",\n            \"content\": full_response\n        })\n\n        # Track usage and cost\n        if usage_data:\n            self.total_tokens += usage_data.total_tokens\n            self.total_cost += self._calculate_call_cost(usage_data)\n\n        return full_response\n\n    def _calculate_call_cost(self, usage):\n        \"\"\"Calculate cost for this call\"\"\"\n        pricing = {\"gpt-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50}}\n\n        input_cost = (usage.prompt_tokens / 1_000_000) * pricing[self.model][\"input\"]\n        output_cost = (usage.completion_tokens / 1_000_000) * pricing[self.model][\"output\"]\n\n        return input_cost + output_cost\n\n    def get_stats(self):\n        \"\"\"Get usage statistics\"\"\"\n        return {\n            \"messages\": len([m for m in self.conversation_history if m['role'] == 'user']),\n            \"total_cost\": f\"${self.total_cost:.4f}\"\n        }\n\n\n# Create and test SupportGenie\nbot = SupportGenieV1(api_key=os.getenv('OPENAI_API_KEY'))\n\nprint(\"=\"* 60)\nprint(\"    SupportGenie v0.1 - AI Customer Support\")\nprint(\"=\"* 60)\nprint(\"\\nHello! I'm SupportGenie, your AI support assistant.\")\nprint(\"How can I help you today?\\n\")\n\n# Example interaction\nprint(\"Customer: My order hasn't arrived yet\\n\")\nprint(\"SupportGenie: \", end=\"\")\nbot.chat(\"My order hasn't arrived yet\", stream=True)\n\nprint(f\"\\nSession Stats: {bot.get_stats()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Challenge: Enhance It!\n",
    "\n",
    "Add these features:\n",
    "1. **Cost warnings** - Alert when cost exceeds $0.10\n",
    "2. **Response time tracking** - Show how long each response took\n",
    "3. **Error handling** - Gracefully handle API errors\n",
    "4. **History export** - Save conversation to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your enhanced SupportGenie code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Review & Next Steps\n",
    "\n",
    "**Duration:** 5 minutes\n",
    "\n",
    "### ğŸ‰ Congratulations!\n",
    "\n",
    "You've completed Lab 1! You now know:\n",
    "- âœ… How LLMs work (prediction, not magic!)\n",
    "- âœ… Tokens and why they matter\n",
    "- âœ… Making API calls to OpenAI, Claude, Gemini\n",
    "- âœ… Key parameters (temperature, max_tokens, top_p)\n",
    "- âœ… Streaming for better UX\n",
    "- âœ… Cost calculation and optimization\n",
    "- âœ… Building production chatbots\n",
    "\n",
    "### ğŸ“Š Key Takeaways\n",
    "\n",
    "âœ… **LLMs predict the next token** based on training data\n",
    "âœ… **Tokens are the basic unit** - ~4 chars or Â¾ word\n",
    "âœ… **Temperature controls randomness** - lower = predictable, higher = creative\n",
    "âœ… **max_tokens limits response length** - plan for context window\n",
    "âœ… **top_p filters by probability** - preferred for production\n",
    "âœ… **Streaming improves UX** for long responses\n",
    "âœ… **Track costs carefully** - they add up quickly!\n",
    "âœ… **System messages set behavior** - use them wisely\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "**Ready for more?**\n",
    "\n",
    "ğŸ‘‰ [Lab 2: Prompt Engineering](../Lab2-Prompt-Engineering/codelab.md)\n",
    "\n",
    "Learn to write prompts that get exceptional results every time!\n",
    "\n",
    "### ğŸ“š Additional Resources\n",
    "\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "- [Anthropic Claude Docs](https://docs.anthropic.com/)\n",
    "- [Tiktoken Library](https://github.com/openai/tiktoken)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "\n",
    "**You're now ready to build AI applications! ğŸ‰**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ Quick Reference\n",
    "\n",
    "### Common Code Snippets\n",
    "\n",
    "**Basic API Call:**\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "```\n",
    "\n",
    "**Streaming:**\n",
    "```python\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "```\n",
    "\n",
    "**Count Tokens:**\n",
    "```python\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "tokens = len(encoding.encode(text))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**End of Lab 1** âœ…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}