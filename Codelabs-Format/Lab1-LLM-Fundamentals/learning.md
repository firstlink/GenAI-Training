# Lab 1: LLM Fundamentals & API Usage
## ğŸ“š Learning Material

> **Purpose:** Understand the theory and concepts before you code

---

## ğŸ“‹ Overview

| Property | Value |
|----------|-------|
| **Duration** | 30 minutes (reading) |
| **Difficulty** | Beginner |
| **Prerequisites** | Basic Python knowledge |
| **Next Step** | [Hands-On Lab â†’](lab.md) |

---

## ğŸ“– Table of Contents

1. [Introduction](#1-introduction)
2. [How LLMs Work](#2-how-llms-work)
3. [Understanding Tokens](#3-understanding-tokens)
4. [API Basics](#4-api-basics)
5. [Key Parameters Explained](#5-key-parameters-explained)
6. [Streaming Responses](#6-streaming-responses)
7. [Cost Calculation](#7-cost-calculation)
8. [Review & Key Takeaways](#8-review--key-takeaways)

---

## 1. Introduction

### Welcome to the World of LLMs! ğŸš€

Large Language Models (LLMs) are transforming every industry:
- ğŸ’¼ **Customer support** - Automated, intelligent responses
- ğŸ“Š **Data analysis** - Natural language queries
- ğŸ¤– **Personal assistants** - Context-aware help
- ğŸ“ **Content generation** - Articles, emails, code

By understanding how LLMs work and how to use their APIs, you'll be able to build production-ready AI applications.

---

### What You'll Learn in This Module

- ğŸ¯ How Large Language Models actually work
- ğŸ¯ What tokens are and why they matter
- ğŸ¯ How to make API calls to different providers
- ğŸ¯ Understanding key parameters that control behavior
- ğŸ¯ When and how to use streaming
- ğŸ¯ How to calculate and optimize costs

---

## 2. How LLMs Work

### What is a Large Language Model?

ğŸ§  **Simple Definition:**
> An LLM is a neural network trained on massive amounts of text to predict the most likely next word (or token) in a sequence.

Think of it as an **incredibly sophisticated autocomplete system**.

---

### The Training Process

LLMs go through three main training stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: PRE-TRAINING                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                     â”‚
â”‚  Input:  Billions of words from books, websites,   â”‚
â”‚          articles, code repositories                â”‚
â”‚                                                     â”‚
â”‚  Task:   Predict the next word in a sentence       â”‚
â”‚                                                     â”‚
â”‚  Result: Model learns grammar, facts, patterns,    â”‚
â”‚          and reasoning from vast amounts of data    â”‚
â”‚                                                     â”‚
â”‚  Example: "The capital of France is ___"           â”‚
â”‚           Model learns â†’ "Paris"                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: FINE-TUNING                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                     â”‚
â”‚  Input:  High-quality instruction-response pairs   â”‚
â”‚          curated by humans                          â”‚
â”‚                                                     â”‚
â”‚  Task:   Learn to follow instructions accurately   â”‚
â”‚          and generate helpful responses             â”‚
â”‚                                                     â”‚
â”‚  Result: Model becomes a helpful assistant that    â”‚
â”‚          can follow complex instructions            â”‚
â”‚                                                     â”‚
â”‚  Example: "Explain photosynthesis simply"          â”‚
â”‚           Model generates clear explanation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: ALIGNMENT (RLHF)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                     â”‚
â”‚  Input:  Human feedback on model responses         â”‚
â”‚          (thumbs up/down, rankings)                 â”‚
â”‚                                                     â”‚
â”‚  Task:   Be helpful, harmless, and honest          â”‚
â”‚          Avoid harmful or biased outputs            â”‚
â”‚                                                     â”‚
â”‚  Result: Safe, reliable, and aligned AI assistant  â”‚
â”‚          that follows ethical guidelines            â”‚
â”‚                                                     â”‚
â”‚  Example: Refuses harmful requests, admits          â”‚
â”‚           uncertainty, avoids making things up      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### How Text Generation Actually Works

Let's see what happens when you ask an LLM to complete a sentence:

**User Input:** "The weather today is"

```
Step 1: TOKENIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input text â†’ ["The", "weather", "today", "is"]

Step 2: PROBABILITY CALCULATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model calculates probability for EVERY possible next token:
  "sunny"      â†’ 30%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  "nice"       â†’ 25%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  "cloudy"     â†’ 20%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  "rainy"      â†’ 15%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  "beautiful"  â†’ 5%   â–ˆâ–ˆâ–ˆ
  "terrible"   â†’ 3%   â–ˆâ–ˆ
  ... (thousands more with tiny probabilities)

Step 3: SAMPLING (based on temperature)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Temperature = 0.0  â†’ Always pick "sunny" (most likely)
- Temperature = 0.7  â†’ Sample from top choices with variation
- Temperature = 2.0  â†’ Consider many options, very creative

Step 4: ADD TOKEN & REPEAT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Selected token "sunny" is added to sequence
New sequence: ["The", "weather", "today", "is", "sunny"]
Process repeats for next token until done
```

---

### ğŸ’¡ Critical Insight

**LLMs don't truly "know" facts or "understand" meaning.**

They're **statistical prediction engines** that:
- âœ… Generate highly convincing text based on patterns
- âœ… Can solve complex problems through learned patterns
- âœ… Produce creative and coherent responses
- âŒ Sometimes "hallucinate" (confidently state false information)
- âŒ Don't have real-time knowledge (training data has a cutoff)
- âŒ Can't truly reason like humans (simulate reasoning patterns)

This is why we need techniques like:
- **RAG (Retrieval-Augmented Generation)** - Ground responses in facts
- **Prompt engineering** - Guide the model effectively
- **Guardrails** - Prevent harmful outputs

---

## 3. Understanding Tokens

### What Are Tokens?

**Tokens** are the fundamental units that LLMs process.

ğŸ¯ **Not characters:** "Hello" could be 1 token, not 5
ğŸ¯ **Not words:** "ChatGPT" is often 2+ tokens
ğŸ¯ **Subword units:** Somewhere between characters and words

---

### The Token Economy

**Rule of Thumb (English):**
```
1 token â‰ˆ 4 characters
1 token â‰ˆ Â¾ of a word
100 tokens â‰ˆ 75 words
1,000 tokens â‰ˆ 750 words
```

---

### Real Examples

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT: "Hello, world!"                      â”‚
â”‚  TOKENS: ["Hello", ",", " world", "!"]      â”‚
â”‚  COUNT: 4 tokens                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT: "ChatGPT is amazing"                 â”‚
â”‚  TOKENS: ["Chat", "G", "PT", " is",         â”‚
â”‚           " amazing"]                        â”‚
â”‚  COUNT: 5 tokens                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT: "OpenAI GPT-4"                       â”‚
â”‚  TOKENS: ["Open", "AI", " G", "PT", "-4"]  â”‚
â”‚  COUNT: 5 tokens                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT: "The quick brown fox"                â”‚
â”‚  TOKENS: ["The", " quick", " brown", " fox"]â”‚
â”‚  COUNT: 4 tokens                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Why Tokens Matter

#### 1. **Context Window Limits**

Every model has a maximum token limit (input + output):

| Model | Context Window | Practical Limit |
|-------|---------------|-----------------|
| GPT-3.5-turbo | 4,096 tokens | ~3,000 words |
| GPT-3.5-turbo-16k | 16,384 tokens | ~12,000 words |
| GPT-4 | 8,192 tokens | ~6,000 words |
| GPT-4-turbo | 128,000 tokens | ~96,000 words |
| Claude 3 | 200,000 tokens | ~150,000 words |
| Gemini Pro | 32,768 tokens | ~24,000 words |

**What happens when you exceed the limit?**
```
âŒ Error: "This model's maximum context length is 4096 tokens"
```

---

#### 2. **Cost Per Token**

APIs charge based on token usage:

```
Example Pricing (GPT-3.5-turbo):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  $0.50 per 1M tokens
Output: $1.50 per 1M tokens

Sample Conversation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User prompt: "Explain photosynthesis" (3 tokens)
System message: 50 tokens
Model response: 150 tokens

Cost calculation:
Input tokens:  53 tokens Ã— $0.50/1M = $0.0000265
Output tokens: 150 tokens Ã— $1.50/1M = $0.0002250
Total cost: $0.0002515 (~$0.00025)

For 1,000 similar requests:
Total cost â‰ˆ $0.25
```

---

#### 3. **Performance Impact**

```
Token Count â†’ Processing Time

Input: 100 tokens, Output: 50 tokens
Response time: ~0.5 seconds

Input: 1,000 tokens, Output: 500 tokens
Response time: ~3-5 seconds

Input: 10,000 tokens, Output: 2,000 tokens
Response time: ~10-20 seconds
```

**More tokens = Slower responses + Higher costs**

---

### Token Encoding Differences

Different models use different tokenization:

```
Text: "Hello, ä¸–ç•Œ!" (Hello, World! in Chinese/Japanese)

GPT (cl100k_base encoding):
  ["Hello", ",", " ", "ä¸–", "ç•Œ", "!"]
  6 tokens

Claude (similar encoding):
  ["Hello", ",", " ä¸–ç•Œ", "!"]
  4 tokens

Why different? Different tokenizers handle:
- Non-English languages differently
- Special characters uniquely
- Common phrases as single tokens
```

**Lesson:** Always count tokens for YOUR specific model!

---

## 4. API Basics

### Understanding API Providers

Three major LLM API providers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPENAI (ChatGPT)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Models: GPT-3.5-turbo, GPT-4, GPT-4-turbo      â”‚
â”‚  Strengths: Fast, reliable, widely adopted       â”‚
â”‚  Best for: General purpose, production apps      â”‚
â”‚  API style: OpenAI standard                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANTHROPIC (Claude)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Models: Claude 3 Haiku, Sonnet, Opus           â”‚
â”‚  Strengths: Long context (200K), nuanced         â”‚
â”‚  Best for: Complex reasoning, long documents     â”‚
â”‚  API style: Similar but slightly different       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOOGLE (Gemini)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Models: Gemini Pro, Gemini Ultra               â”‚
â”‚  Strengths: Multimodal, generous free tier       â”‚
â”‚  Best for: Experimentation, cost-conscious       â”‚
â”‚  API style: Google's generative AI API          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Message Structure

All chat-based LLM APIs use a **message array** format:

```python
messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant."
    },
    {
        "role": "user",
        "content": "What is the capital of France?"
    },
    {
        "role": "assistant",
        "content": "The capital of France is Paris."
    },
    {
        "role": "user",
        "content": "What about Germany?"
    }
]
```

---

### Role Explanations

| Role | Purpose | When to Use |
|------|---------|-------------|
| **system** | Sets AI behavior, personality, constraints | First message, defines how AI should act |
| **user** | Human messages/questions | Every user input |
| **assistant** | AI responses | Conversation history, few-shot examples |

---

### System Message Best Practices

```python
# âŒ WEAK System Message
"You are helpful."

# âœ… STRONG System Message
"""
You are a professional customer support agent for TechStore.

Guidelines:
- Be empathetic and patient
- Keep responses under 100 words
- If you don't know, say so and offer to escalate
- Never make up product information
- Use a friendly but professional tone

Response format:
1. Acknowledge the customer's concern
2. Provide solution or next steps
3. Ask if there's anything else needed
"""
```

---

## 5. Key Parameters Explained

### Temperature (0.0 - 2.0)

**Controls randomness and creativity** in responses.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOW TEMPERATURE WORKS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Original token probabilities:                 â”‚
â”‚    Token A: 60%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚    Token B: 25%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       â”‚
â”‚    Token C: 10%  â–ˆâ–ˆâ–ˆ                           â”‚
â”‚    Token D: 5%   â–ˆâ–ˆ                            â”‚
â”‚                                                 â”‚
â”‚  Temperature = 0.1 (Very focused):             â”‚
â”‚    Token A: 95%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â”‚
â”‚    Token B: 4%   â–ˆ                             â”‚
â”‚    Token C: 0.8% â–Œ                             â”‚
â”‚    Token D: 0.2% â–Œ                             â”‚
â”‚                                                 â”‚
â”‚  Temperature = 1.0 (Balanced):                 â”‚
â”‚    Token A: 60%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚    Token B: 25%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       â”‚
â”‚    Token C: 10%  â–ˆâ–ˆâ–ˆ                           â”‚
â”‚    Token D: 5%   â–ˆâ–ˆ                            â”‚
â”‚                                                 â”‚
â”‚  Temperature = 2.0 (Very creative):            â”‚
â”‚    Token A: 40%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    â”‚
â”‚    Token B: 30%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚
â”‚    Token C: 20%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚
â”‚    Token D: 10%  â–ˆâ–ˆâ–ˆ                           â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Temperature Use Cases

| Temperature | Behavior | Best For | Example Use |
|-------------|----------|----------|-------------|
| **0.0** | Deterministic, always same output | Exact consistency needed | Data extraction, classification |
| **0.0-0.3** | Very focused, minimal variation | Factual tasks | Summarization, translation |
| **0.3-0.7** | Slight creativity, natural | General purpose | Chatbots, Q&A systems |
| **0.7-1.0** | Balanced creativity | Content generation | Email drafting, explanations |
| **1.0-1.5** | High creativity | Creative tasks | Story writing, brainstorming |
| **1.5-2.0** | Maximum creativity (may ramble) | Experimental | Poetry, unusual ideas |

---

### max_tokens

**Limits the maximum length** of the generated response.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UNDERSTANDING max_tokens                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Formula:                                      â”‚
â”‚  Input tokens + max_tokens â‰¤ Context Window   â”‚
â”‚                                                 â”‚
â”‚  Example with GPT-3.5-turbo (4K context):     â”‚
â”‚                                                 â”‚
â”‚  Scenario 1: âœ… Success                        â”‚
â”‚    Input: 500 tokens                           â”‚
â”‚    max_tokens: 1,000                           â”‚
â”‚    Total: 1,500 tokens < 4,096 âœ…             â”‚
â”‚                                                 â”‚
â”‚  Scenario 2: âŒ Error                          â”‚
â”‚    Input: 3,800 tokens                         â”‚
â”‚    max_tokens: 1,000                           â”‚
â”‚    Total: 4,800 tokens > 4,096 âŒ             â”‚
â”‚    Error: "exceeds maximum context length"     â”‚
â”‚                                                 â”‚
â”‚  Scenario 3: âš ï¸ Truncation                    â”‚
â”‚    Input: 100 tokens                           â”‚
â”‚    max_tokens: 50 (very low!)                  â”‚
â”‚    Response: "Quantum computing uses qubi..."   â”‚
â”‚    (cut off mid-sentence)                      â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### max_tokens Recommendations

| Use Case | Recommended max_tokens | Reason |
|----------|------------------------|--------|
| Simple Q&A | 50-100 | Short, concise answers |
| Chatbot responses | 150-300 | Conversational length |
| Explanations | 200-500 | Detailed but focused |
| Code generation | 500-1,000 | Complete functions |
| Long-form content | 1,000-2,000 | Articles, essays |
| Document summaries | 100-200 | Concise summaries |

---

### top_p (Nucleus Sampling)

**Alternative to temperature** - filters tokens by cumulative probability.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOW top_p WORKS (Nucleus Sampling)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Token probabilities (sorted):                 â”‚
â”‚    Token A: 40%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚    Token B: 30%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  â”‚
â”‚    Token C: 15%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        â”‚
â”‚    Token D: 10%  â–ˆâ–ˆâ–ˆâ–ˆ                          â”‚
â”‚    Token E: 3%   â–ˆ                             â”‚
â”‚    Token F: 2%   â–ˆ                             â”‚
â”‚                                                 â”‚
â”‚  top_p = 0.5:                                  â”‚
â”‚    Keep A and B only (40% + 30% = 70% â‰¥ 50%)  â”‚
â”‚    Very focused, predictable                   â”‚
â”‚                                                 â”‚
â”‚  top_p = 0.7:                                  â”‚
â”‚    Keep A, B, C (40%+30%+15% = 85% â‰¥ 70%)     â”‚
â”‚    Balanced variety                            â”‚
â”‚                                                 â”‚
â”‚  top_p = 0.9:                                  â”‚
â”‚    Keep A, B, C, D (95% â‰¥ 90%)                â”‚
â”‚    Good variety, filters outliers              â”‚
â”‚                                                 â”‚
â”‚  top_p = 1.0:                                  â”‚
â”‚    Keep all tokens                             â”‚
â”‚    No filtering                                â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### top_p vs temperature

| Aspect | top_p | temperature |
|--------|-------|-------------|
| **Mechanism** | Filters tokens by probability mass | Reshapes probability distribution |
| **Adaptivity** | Adapts to each context | Same effect everywhere |
| **Quality control** | Better at filtering bad tokens | Can include very unlikely tokens |
| **Common values** | 0.9 (production standard) | 0.7 (general use) |
| **Best practice** | Use OR temperature, not both! | Use OR top_p, not both! |

**ğŸ¯ Production Recommendation:** Use `top_p=0.9` for most applications.

---

### top_k (Available in some APIs)

**Limits consideration to top K most likely tokens.**

```
Available in:
âœ… Google Gemini (default: 40)
âœ… Cohere
âŒ OpenAI (use top_p instead)
âŒ Anthropic Claude (use top_p instead)

Example:
  top_k = 1  â†’ Always pick most likely (deterministic)
  top_k = 40 â†’ Consider top 40 tokens (balanced)
  top_k = 100 â†’ Consider top 100 (very creative)

Problem: Fixed size doesn't adapt to context
Solution: top_p is usually better!
```

---

## 6. Streaming Responses

### Why Streaming Matters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WITHOUT STREAMING (Bad UX)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  User: "Explain machine learning"              â”‚
â”‚                                                 â”‚
â”‚  [Loading for 8 seconds...]                    â”‚
â”‚  â³ User sees nothing...                        â”‚
â”‚  â³ User waits...                               â”‚
â”‚  â³ User gets frustrated...                     â”‚
â”‚                                                 â”‚
â”‚  Assistant: [Full response appears suddenly]    â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WITH STREAMING (Good UX)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  User: "Explain machine learning"              â”‚
â”‚                                                 â”‚
â”‚  Assistant: Machine                            â”‚
â”‚  Assistant: Machine learning                   â”‚
â”‚  Assistant: Machine learning is               â”‚
â”‚  Assistant: Machine learning is a              â”‚
â”‚  Assistant: Machine learning is a way...       â”‚
â”‚                                                 â”‚
â”‚  âœ… User sees progress immediately              â”‚
â”‚  âœ… Feels faster (perception)                   â”‚
â”‚  âœ… Can interrupt if not relevant               â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### When to Use Streaming

âœ… **Use streaming when:**
- Responses typically > 50 tokens
- User experience matters
- Building chat interfaces
- Long-form content generation

âŒ **Don't use streaming when:**
- Responses are very short (<20 tokens)
- You need the complete response before processing
- Building APIs (usually want complete response)
- Cost calculation needs to be done upfront

---

## 7. Cost Calculation

### Understanding API Pricing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRICING MODEL (All providers)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Cost = (Input tokens Ã— Input price) +        â”‚
â”‚         (Output tokens Ã— Output price)         â”‚
â”‚                                                 â”‚
â”‚  Input:  Your prompt + conversation history    â”‚
â”‚  Output: Model's generated response            â”‚
â”‚                                                 â”‚
â”‚  âš ï¸ Output tokens typically cost MORE!         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Current Pricing (December 2024)

| Provider | Model | Input (per 1M tokens) | Output (per 1M tokens) |
|----------|-------|----------------------|------------------------|
| **OpenAI** | GPT-3.5-turbo | $0.50 | $1.50 |
| | GPT-4-turbo | $10.00 | $30.00 |
| | GPT-4 | $30.00 | $60.00 |
| **Anthropic** | Claude 3 Haiku | $0.25 | $1.25 |
| | Claude 3 Sonnet | $3.00 | $15.00 |
| | Claude 3 Opus | $15.00 | $75.00 |
| **Google** | Gemini Pro | $0.125 | $0.375 |

---

### Real-World Cost Examples

```
Example 1: Customer Support Chatbot
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model: GPT-3.5-turbo
Average conversation:
  - Input: 200 tokens (history + new question)
  - Output: 100 tokens (response)

Cost per conversation:
  (200 Ã— $0.50/1M) + (100 Ã— $1.50/1M)
  = $0.0001 + $0.00015
  = $0.00025 per conversation

1,000 conversations = $0.25
10,000 conversations = $2.50
100,000 conversations = $25.00

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Example 2: Document Summarization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model: GPT-4-turbo
Long document:
  - Input: 5,000 tokens (document)
  - Output: 500 tokens (summary)

Cost per summary:
  (5,000 Ã— $10/1M) + (500 Ã— $30/1M)
  = $0.05 + $0.015
  = $0.065 per summary

100 summaries = $6.50

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Example 3: Code Generation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model: Claude 3 Sonnet
Code request:
  - Input: 500 tokens (requirements)
  - Output: 800 tokens (code + explanation)

Cost per generation:
  (500 Ã— $3/1M) + (800 Ã— $15/1M)
  = $0.0015 + $0.012
  = $0.0135 per generation

1,000 generations = $13.50
```

---

### Cost Optimization Strategies

#### 1. **Choose the Right Model**
```
Question: "What's 2+2?"
âŒ GPT-4: Overkill, expensive
âœ… GPT-3.5-turbo: Perfect, cheap

Question: "Analyze this legal contract for risks"
âŒ GPT-3.5-turbo: May miss nuances
âœ… GPT-4 or Claude Opus: Worth the cost
```

#### 2. **Limit max_tokens Appropriately**
```
âŒ max_tokens=2000 for simple Q&A (wasteful)
âœ… max_tokens=100 for simple Q&A (efficient)
```

#### 3. **Manage Conversation History**
```
âŒ Sending entire 50-message history every time
âœ… Keep only last 10 messages + system message
âœ… Summarize old messages periodically
```

#### 4. **Cache Responses**
```
âœ… Cache common questions
âœ… Cache system prompts
âœ… Reuse responses when possible
```

#### 5. **Monitor Usage**
```
âœ… Track costs per user/session
âœ… Set spending limits
âœ… Alert on unusual usage
```

---

## 8. Review & Key Takeaways

### ğŸ¯ What You've Learned

âœ… **How LLMs Work**
- Statistical prediction engines, not true intelligence
- Trained in 3 stages: pre-training, fine-tuning, alignment
- Generate text by predicting next token probabilities

âœ… **Tokens**
- Basic units of LLM processing (~4 chars, Â¾ word)
- Critical for costs, context limits, and performance
- Different encodings for different models

âœ… **API Basics**
- Three major providers: OpenAI, Anthropic, Google
- Message structure: system, user, assistant roles
- System messages define behavior

âœ… **Key Parameters**
- **temperature** (0-2): Controls creativity/randomness
- **max_tokens**: Limits response length
- **top_p** (0-1): Filters by probability (preferred)
- **top_k**: Fixed token limit (some APIs)

âœ… **Streaming**
- Better UX for long responses
- Shows progress immediately
- Can be interrupted

âœ… **Costs**
- Charged per token (input + output)
- Output typically costs more
- Optimize by choosing right model, limiting tokens, caching

---

### ğŸ“ Conceptual Knowledge Check

Before moving to the hands-on lab, make sure you understand:

1. **What does an LLM actually do?**
   <details>
   <summary>Answer</summary>
   Predicts the next most likely token based on input and training data. It's a statistical model, not truly intelligent.
   </details>

2. **Why do tokens matter?**
   <details>
   <summary>Answer</summary>
   They determine: (1) context limits, (2) API costs, (3) response speed
   </details>

3. **When would you use temperature=0 vs temperature=1.5?**
   <details>
   <summary>Answer</summary>
   Temp 0: Consistent, factual tasks (data extraction, classification)
   Temp 1.5: Creative tasks (writing, brainstorming, marketing)
   </details>

4. **What's the difference between temperature and top_p?**
   <details>
   <summary>Answer</summary>
   Temperature reshapes probabilities globally. top_p filters tokens by cumulative probability (more adaptive). Use one or the other, not both.
   </details>

5. **Why does streaming matter?**
   <details>
   <summary>Answer</summary>
   Better user experience - users see progress immediately rather than waiting 10+ seconds for complete response.
   </details>

---

### ğŸš€ Ready for Hands-On Practice?

Now that you understand the theory, it's time to **write actual code**!

ğŸ‘‰ **[Continue to Hands-On Lab â†’](lab.md)**

In the lab, you'll:
- âœ… Set up your environment and API keys
- âœ… Make your first API calls
- âœ… Experiment with different parameters
- âœ… Implement streaming responses
- âœ… Build a complete chatbot (SupportGenie v0.1)

---

### ğŸ“š Additional Reading (Optional)

Want to go deeper? Check out:
- [How GPT-3 Works - Visualizations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
- [OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)
- [Anthropic's Model Context Protocol](https://www.anthropic.com/research)
- [Google's AI Principles](https://ai.google/responsibility/principles/)

---

**Next:** [Hands-On Lab â†’](lab.md)
