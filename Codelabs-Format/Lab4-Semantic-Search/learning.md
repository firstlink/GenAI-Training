# Lab 4: Semantic Search & Retrieval

## ğŸ“š Learning Material

**Duration:** 30 minutes
**Difficulty:** Intermediate
**Prerequisites:** Lab 3 completed (document processing & embeddings)

---

## ğŸ¯ Learning Objectives

By the end of this learning module, you will understand:
- âœ… Semantic search vs. traditional keyword search
- âœ… How to query vector databases
- âœ… Distance metrics and similarity scoring
- âœ… Top-K retrieval strategies
- âœ… Metadata filtering for advanced queries
- âœ… Hybrid search combining semantic + keyword approaches
- âœ… When to use which search method

---

## ğŸ“– Table of Contents

1. [Introduction: Search Paradigms](#1-introduction-search-paradigms)
2. [How Semantic Search Works](#2-how-semantic-search-works)
3. [Distance Metrics Explained](#3-distance-metrics-explained)
4. [Top-K Retrieval](#4-top-k-retrieval)
5. [Advanced Querying](#5-advanced-querying)
6. [Hybrid Search](#6-hybrid-search)
7. [Search Strategy Selection](#7-search-strategy-selection)
8. [Review & Key Takeaways](#8-review--key-takeaways)

---

## 1. Introduction: Search Paradigms

### The Evolution of Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEARCH EVOLUTION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  1990s: EXACT MATCH                                   â”‚
â”‚  "Find 'artificial intelligence'"                     â”‚
â”‚  â†’ Only matches those exact words                     â”‚
â”‚                                                        â”‚
â”‚  2000s: KEYWORD SEARCH (Google, BM25)                 â”‚
â”‚  "artificial intelligence"                            â”‚
â”‚  â†’ Matches: AI, A.I., artificial, intelligence        â”‚
â”‚  â†’ Uses TF-IDF, ranking algorithms                    â”‚
â”‚                                                        â”‚
â”‚  2020s: SEMANTIC SEARCH (Vector-based)                â”‚
â”‚  "teaching computers to think"                        â”‚
â”‚  â†’ Matches: machine learning, AI, neural networks     â”‚
â”‚  â†’ Understands MEANING, not just words                â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Traditional Keyword Search

**How it works:**
1. User enters query: "machine learning"
2. System looks for documents containing those words
3. Returns documents with "machine" AND "learning"
4. Ranks by frequency (TF-IDF) or other metrics

**Limitations:**
```
Query: "How do I get my money back?"

Keyword Search Results:
âŒ Misses: "Refund Policy" (no keywords match!)
âŒ Misses: "Return Process" (different words)
âŒ Finds: "We don't give money back" (has keywords but wrong meaning!)
```

### Semantic Search

**How it works:**
1. User enters query: "How do I get my money back?"
2. System converts query â†’ embedding vector
3. Compares query vector with all document vectors
4. Returns most similar documents by meaning

**Advantages:**
```
Query: "How do I get my money back?"

Semantic Search Results:
âœ… Finds: "Refund Policy" (similar meaning!)
âœ… Finds: "Return Process" (conceptually related)
âœ… Finds: "Getting Your Money Refunded" (exact match semantically)
```

### Side-by-Side Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Feature            â”‚  Keyword Search      â”‚ Semantic Search  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Exact matches        â”‚  â˜…â˜…â˜…â˜…â˜…               â”‚  â˜…â˜…â˜…â˜†â˜†           â”‚
â”‚ Synonyms             â”‚  â˜…â˜†â˜†â˜†â˜†               â”‚  â˜…â˜…â˜…â˜…â˜…           â”‚
â”‚ Related concepts     â”‚  â˜…â˜†â˜†â˜†â˜†               â”‚  â˜…â˜…â˜…â˜…â˜…           â”‚
â”‚ Acronyms             â”‚  â˜…â˜…â˜…â˜…â˜…               â”‚  â˜…â˜…â˜†â˜†â˜†           â”‚
â”‚ Speed                â”‚  â˜…â˜…â˜…â˜…â˜…               â”‚  â˜…â˜…â˜…â˜…â˜†           â”‚
â”‚ Understands context  â”‚  â˜…â˜†â˜†â˜†â˜†               â”‚  â˜…â˜…â˜…â˜…â˜…           â”‚
â”‚ Handles typos        â”‚  â˜…â˜†â˜†â˜†â˜†               â”‚  â˜…â˜…â˜…â˜†â˜†           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**â±ï¸ Duration so far:** 5 minutes

---

## 2. How Semantic Search Works

### The Search Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEMANTIC SEARCH PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  USER INPUT:                                            â”‚
â”‚  "What is deep learning?"                               â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚  EMBEDDING MODEL         â”‚                          â”‚
â”‚  â”‚  (same as documents!)    â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚         â†“                                               â”‚
â”‚  QUERY EMBEDDING:                                       â”‚
â”‚  [0.23, -0.45, 0.78, 0.12, ...]  (384 dimensions)      â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚  VECTOR DATABASE         â”‚                          â”‚
â”‚  â”‚  (ChromaDB)              â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚         â†“                                               â”‚
â”‚  SIMILARITY CALCULATION:                                â”‚
â”‚  Compare query embedding with all document embeddings   â”‚
â”‚         â†“                                               â”‚
â”‚  RANKED RESULTS:                                        â”‚
â”‚  1. "Deep learning uses neural networks..." (0.12)      â”‚
â”‚  2. "Neural networks with multiple layers..." (0.18)    â”‚
â”‚  3. "Machine learning subset..." (0.25)                 â”‚
â”‚     (Lower distance = more similar)                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Requirements

**1. Same Embedding Model**

âš ï¸ **CRITICAL**: Must use the SAME model for documents and queries!

```
Documents embedded with:
  all-MiniLM-L6-v2 (384 dimensions)

Query MUST be embedded with:
  all-MiniLM-L6-v2 (384 dimensions)

âŒ WRONG: Query with all-mpnet-base-v2 (768 dimensions)
â†’ Dimension mismatch! Won't work!
```

**2. Vector Database**

The database stores:
- âœ… Document chunks (text)
- âœ… Embeddings (vectors)
- âœ… Metadata (source, chunk index, etc.)

And provides:
- âœ… Fast similarity search
- âœ… Ranking by distance/similarity
- âœ… Filtering capabilities

**3. Similarity Calculation**

How similar is the query to each document?

```python
query_embedding = [0.5, 0.8, -0.2, ...]
doc1_embedding = [0.5, 0.8, -0.2, ...]  # Very similar!
doc2_embedding = [-0.3, 0.1, 0.7, ...]  # Different

similarity(query, doc1) = 0.99  # High!
similarity(query, doc2) = 0.35  # Low
```

### Real-World Example

```
Question: "How do neural networks learn?"

Step 1: Embed the question
â†’ [0.23, -0.11, 0.56, 0.78, -0.34, ...]

Step 2: Compare with documents in database
Document 1: "Neural networks adjust weights..."
  Embedding: [0.25, -0.10, 0.58, 0.76, -0.32, ...]
  Distance: 0.08 â† Very close!

Document 2: "Computer vision applications..."
  Embedding: [-0.12, 0.45, -0.23, 0.11, 0.67, ...]
  Distance: 1.45 â† Far away

Document 3: "Training with backpropagation..."
  Embedding: [0.22, -0.13, 0.54, 0.79, -0.35, ...]
  Distance: 0.12 â† Close!

Step 3: Return top results (lowest distances)
1. Document 1 (distance: 0.08)
2. Document 3 (distance: 0.12)
```

**â±ï¸ Duration so far:** 10 minutes

---

## 3. Distance Metrics Explained

### What is Distance?

**Distance** = How "far apart" two vectors are in multi-dimensional space.

In 2D (easy to visualize):
```
     Y
     â†‘
  B  â€¢
     â”‚
     â”‚   â€¢ A
     â”‚
  â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â†’ X

Distance(A, B) = âˆš[(xâ‚‚-xâ‚)Â² + (yâ‚‚-yâ‚)Â²]
```

In 384D (what we actually use):
- Same concept, just 384 dimensions instead of 2!
- Can't visualize, but math is the same

### Common Distance Metrics

#### 1. Euclidean Distance (L2)

**What it is:** Straight-line distance between two points.

```
Formula: âˆš[Î£(aâ‚ - bâ‚)Â² + (aâ‚‚ - bâ‚‚)Â² + ... + (aâ‚ƒâ‚ˆâ‚„ - bâ‚ƒâ‚ˆâ‚„)Â²]

Properties:
- Range: 0 to âˆ
- 0 = identical vectors
- Larger = more different
```

**Example:**
```python
vector1 = [0.5, 0.8, -0.2]
vector2 = [0.5, 0.8, -0.2]  # Identical
euclidean_distance = 0.0

vector3 = [1.0, 0.0, 0.5]   # Different
euclidean_distance = 1.24
```

**ChromaDB uses L2 (Euclidean) distance by default!**

#### 2. Cosine Similarity

**What it is:** Measures the angle between two vectors.

```
Formula: (A Â· B) / (||A|| Ã— ||B||)

Properties:
- Range: -1 to 1
- 1 = same direction (identical meaning)
- 0 = perpendicular (unrelated)
- -1 = opposite direction (opposite meaning)
```

**Visual:**
```
Same direction (similar):
  A â†’â†’â†’â†’â†’
  B â†’â†’â†’â†’

Cosine similarity = 1.0

Different directions (dissimilar):
  A â†’â†’â†’â†’â†’
        â†“
        â†“ B
        â†“

Cosine similarity = 0.0
```

**Often converted to distance:**
```python
cosine_distance = 1 - cosine_similarity

# If similarity = 0.9 (very similar)
# Then distance = 1 - 0.9 = 0.1 (very close)
```

#### 3. Dot Product

**What it is:** Simple multiplication and sum.

```
Formula: Aâ‚Ã—Bâ‚ + Aâ‚‚Ã—Bâ‚‚ + ... + Aâ‚ƒâ‚ˆâ‚„Ã—Bâ‚ƒâ‚ˆâ‚„

Properties:
- Range: -âˆ to âˆ
- Higher = more similar (if vectors normalized)
- Fast to compute
```

### Which Metric to Use?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric          â”‚ When to Use                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Euclidean (L2)  â”‚ Default for ChromaDB                 â”‚
â”‚                 â”‚ Good general-purpose metric          â”‚
â”‚                 â”‚ Considers both direction and length  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cosine          â”‚ When vector magnitude doesn't matter â”‚
â”‚                 â”‚ Good for text (sentence length â‰      â”‚
â”‚                 â”‚   importance)                        â”‚
â”‚                 â”‚ Often better for semantic search    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dot Product     â”‚ When vectors are normalized          â”‚
â”‚                 â”‚ Fastest to compute                   â”‚
â”‚                 â”‚ Equivalent to cosine if normalized   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Interpreting Distance Scores

**For L2 (Euclidean) Distance:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Distance    â”‚  Interpretation            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  0.0 - 0.3   â”‚  Extremely similar         â”‚
â”‚  0.3 - 0.6   â”‚  Very similar              â”‚
â”‚  0.6 - 1.0   â”‚  Similar                   â”‚
â”‚  1.0 - 1.5   â”‚  Somewhat related          â”‚
â”‚  > 1.5       â”‚  Not very related          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**For Cosine Similarity:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Similarity  â”‚  Interpretation            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  0.9 - 1.0   â”‚  Extremely similar         â”‚
â”‚  0.8 - 0.9   â”‚  Very similar              â”‚
â”‚  0.7 - 0.8   â”‚  Similar                   â”‚
â”‚  0.5 - 0.7   â”‚  Somewhat related          â”‚
â”‚  < 0.5       â”‚  Not very related          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**â±ï¸ Duration so far:** 15 minutes

---

## 4. Top-K Retrieval

### What is Top-K?

**Top-K retrieval** = Returning the K most similar results.

```
Query: "What is machine learning?"

K=1 (Top-1): Return only the BEST match
K=3 (Top-3): Return the 3 BEST matches
K=10 (Top-10): Return the 10 BEST matches
```

### How to Choose K

**Too Small (K=1):**
```
âŒ Might miss relevant information
âŒ No diversity in results
âŒ If best match is poor, you're stuck with it
```

**Too Large (K=20):**
```
âŒ Includes less relevant results
âŒ More noise for the LLM to process
âŒ Higher costs (more tokens)
âŒ Slower processing
```

**Just Right (K=3-5):**
```
âœ… Multiple perspectives
âœ… Captures main relevant content
âœ… Reasonable token count
âœ… Good balance of precision and recall
```

### Recommended K Values

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Use Case            â”‚  K Value   â”‚  Why            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Quick answers        â”‚  1-2       â”‚ Fast, focused   â”‚
â”‚ Standard RAG         â”‚  3-5       â”‚ Balanced        â”‚
â”‚ Comprehensive        â”‚  5-10      â”‚ More context    â”‚
â”‚ Research/Analysis    â”‚  10-20     â”‚ Thorough        â”‚
â”‚ Reranking pipeline   â”‚  20-50     â”‚ Filter later    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### K vs. Context Window

**Important consideration:**

```
LLM Context Window: 4096 tokens
Chunk size: ~200 tokens
System prompt: ~100 tokens
User query: ~50 tokens

Available for context: 4096 - 100 - 50 = 3946 tokens

Maximum K: 3946 / 200 â‰ˆ 19 chunks

Practical K (with room for response): 3-10 chunks
```

### Example: Different K Values

```
Query: "How do neural networks learn?"

K=1:
[1] "Neural networks learn by adjusting weights through
     backpropagation based on training data..."
â†’ Single focused answer

K=3:
[1] "Neural networks learn by adjusting weights..."
[2] "The learning process involves forward and backward
     passes through the network..."
[3] "Training data is used to optimize the network's
     parameters using gradient descent..."
â†’ Multiple perspectives, richer context

K=10:
[1-3] Directly relevant
[4-6] Related concepts
[7-10] Loosely related or redundant
â†’ More comprehensive but potentially noisy
```

**â±ï¸ Duration so far:** 20 minutes

---

## 5. Advanced Querying

### Metadata Filtering

**Metadata** = Additional information stored with each chunk.

```
Chunk: "Our return policy allows 30-day returns..."
Metadata: {
  "source": "policy_2024.pdf",
  "chunk_index": 3,
  "document_type": "policy",
  "last_updated": "2024-01-15"
}
```

### Why Use Metadata Filtering?

**Scenario 1: Multi-Source Database**
```
Question: "What's the return policy?"

Without filtering:
â†’ Returns results from ALL sources (policies, emails, blogs, etc.)

With filtering:
â†’ Only search in source="policy_2024.pdf"
â†’ More accurate, relevant results
```

**Scenario 2: Time-Sensitive Information**
```
Question: "What are the current shipping rates?"

Filter: last_updated >= "2024-01-01"
â†’ Only get recent information, not outdated rates
```

### Filter Examples

```python
# Filter by source
filter = {"source": {"$eq": "employee_handbook.pdf"}}

# Filter by date
filter = {"last_updated": {"$gte": "2024-01-01"}}

# Filter by document type
filter = {"document_type": {"$in": ["policy", "guideline"]}}

# Combine filters
filter = {
  "$and": [
    {"source": {"$eq": "handbook.pdf"}},
    {"section": {"$eq": "benefits"}}
  ]
}
```

### ChromaDB Filter Operators

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Operator    â”‚  Meaning                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  $eq         â”‚  Equal to                          â”‚
â”‚  $ne         â”‚  Not equal to                      â”‚
â”‚  $gt         â”‚  Greater than                      â”‚
â”‚  $gte        â”‚  Greater than or equal             â”‚
â”‚  $lt         â”‚  Less than                         â”‚
â”‚  $lte        â”‚  Less than or equal                â”‚
â”‚  $in         â”‚  In list                           â”‚
â”‚  $nin        â”‚  Not in list                       â”‚
â”‚  $and        â”‚  All conditions must match         â”‚
â”‚  $or         â”‚  Any condition must match          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Combining Filters with Search

```
Query: "vacation policy"
Filter: {
  "$and": [
    {"source": "employee_handbook.pdf"},
    {"last_updated": {"$gte": "2024-01-01"}},
    {"section": "benefits"}
  ]
}

Result:
1. Find all chunks matching the filter
2. Calculate similarity with query
3. Return top-k most similar chunks
â†’ Only from employee handbook, recent, in benefits section
```

**â±ï¸ Duration so far:** 25 minutes

---

## 6. Hybrid Search

### The Problem with Single Methods

**Semantic Search Alone:**
```
Query: "NLP applications"

Issue: "NLP" is an acronym
- Semantic search might not find exact "NLP" mentions
- Might return "natural language processing" (good!)
- But could miss specific "NLP" technical discussions
```

**Keyword Search Alone:**
```
Query: "teaching computers to understand language"

Issue: No exact keyword matches
- Document says "natural language processing"
- Keyword search misses it (different words!)
- But meaning is identical
```

### Solution: Hybrid Search

**Hybrid Search** = Semantic Search + Keyword Search combined!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HYBRID SEARCH ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Query: "NLP applications"                              â”‚
â”‚         â†“                     â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  SEMANTIC    â”‚      â”‚   KEYWORD    â”‚               â”‚
â”‚  â”‚   SEARCH     â”‚      â”‚   SEARCH     â”‚               â”‚
â”‚  â”‚  (Vectors)   â”‚      â”‚   (BM25)     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â†“                     â†“                         â”‚
â”‚    Results A             Results B                      â”‚
â”‚    (semantic)            (keyword)                      â”‚
â”‚         â†“                     â†“                         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                   â†“                                     â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚          â”‚  MERGE & RANK   â”‚                           â”‚
â”‚          â”‚   (Weighted)    â”‚                           â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                   â†“                                     â”‚
â”‚          Final Ranked Results                          â”‚
â”‚          (best of both!)                               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### BM25: The Keyword Search Algorithm

**BM25** = Best Match 25, a ranking function used by search engines.

**How it works:**
1. **Term Frequency (TF)**: How often does the term appear in the document?
2. **Inverse Document Frequency (IDF)**: How rare is the term overall?
3. **Document Length**: Normalize by document length

**Formula (simplified):**
```
score(query, doc) = Î£ IDF(term) Ã— TF(term, doc) Ã— boost_factors

For each term in query:
  - Rare terms (high IDF) = more important
  - Frequent in doc (high TF) = more relevant
  - Shorter docs get slight boost
```

**Example:**
```
Query: "machine learning algorithms"

Document 1: "Machine learning algorithms are used for..."
  - "machine" appears 3 times (common word, low IDF)
  - "learning" appears 2 times (common word, low IDF)
  - "algorithms" appears 5 times (important word, high IDF)
  BM25 score: 8.5

Document 2: "Algorithms for sorting data include..."
  - "algorithms" appears 2 times
  - "machine" appears 0 times
  - "learning" appears 0 times
  BM25 score: 3.2

Document 1 wins!
```

### Combining Scores

**Method 1: Weighted Average**
```python
final_score = (semantic_weight Ã— semantic_score) +
              (keyword_weight Ã— keyword_score)

# Example:
semantic_score = 0.85  # High similarity
keyword_score = 0.30   # Low keyword match

# 70% semantic, 30% keyword
final_score = (0.7 Ã— 0.85) + (0.3 Ã— 0.30)
            = 0.595 + 0.09
            = 0.685
```

**Method 2: Reciprocal Rank Fusion (RRF)**
```python
# Instead of combining scores, combine rankings

Semantic ranking:   [doc3, doc1, doc5, doc2, doc4]
Keyword ranking:    [doc1, doc3, doc2, doc5, doc4]

RRF score for each doc = Î£(1 / (k + rank))
where k = constant (usually 60)

doc1: 1/(60+2) + 1/(60+1) = 0.0161 + 0.0164 = 0.0325
doc3: 1/(60+1) + 1/(60+2) = 0.0164 + 0.0161 = 0.0325
doc5: 1/(60+3) + 1/(60+4) = 0.0159 + 0.0156 = 0.0315

Final ranking: [doc1, doc3, doc5, doc2, doc4]
```

**RRF Advantages:**
âœ… No need to normalize scores
âœ… Works with different scoring systems
âœ… Resistant to outliers
âœ… Industry-proven method

### When to Use Hybrid Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Type          â”‚  Best Method    â”‚  Why             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "What is ML?"        â”‚  Semantic       â”‚  Conceptual      â”‚
â”‚ "NLP"                â”‚  Keyword        â”‚  Acronym         â”‚
â”‚ "neural networks"    â”‚  Hybrid         â”‚  Exact + similar â”‚
â”‚ "teaching computers" â”‚  Semantic       â”‚  No exact match  â”‚
â”‚ "API key"            â”‚  Keyword        â”‚  Technical term  â”‚
â”‚ "refund process"     â”‚  Hybrid         â”‚  Both useful     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**â±ï¸ Duration so far:** 30 minutes

---

## 7. Search Strategy Selection

### Decision Tree

```
                    START
                      â”‚
                      â†“
            Is query an acronym?
                 /        \
               YES         NO
                â”‚           â”‚
                â†“           â†“
          Use Keyword    Is query conceptual?
              or            /        \
           Hybrid         YES         NO
                           â”‚           â”‚
                           â†“           â†“
                      Use Semantic  Exact match needed?
                                      /        \
                                    YES         NO
                                     â”‚           â”‚
                                     â†“           â†“
                                Use Hybrid  Use Semantic
```

### Strategy Guide

**1. Semantic Search (Vector-only)**

**Use when:**
- âœ… Queries are natural language questions
- âœ… Synonyms and paraphrasing are common
- âœ… Conceptual understanding is key
- âœ… No specific technical terms required

**Examples:**
- "How do I return a product?"
- "What is the vacation policy?"
- "Explain machine learning"

---

**2. Keyword Search (BM25-only)**

**Use when:**
- âœ… Exact term matching is critical
- âœ… Queries contain acronyms or codes
- âœ… Technical jargon that shouldn't be paraphrased
- âœ… Speed is paramount

**Examples:**
- "API-KEY-123"
- "NLP transformer"
- "HTTP 404 error"

---

**3. Hybrid Search (Combined)**

**Use when:**
- âœ… General-purpose search
- âœ… Mix of exact and conceptual matching
- âœ… Production systems (best overall performance)
- âœ… User queries are unpredictable

**Examples:**
- "NLP sentiment analysis" (acronym + concept)
- "return policy for laptops" (exact product + concept)
- "API authentication methods" (technical + general)

**Recommended weight:** 60% semantic, 40% keyword

### Real-World Recommendations

**Customer Support Chatbot:**
```
Strategy: Hybrid (70% semantic, 30% keyword)
Why: Users ask natural questions but mention specific product names/codes
```

**Technical Documentation Search:**
```
Strategy: Hybrid (40% semantic, 60% keyword)
Why: Developers search for exact function names but also concepts
```

**Research Paper Search:**
```
Strategy: Semantic (100%)
Why: Conceptual understanding is key, synonyms are common
```

**E-commerce Product Search:**
```
Strategy: Keyword (80%), Semantic (20%)
Why: Users search by brand names, model numbers, but also descriptions
```

---

## 8. Review & Key Takeaways

### ğŸ¯ What You Learned

âœ… **Search Paradigms**: Traditional keyword vs. modern semantic search
âœ… **Distance Metrics**: L2 (Euclidean), Cosine similarity, Dot product
âœ… **Top-K Retrieval**: Choosing the right number of results
âœ… **Advanced Filtering**: Using metadata to refine searches
âœ… **Hybrid Search**: Combining semantic + keyword for best results
âœ… **Strategy Selection**: When to use which approach

### ğŸ’¡ Key Concepts

**1. Semantic Search Captures Meaning**
```
"refund" â‰ˆ "money back" â‰ˆ "return my purchase"
All have similar embeddings despite different words!
```

**2. Same Embedding Model Required**
```
Documents: all-MiniLM-L6-v2 (384D)
Queries:   all-MiniLM-L6-v2 (384D) âœ“

Documents: all-MiniLM-L6-v2 (384D)
Queries:   all-mpnet-base-v2 (768D) âœ—
```

**3. Lower Distance = More Similar**
```
Distance 0.08: Extremely relevant
Distance 0.85: Somewhat relevant
Distance 2.50: Not relevant
```

**4. Top-K Balance**
```
Too few (K=1): Might miss important info
Sweet spot (K=3-5): Best for RAG
Too many (K=20): Noise and high cost
```

**5. Hybrid Search is Often Best**
```
Semantic: Understands meaning
Keyword: Finds exact matches
Hybrid: Best of both worlds!
```

### ğŸ§  Knowledge Check

<details>
<summary><strong>Question 1:</strong> What's the main difference between semantic and keyword search?</summary>

**Answer:**
- **Keyword search** looks for exact word matches
- **Semantic search** understands meaning and finds conceptually similar content
- Semantic search uses embeddings to find "refund policy" when you search for "how to get money back"
</details>

<details>
<summary><strong>Question 2:</strong> Why must you use the same embedding model for documents and queries?</summary>

**Answer:**
Different models produce embeddings with different dimensions and different semantic spaces. Using different models is like trying to match GPS coordinates from Earth with coordinates from Mars - they won't align correctly!
</details>

<details>
<summary><strong>Question 3:</strong> What does a distance score of 0.5 mean in L2 distance?</summary>

**Answer:**
With L2 (Euclidean) distance, 0.5 typically indicates a somewhat similar/related document. Lower is better (0.0 = identical). Generally:
- 0.0-0.3: Very similar
- 0.3-0.6: Similar  â† 0.5 is here
- 0.6-1.0: Somewhat related
- >1.0: Less related
</details>

<details>
<summary><strong>Question 4:</strong> When should you use hybrid search instead of pure semantic search?</summary>

**Answer:**
Use hybrid search when:
- Queries contain both conceptual questions and specific terms/acronyms
- You want the benefits of both semantic understanding AND exact matching
- In production systems where query types are unpredictable
- When searching technical documentation with specific function/API names

Example: "NLP sentiment analysis" benefits from keyword matching "NLP" and semantic understanding of "sentiment analysis"
</details>

<details>
<summary><strong>Question 5:</strong> What's the recommended K value for standard RAG applications?</summary>

**Answer:**
K=3-5 is recommended for standard RAG because:
- Provides multiple perspectives
- Doesn't overwhelm the LLM with too much context
- Balances precision and recall
- Fits well within typical context windows
- Cost-effective (fewer tokens)
</details>

### ğŸš€ Ready for Hands-On Practice?

You now understand:
- âœ… How semantic search finds meaning, not just keywords
- âœ… Distance metrics and what scores mean
- âœ… Top-K retrieval strategies
- âœ… Metadata filtering for advanced queries
- âœ… Hybrid search combining both approaches
- âœ… When to use which search method

**Next step**: [Hands-On Lab â†’](lab.md)

In the lab, you'll:
1. Query your vector database from Lab 3
2. Implement semantic search
3. Compare different K values
4. Add metadata filtering
5. Build hybrid search with BM25
6. Create a complete search system

---

### ğŸ“š Additional Resources

**Want to dive deeper?**
- [ChromaDB Querying Documentation](https://docs.trychroma.com/usage-guide#querying-a-collection)
- [BM25 Algorithm Explained](https://en.wikipedia.org/wiki/Okapi_BM25)
- [Reciprocal Rank Fusion Paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
- [Vector Search Best Practices](https://www.pinecone.io/learn/vector-search/)

---

**Learning Material Complete!** âœ…
[â† Back to README](../README.md) | [Start Hands-On Lab â†’](lab.md)
