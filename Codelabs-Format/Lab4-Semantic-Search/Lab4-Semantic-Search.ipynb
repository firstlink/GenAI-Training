{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Semantic Search & Retrieval\n",
    "\n",
    "## üõ†Ô∏è Hands-On Lab\n",
    "\n",
    "**Duration:** 60-90 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** Lab 3 completed (ChromaDB with stored documents)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this lab, you'll have:\n",
    "- ‚úÖ Semantic search engine querying vector databases\n",
    "- ‚úÖ Top-K retrieval with configurable parameters\n",
    "- ‚úÖ Metadata filtering system\n",
    "- ‚úÖ Hybrid search combining semantic + keyword (BM25)\n",
    "- ‚úÖ Search comparison and evaluation tools\n",
    "- ‚úÖ **Capstone**: Production-ready search system with multiple strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup\n",
    "\n",
    "### Step 1: Verify Lab 3 Completion\n",
    "\n",
    "You need the ChromaDB database from Lab 3. Let's verify it exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if ChromaDB exists\n",
    "if os.path.exists('./chroma_db'):\n",
    "    print(\"‚úÖ ChromaDB directory found\")\n",
    "    files = os.listdir('./chroma_db')\n",
    "    print(f\"Files: {files}\")\n",
    "else:\n",
    "    print(\"‚ùå ChromaDB not found. Please run Lab 3 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Additional Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install rank-bm25 matplotlib numpy scikit-learn langchain langchain-community sentence-transformers chromadb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: ChromaDB from Lab 3 exists and libraries installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Semantic Search (15 min)\n",
    "\n",
    "**Objective:** Query the vector database and retrieve relevant chunks.\n",
    "\n",
    "### Task 1A: Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "# Load the same embedding model from Lab 3\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(f\"‚úì Model loaded (dimension: {embedding_model.get_sentence_embedding_dimension()})\")\n",
    "\n",
    "# Connect to ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Get the collection from Lab 3\n",
    "# Note: Adjust collection name if you used a different name\n",
    "try:\n",
    "    collection = client.get_collection(name=\"lab3_documents\")\n",
    "    print(f\"‚úì Connected to collection: {collection.name}\")\n",
    "    print(f\"  Documents in collection: {collection.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Hint: Check collection name from Lab 3 or run Lab 3 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: Simple Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, n_results=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the vector database\n",
    "\n",
    "    Args:\n",
    "        query: Search query string\n",
    "        n_results: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        dict: Search results from ChromaDB\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test the search\n",
    "query = \"What is deep learning?\"\n",
    "results = semantic_search(query, n_results=3)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"QUERY: {query}\")\n",
    "print('='*60)\n",
    "\n",
    "for i in range(len(results['documents'][0])):\n",
    "    distance = results['distances'][0][i]\n",
    "    document = results['documents'][0][i]\n",
    "\n",
    "    print(f\"\\nResult {i+1} (Distance: {distance:.4f}):\")\n",
    "    print(f\"{document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C: Interpret Distance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_distance(distance):\n",
    "    \"\"\"\n",
    "    Provide human-readable interpretation of L2 distance\n",
    "\n",
    "    Args:\n",
    "        distance: L2 (Euclidean) distance score\n",
    "\n",
    "    Returns:\n",
    "        str: Interpretation\n",
    "    \"\"\"\n",
    "    if distance < 0.3:\n",
    "        return \"Extremely similar ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\"\n",
    "    elif distance < 0.6:\n",
    "        return \"Very similar ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ\"\n",
    "    elif distance < 1.0:\n",
    "        return \"Similar ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ\"\n",
    "    elif distance < 1.5:\n",
    "        return \"Somewhat related ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ\"\n",
    "    else:\n",
    "        return \"Not very related ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ\"\n",
    "\n",
    "# Enhanced display\n",
    "def display_results(query, results):\n",
    "    \"\"\"Display search results with interpretations\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEARCH: {query}\")\n",
    "    print('='*70)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        distance = results['distances'][0][i]\n",
    "        document = results['documents'][0][i]\n",
    "        metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "\n",
    "        interpretation = interpret_distance(distance)\n",
    "\n",
    "        print(f\"\\n[Result {i+1}] {interpretation}\")\n",
    "        print(f\"Distance: {distance:.4f}\")\n",
    "        if metadata:\n",
    "            print(f\"Metadata: {metadata}\")\n",
    "        print(f\"Text: {document[:200]}...\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"Tell me about computer vision\",\n",
    "    \"What is AI ethics?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = semantic_search(query, n_results=2)\n",
    "    display_results(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: You should see relevant results for each query with distance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Top-K Retrieval Strategies (15 min)\n",
    "\n",
    "**Objective:** Understand how different K values affect results.\n",
    "\n",
    "### Task 2A: Compare Different K Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_k_values(query, k_values=[1, 3, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compare search results with different K values\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "        k_values: List of K values to test\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TOP-K COMPARISON: '{query}'\")\n",
    "    print('='*70)\n",
    "\n",
    "    for k in k_values:\n",
    "        results = semantic_search(query, n_results=k)\n",
    "        distances = results['distances'][0]\n",
    "\n",
    "        print(f\"\\nK={k}:\")\n",
    "        print(f\"  Results returned: {len(distances)}\")\n",
    "        print(f\"  Best distance: {min(distances):.4f} ({interpret_distance(min(distances))})\")\n",
    "        print(f\"  Average distance: {np.mean(distances):.4f}\")\n",
    "        print(f\"  Worst distance: {max(distances):.4f} ({interpret_distance(max(distances))})\")\n",
    "\n",
    "# Test\n",
    "compare_k_values(\"What is machine learning?\")\n",
    "compare_k_values(\"Tell me about reinforcement learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B: Quality vs. Quantity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retrieval_quality(query, max_k=10):\n",
    "    \"\"\"\n",
    "    Analyze how result quality degrades with higher K\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "        max_k: Maximum K to test\n",
    "    \"\"\"\n",
    "    results = semantic_search(query, n_results=max_k)\n",
    "    distances = results['distances'][0]\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RETRIEVAL QUALITY ANALYSIS: '{query}'\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Calculate quality metrics at different K\n",
    "    for k in [1, 3, 5, max_k]:\n",
    "        if k <= len(distances):\n",
    "            top_k_distances = distances[:k]\n",
    "            avg_distance = np.mean(top_k_distances)\n",
    "            relevant_count = sum(1 for d in top_k_distances if d < 1.0)\n",
    "\n",
    "            print(f\"\\nTop-{k} Results:\")\n",
    "            print(f\"  Average distance: {avg_distance:.4f}\")\n",
    "            print(f\"  Relevant (distance < 1.0): {relevant_count}/{k}\")\n",
    "            print(f\"  Quality score: {relevant_count/k * 100:.1f}%\")\n",
    "\n",
    "# Test\n",
    "analyze_retrieval_quality(\"What is deep learning?\", max_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2C: Find Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(query, distance_threshold=1.0, max_k=20):\n",
    "    \"\"\"\n",
    "    Find optimal K based on distance threshold\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "        distance_threshold: Maximum acceptable distance\n",
    "        max_k: Maximum K to consider\n",
    "\n",
    "    Returns:\n",
    "        int: Recommended K value\n",
    "    \"\"\"\n",
    "    results = semantic_search(query, n_results=max_k)\n",
    "    distances = results['distances'][0]\n",
    "\n",
    "    # Find where quality drops below threshold\n",
    "    optimal_k = 0\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance <= distance_threshold:\n",
    "            optimal_k = i + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Distance threshold: {distance_threshold}\")\n",
    "    print(f\"Recommended K: {optimal_k}\")\n",
    "    print(f\"\\nRationale:\")\n",
    "    for i in range(min(optimal_k + 2, len(distances))):\n",
    "        status = \"‚úì Include\" if distances[i] <= distance_threshold else \"‚úó Exclude\"\n",
    "        print(f\"  Result {i+1}: {distances[i]:.4f} {status}\")\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "# Test with different queries\n",
    "find_optimal_k(\"What is machine learning?\", distance_threshold=0.8)\n",
    "find_optimal_k(\"Tell me about NLP\", distance_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: You should see how different K values affect result quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Metadata Filtering (15 min)\n",
    "\n",
    "**Objective:** Use metadata to refine searches.\n",
    "\n",
    "### Task 3A: Add Documents with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def add_documents_with_metadata():\n",
    "    \"\"\"Add sample documents with detailed metadata\"\"\"\n",
    "\n",
    "    # Sample documents with different sources\n",
    "    documents = {\n",
    "        \"policy.txt\": \"Our return policy allows customers to return unused items within 30 days of purchase. Items must be in original packaging. Refunds are processed within 5-7 business days.\",\n",
    "        \"faq.txt\": \"Q: How long does shipping take? A: Standard shipping takes 5-7 business days. Express shipping takes 2-3 business days.\",\n",
    "        \"guide.txt\": \"To set up your device, first charge it fully. Then download the companion app from the app store. Follow the in-app instructions to pair your device.\"\n",
    "    }\n",
    "\n",
    "    # Chunk and add each document\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "    for source, content in documents.items():\n",
    "        chunks = splitter.split_text(content)\n",
    "        embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "        # Get current count for ID generation\n",
    "        current_count = collection.count()\n",
    "\n",
    "        # Create IDs and metadata\n",
    "        ids = [f\"doc_{current_count + i}\" for i in range(len(chunks))]\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": source,\n",
    "                \"document_type\": source.split('.')[0],\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_size\": len(chunk)\n",
    "            }\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "\n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            documents=chunks,\n",
    "            embeddings=embeddings.tolist(),\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "        print(f\"‚úì Added {len(chunks)} chunks from {source}\")\n",
    "\n",
    "    print(f\"\\nTotal documents in collection: {collection.count()}\")\n",
    "\n",
    "# Add documents (uncomment to run)\n",
    "# add_documents_with_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3B: Search with Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_filter(query, where_filter, n_results=3):\n",
    "    \"\"\"\n",
    "    Search with metadata filtering\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "        where_filter: ChromaDB filter dictionary\n",
    "        n_results: Number of results\n",
    "\n",
    "    Returns:\n",
    "        dict: Filtered search results\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        where=where_filter,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example filters\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILTERED SEARCH EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Note: These examples assume you've added documents with metadata\n",
    "# Uncomment and run if you executed add_documents_with_metadata()\n",
    "\n",
    "# Filter 1: Search only in policy documents\n",
    "# query = \"How do I return an item?\"\n",
    "# filter1 = {\"document_type\": {\"$eq\": \"policy\"}}\n",
    "# results1 = search_with_filter(query, filter1, n_results=2)\n",
    "\n",
    "# print(f\"\\n1. Query: '{query}'\")\n",
    "# print(f\"   Filter: document_type = 'policy'\")\n",
    "# for i, (doc, meta) in enumerate(zip(results1['documents'][0], results1['metadatas'][0])):\n",
    "#     print(f\"\\n   Result {i+1}:\")\n",
    "#     print(f\"   Source: {meta.get('source', 'unknown')}\")\n",
    "#     print(f\"   Text: {doc[:100]}...\")\n",
    "\n",
    "print(\"\\nFiltered search requires documents with metadata.\")\n",
    "print(\"Run add_documents_with_metadata() first to test this feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C: Advanced Filter Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_advanced_filters():\n",
    "    \"\"\"Show different filter combinations\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ADVANCED FILTERING\")\n",
    "    print('='*70)\n",
    "\n",
    "    print(\"\\nExample filter operations:\")\n",
    "    print(\"1. Multiple sources: {'document_type': {'$in': ['policy', 'faq']}}\")\n",
    "    print(\"2. Chunk size range: {'chunk_size': {'$gte': 100}}\")\n",
    "    print(\"3. Combined (AND): {'$and': [filter1, filter2]}\")\n",
    "    \n",
    "    print(\"\\nNote: Add documents with metadata to test these filters.\")\n",
    "\n",
    "demonstrate_advanced_filters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: Filtered searches should return only results matching the metadata criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Hybrid Search with BM25 (25 min)\n",
    "\n",
    "**Objective:** Combine semantic search with keyword search for better results.\n",
    "\n",
    "### Task 4A: Implement BM25 Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Hybrid search combining semantic (vector) and keyword (BM25) search\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection, embedding_model):\n",
    "        \"\"\"\n",
    "        Initialize hybrid searcher\n",
    "\n",
    "        Args:\n",
    "            collection: ChromaDB collection\n",
    "            embedding_model: SentenceTransformer model\n",
    "        \"\"\"\n",
    "        self.collection = collection\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "        # Get all documents for BM25 indexing\n",
    "        all_data = collection.get(include=[\"documents\"])\n",
    "        self.documents = all_data['documents']\n",
    "        self.doc_ids = all_data['ids']\n",
    "\n",
    "        # Create BM25 index\n",
    "        tokenized_docs = [doc.lower().split() for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "        print(f\"‚úì Hybrid searcher initialized\")\n",
    "        print(f\"  Indexed {len(self.documents)} documents for BM25\")\n",
    "\n",
    "    def keyword_search(self, query, top_k=10):\n",
    "        \"\"\"Perform BM25 keyword search\"\"\"\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0:  # Only non-zero scores\n",
    "                results.append({\n",
    "                    'document': self.documents[idx],\n",
    "                    'score': scores[idx],\n",
    "                    'id': self.doc_ids[idx],\n",
    "                    'index': idx\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def semantic_search(self, query, top_k=10):\n",
    "        \"\"\"Perform semantic vector search\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        semantic_results = []\n",
    "        for doc, distance in zip(results['documents'][0], results['distances'][0]):\n",
    "            # Convert distance to similarity score\n",
    "            similarity = 1 / (1 + distance)\n",
    "            semantic_results.append({\n",
    "                'document': doc,\n",
    "                'score': similarity,\n",
    "                'distance': distance\n",
    "            })\n",
    "\n",
    "        return semantic_results\n",
    "\n",
    "    def hybrid_search(self, query, top_k=5, semantic_weight=0.5, keyword_weight=0.5):\n",
    "        \"\"\"Combine semantic and keyword search\"\"\"\n",
    "        # Get results from both methods\n",
    "        semantic_results = self.semantic_search(query, top_k=10)\n",
    "        keyword_results = self.keyword_search(query, top_k=10)\n",
    "\n",
    "        # Normalize scores to 0-1 range\n",
    "        if semantic_results:\n",
    "            max_sem = max(r['score'] for r in semantic_results)\n",
    "            min_sem = min(r['score'] for r in semantic_results)\n",
    "            sem_range = max_sem - min_sem if max_sem != min_sem else 1\n",
    "\n",
    "            for r in semantic_results:\n",
    "                r['normalized_score'] = (r['score'] - min_sem) / sem_range\n",
    "\n",
    "        if keyword_results:\n",
    "            max_kw = max(r['score'] for r in keyword_results)\n",
    "            if max_kw > 0:\n",
    "                for r in keyword_results:\n",
    "                    r['normalized_score'] = r['score'] / max_kw\n",
    "\n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "\n",
    "        for result in semantic_results:\n",
    "            doc = result['document']\n",
    "            combined_scores[doc] = semantic_weight * result['normalized_score']\n",
    "\n",
    "        for result in keyword_results:\n",
    "            doc = result['document']\n",
    "            if doc in combined_scores:\n",
    "                combined_scores[doc] += keyword_weight * result['normalized_score']\n",
    "            else:\n",
    "                combined_scores[doc] = keyword_weight * result['normalized_score']\n",
    "\n",
    "        # Sort by combined score\n",
    "        sorted_results = sorted(\n",
    "            combined_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "\n",
    "        return [{'document': doc, 'hybrid_score': score} for doc, score in sorted_results]\n",
    "\n",
    "# Initialize hybrid searcher\n",
    "hybrid_searcher = HybridSearcher(collection, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4B: Compare Search Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(query):\n",
    "    \"\"\"\n",
    "    Compare semantic, keyword, and hybrid search side-by-side\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEARCH METHOD COMPARISON\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Semantic search\n",
    "    print(\"\\n1. SEMANTIC SEARCH (Vector-based):\")\n",
    "    print(\"-\" * 70)\n",
    "    semantic_results = hybrid_searcher.semantic_search(query, top_k=3)\n",
    "    for i, result in enumerate(semantic_results):\n",
    "        print(f\"[{i+1}] Similarity: {result['score']:.4f} | Distance: {result['distance']:.4f}\")\n",
    "        print(f\"    {result['document'][:80]}...\")\n",
    "\n",
    "    # Keyword search\n",
    "    print(\"\\n2. KEYWORD SEARCH (BM25):\")\n",
    "    print(\"-\" * 70)\n",
    "    keyword_results = hybrid_searcher.keyword_search(query, top_k=3)\n",
    "    if keyword_results:\n",
    "        for i, result in enumerate(keyword_results):\n",
    "            print(f\"[{i+1}] BM25 Score: {result['score']:.4f}\")\n",
    "            print(f\"    {result['document'][:80]}...\")\n",
    "    else:\n",
    "        print(\"No keyword matches found.\")\n",
    "\n",
    "    # Hybrid search\n",
    "    print(\"\\n3. HYBRID SEARCH (50/50 blend):\")\n",
    "    print(\"-\" * 70)\n",
    "    hybrid_results = hybrid_searcher.hybrid_search(\n",
    "        query,\n",
    "        top_k=3,\n",
    "        semantic_weight=0.5,\n",
    "        keyword_weight=0.5\n",
    "    )\n",
    "    for i, result in enumerate(hybrid_results):\n",
    "        print(f\"[{i+1}] Hybrid Score: {result['hybrid_score']:.4f}\")\n",
    "        print(f\"    {result['document'][:80]}...\")\n",
    "\n",
    "# Test with different query types\n",
    "test_queries = [\n",
    "    \"What is deep learning?\",          # Conceptual\n",
    "    \"NLP\",                             # Acronym\n",
    "    \"neural networks layers\",           # Mix\n",
    "    \"teaching computers to understand\" # Paraphrase\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    compare_search_methods(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4C: Tune Hybrid Search Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hybrid_weights(query):\n",
    "    \"\"\"\n",
    "    Test different weight combinations\n",
    "\n",
    "    Args:\n",
    "        query: Search query\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HYBRID SEARCH WEIGHT TUNING\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print('='*70)\n",
    "\n",
    "    weight_combinations = [\n",
    "        (1.0, 0.0, \"100% Semantic\"),\n",
    "        (0.7, 0.3, \"70% Semantic, 30% Keyword\"),\n",
    "        (0.5, 0.5, \"50/50 Balanced\"),\n",
    "        (0.3, 0.7, \"30% Semantic, 70% Keyword\"),\n",
    "        (0.0, 1.0, \"100% Keyword\"),\n",
    "    ]\n",
    "\n",
    "    for sem_w, kw_w, description in weight_combinations:\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        results = hybrid_searcher.hybrid_search(\n",
    "            query,\n",
    "            top_k=3,\n",
    "            semantic_weight=sem_w,\n",
    "            keyword_weight=kw_w\n",
    "        )\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"  [{i+1}] Score: {result['hybrid_score']:.4f}\")\n",
    "            print(f\"      {result['document'][:70]}...\")\n",
    "\n",
    "# Test\n",
    "tune_hybrid_weights(\"deep learning neural networks\")\n",
    "tune_hybrid_weights(\"machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: Hybrid search should combine benefits of both semantic and keyword approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Capstone Project: Production Search System (30 min)\n",
    "\n",
    "**Objective:** Build a complete, production-ready search system with multiple strategies.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Your system must:\n",
    "1. ‚úÖ Support semantic, keyword, and hybrid search\n",
    "2. ‚úÖ Allow configurable Top-K values\n",
    "3. ‚úÖ Support metadata filtering\n",
    "4. ‚úÖ Provide search quality metrics\n",
    "5. ‚úÖ Handle edge cases gracefully\n",
    "6. ‚úÖ Include a user-friendly interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class ProductionSearchSystem:\n",
    "    \"\"\"\n",
    "    Production-ready search system with multiple strategies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection, embedding_model):\n",
    "        \"\"\"Initialize the search system\"\"\"\n",
    "        self.collection = collection\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "        # Initialize hybrid searcher\n",
    "        self.hybrid_searcher = HybridSearcher(collection, embedding_model)\n",
    "\n",
    "        print(\"‚úì Production Search System initialized\")\n",
    "        print(f\"  Collection size: {collection.count()} documents\")\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query,\n",
    "        method=\"hybrid\",\n",
    "        top_k=5,\n",
    "        semantic_weight=0.6,\n",
    "        keyword_weight=0.4,\n",
    "        metadata_filter=None,\n",
    "        distance_threshold=None\n",
    "    ):\n",
    "        \"\"\"Unified search interface\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform search based on method\n",
    "        if method == \"semantic\":\n",
    "            results = self._semantic_search_filtered(\n",
    "                query, top_k, metadata_filter, distance_threshold\n",
    "            )\n",
    "        elif method == \"keyword\":\n",
    "            results = self.hybrid_searcher.keyword_search(query, top_k)\n",
    "        elif method == \"hybrid\":\n",
    "            raw_results = self.hybrid_searcher.hybrid_search(\n",
    "                query, top_k*2, semantic_weight, keyword_weight\n",
    "            )\n",
    "            results = raw_results[:top_k]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = self._calculate_metrics(results, method)\n",
    "\n",
    "        return {\n",
    "            'query': query,\n",
    "            'method': method,\n",
    "            'results': results,\n",
    "            'metrics': {\n",
    "                **metrics,\n",
    "                'search_time_ms': elapsed_time * 1000,\n",
    "                'results_returned': len(results)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _semantic_search_filtered(self, query, top_k, metadata_filter, distance_threshold):\n",
    "        \"\"\"Semantic search with optional filtering\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        search_params = {\n",
    "            'query_embeddings': query_embedding.tolist(),\n",
    "            'n_results': top_k,\n",
    "            'include': ['documents', 'distances', 'metadatas']\n",
    "        }\n",
    "\n",
    "        if metadata_filter:\n",
    "            search_params['where'] = metadata_filter\n",
    "\n",
    "        results = self.collection.query(**search_params)\n",
    "\n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            distance = results['distances'][0][i]\n",
    "\n",
    "            # Apply distance threshold if specified\n",
    "            if distance_threshold and distance > distance_threshold:\n",
    "                continue\n",
    "\n",
    "            formatted.append({\n",
    "                'document': results['documents'][0][i],\n",
    "                'score': 1 / (1 + distance),\n",
    "                'distance': distance,\n",
    "                'metadata': results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "            })\n",
    "\n",
    "        return formatted\n",
    "\n",
    "    def _calculate_metrics(self, results, method):\n",
    "        \"\"\"Calculate quality metrics for results\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                'avg_score': 0.0,\n",
    "                'min_score': 0.0,\n",
    "                'max_score': 0.0\n",
    "            }\n",
    "\n",
    "        if method == \"semantic\":\n",
    "            scores = [r.get('score', 0) for r in results]\n",
    "        elif method == \"keyword\":\n",
    "            scores = [r.get('score', 0) for r in results]\n",
    "        elif method == \"hybrid\":\n",
    "            scores = [r.get('hybrid_score', 0) for r in results]\n",
    "        else:\n",
    "            scores = [0]\n",
    "\n",
    "        return {\n",
    "            'avg_score': np.mean(scores) if scores else 0.0,\n",
    "            'min_score': min(scores) if scores else 0.0,\n",
    "            'max_score': max(scores) if scores else 0.0,\n",
    "            'std_score': np.std(scores) if scores else 0.0\n",
    "        }\n",
    "\n",
    "    def batch_evaluate(self, test_queries):\n",
    "        \"\"\"Evaluate search quality on a batch of queries\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"BATCH EVALUATION: {len(test_queries)} queries\")\n",
    "        print('='*70)\n",
    "\n",
    "        methods = ['semantic', 'keyword', 'hybrid']\n",
    "        results = {method: [] for method in methods}\n",
    "\n",
    "        for query in test_queries:\n",
    "            for method in methods:\n",
    "                result = self.search(query, method=method, top_k=3)\n",
    "                results[method].append(result['metrics'])\n",
    "\n",
    "        # Aggregate metrics\n",
    "        print(f\"\\n{'Method':<15} {'Avg Score':<12} {'Avg Time (ms)':<15}\")\n",
    "        print('-'*70)\n",
    "\n",
    "        for method in methods:\n",
    "            avg_score = np.mean([r['avg_score'] for r in results[method]])\n",
    "            avg_time = np.mean([r['search_time_ms'] for r in results[method]])\n",
    "            print(f\"{method:<15} {avg_score:<12.4f} {avg_time:<15.2f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize the system\n",
    "search_system = ProductionSearchSystem(collection, embedding_model)\n",
    "\n",
    "# Test batch evaluation\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"deep learning neural networks\",\n",
    "    \"computer vision applications\",\n",
    "    \"NLP transformers\",\n",
    "    \"reinforcement learning agents\"\n",
    "]\n",
    "\n",
    "evaluation_results = search_system.batch_evaluate(test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Tasks\n",
    "\n",
    "1. **Run the production system** and test all search methods\n",
    "2. **Add error handling** for edge cases (empty queries, no results)\n",
    "3. **Implement result caching** to speed up repeated queries\n",
    "4. **Add search history tracking**\n",
    "5. **Create a confidence score** that combines distance and score\n",
    "\n",
    "‚úÖ **Checkpoint**: Production system should handle all search methods with quality metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Extension Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Reciprocal Rank Fusion (RRF)\n",
    "def reciprocal_rank_fusion(semantic_results, keyword_results, k=60, top_n=5):\n",
    "    \"\"\"Combine rankings using RRF\"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    # Add semantic rankings\n",
    "    for rank, result in enumerate(semantic_results):\n",
    "        doc = result['document']\n",
    "        scores[doc] = scores.get(doc, 0) + 1 / (k + rank + 1)\n",
    "\n",
    "    # Add keyword rankings\n",
    "    for rank, result in enumerate(keyword_results):\n",
    "        doc = result['document']\n",
    "        scores[doc] = scores.get(doc, 0) + 1 / (k + rank + 1)\n",
    "\n",
    "    # Sort by RRF score\n",
    "    sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    return [{'document': doc, 'rrf_score': score} for doc, score in sorted_results]\n",
    "\n",
    "\n",
    "# Challenge 2: Query Expansion\n",
    "def expand_query(query):\n",
    "    \"\"\"Expand query with synonyms for better recall\"\"\"\n",
    "    expansions = {\n",
    "        'ML': 'machine learning',\n",
    "        'AI': 'artificial intelligence',\n",
    "        'DL': 'deep learning',\n",
    "        'NLP': 'natural language processing'\n",
    "    }\n",
    "\n",
    "    expanded = query\n",
    "    for abbrev, full in expansions.items():\n",
    "        if abbrev in query:\n",
    "            expanded += f\" {full}\"\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "# Challenge 3: Result Re-ranking (requires additional library)\n",
    "# from sentence_transformers import CrossEncoder\n",
    "#\n",
    "# def rerank_results(query, results, top_k=5):\n",
    "#     \"\"\"Re-rank results using cross-encoder\"\"\"\n",
    "#     cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "#     pairs = [[query, r['document']] for r in results]\n",
    "#     scores = cross_encoder.predict(pairs)\n",
    "#     ranked = sorted(zip(results, scores), key=lambda x: x[1], reverse=True)\n",
    "#     return [{'document': r['document'], 'rerank_score': float(s)}\n",
    "#             for r, s in ranked[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "After completing this lab, you should understand:\n",
    "\n",
    "‚úÖ **Semantic Search** - Finding by meaning, not just keywords  \n",
    "‚úÖ **Distance Metrics** - L2, cosine similarity, and interpretation  \n",
    "‚úÖ **Top-K Retrieval** - Balancing quality vs. quantity  \n",
    "‚úÖ **Metadata Filtering** - Refining searches with filters  \n",
    "‚úÖ **Hybrid Search** - Best of semantic + keyword  \n",
    "‚úÖ **Production Considerations** - Speed, quality, edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Troubleshooting\n",
    "\n",
    "**Issue**: \"Collection not found\"  \n",
    "**Solution**: Run Lab 3 first to create the ChromaDB collection, or adjust the collection name.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: \"BM25 returns no results\"  \n",
    "**Solution**: BM25 requires exact word matches. For conceptual queries, use semantic or hybrid search.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: \"Hybrid search seems worse than semantic alone\"  \n",
    "**Solution**: Adjust weights based on query type. Try 70% semantic, 30% keyword for conceptual queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What's Next?\n",
    "\n",
    "You've completed Lab 4! You now have:\n",
    "- ‚úÖ Working semantic search system\n",
    "- ‚úÖ Hybrid search with BM25\n",
    "- ‚úÖ Production-ready search infrastructure\n",
    "- ‚úÖ Quality metrics and evaluation tools\n",
    "\n",
    "**Next Lab**: Lab 5 - Complete RAG Pipeline  \n",
    "Combine retrieval (Lab 4) with generation (LLM) to create a full RAG system!\n",
    "\n",
    "---\n",
    "\n",
    "**Lab 4 Complete!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}