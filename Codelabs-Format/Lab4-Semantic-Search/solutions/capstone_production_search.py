"""
Lab 4 - Capstone: Production Search System
Complete search system combining all Lab 4 concepts

Features:
- Semantic search with ChromaDB
- BM25 keyword search
- Hybrid search with RRF
- Metadata filtering
- Top-K optimization
- Query expansion
- Result reranking
- Performance monitoring
"""

from sentence_transformers import SentenceTransformer
import chromadb
from typing import List, Dict, Optional
import numpy as np
import re
from collections import Counter
import math
import time


class ProductionSearchSystem:
    """
    Production-ready search system

    Capabilities:
    - Multiple search strategies
    - Intelligent query routing
    - Result ranking and filtering
    - Performance tracking
    - Configurable parameters
    """

    def __init__(
        self,
        embedding_model: str = 'all-MiniLM-L6-v2',
        persist_directory: str = "./production_search_db",
        default_strategy: str = 'hybrid'
    ):
        """
        Initialize production search system

        Args:
            embedding_model: Sentence-transformer model name
            persist_directory: ChromaDB storage path
            default_strategy: Default search strategy (semantic, keyword, hybrid)
        """
        # Initialize components
        self.embedding_model = SentenceTransformer(embedding_model)
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.default_strategy = default_strategy

        # Statistics
        self.stats = {
            'total_queries': 0,
            'semantic_queries': 0,
            'keyword_queries': 0,
            'hybrid_queries': 0,
            'avg_query_time': 0.0
        }

        # Query history for analytics
        self.query_history = []

    def create_knowledge_base(
        self,
        documents: List[str],
        metadatas: List[Dict],
        collection_name: str = "production_kb"
    ) -> Dict:
        """
        Create searchable knowledge base

        Args:
            documents: List of documents
            metadatas: List of metadata dicts
            collection_name: Collection name

        Returns:
            dict: Creation result
        """
        print(f"\nüìö Creating knowledge base: {collection_name}")

        # Create collection
        try:
            self.client.delete_collection(collection_name)
        except:
            pass

        collection = self.client.create_collection(collection_name)

        # Generate embeddings
        print(f"   Generating embeddings for {len(documents)} documents...")
        embeddings = self.embedding_model.encode(documents, show_progress_bar=False)

        # Add to collection
        collection.add(
            documents=documents,
            embeddings=embeddings.tolist(),
            metadatas=metadatas,
            ids=[f"doc_{i}" for i in range(len(documents))]
        )

        print(f"   ‚úÖ Knowledge base created ({len(documents)} documents)")

        return {
            "success": True,
            "collection": collection_name,
            "documents": len(documents)
        }

    def _semantic_search(
        self,
        collection,
        query: str,
        n_results: int = 10,
        metadata_filter: Optional[Dict] = None
    ) -> List[Dict]:
        """Semantic search using embeddings"""
        query_embedding = self.embedding_model.encode(query)

        # Build query parameters
        query_params = {
            "query_embeddings": [query_embedding.tolist()],
            "n_results": n_results
        }

        if metadata_filter:
            query_params["where"] = metadata_filter

        results = collection.query(**query_params)

        # Format results
        formatted = []
        for i in range(len(results['documents'][0])):
            formatted.append({
                'rank': i + 1,
                'id': results['ids'][0][i],
                'document': results['documents'][0][i],
                'score': 1 / (1 + results['distances'][0][i]),  # Convert distance to score
                'distance': results['distances'][0][i],
                'metadata': results['metadatas'][0][i],
                'method': 'semantic'
            })

        return formatted

    def _keyword_search(
        self,
        documents: List[str],
        doc_ids: List[str],
        metadatas: List[Dict],
        query: str,
        n_results: int = 10
    ) -> List[Dict]:
        """Keyword search using BM25"""
        # Initialize BM25
        bm25 = BM25Scorer(documents)

        # Score all documents
        scores = [(i, bm25.score(query, i)) for i in range(len(documents))]

        # Sort by score
        scores.sort(key=lambda x: x[1], reverse=True)

        # Format results
        results = []
        for rank, (idx, score) in enumerate(scores[:n_results], 1):
            results.append({
                'rank': rank,
                'id': doc_ids[idx],
                'document': documents[idx],
                'score': score,
                'metadata': metadatas[idx],
                'method': 'keyword'
            })

        return results

    def _hybrid_search(
        self,
        collection,
        query: str,
        n_results: int = 10,
        metadata_filter: Optional[Dict] = None
    ) -> List[Dict]:
        """Hybrid search combining semantic and keyword"""
        # Get semantic results
        semantic_results = self._semantic_search(
            collection,
            query,
            n_results=min(20, n_results * 2),
            metadata_filter=metadata_filter
        )

        # Get all documents for keyword search
        all_docs = collection.get()
        documents = all_docs['documents']
        doc_ids = all_docs['ids']
        metadatas = all_docs['metadatas']

        # Apply metadata filter if specified
        if metadata_filter:
            filtered_data = self._apply_metadata_filter(
                documents, doc_ids, metadatas, metadata_filter
            )
            documents, doc_ids, metadatas = filtered_data

        # Get keyword results
        keyword_results = self._keyword_search(
            documents, doc_ids, metadatas, query,
            n_results=min(20, n_results * 2)
        )

        # Fuse results using RRF
        fused_results = self._reciprocal_rank_fusion(
            semantic_results,
            keyword_results
        )

        # Update method
        for result in fused_results:
            result['method'] = 'hybrid'

        return fused_results[:n_results]

    def _apply_metadata_filter(
        self,
        documents: List[str],
        doc_ids: List[str],
        metadatas: List[Dict],
        metadata_filter: Dict
    ) -> tuple:
        """Apply metadata filter to document lists"""
        filtered_docs = []
        filtered_ids = []
        filtered_metas = []

        for doc, doc_id, meta in zip(documents, doc_ids, metadatas):
            # Simple filter implementation (supports single key-value)
            if all(meta.get(k) == v for k, v in metadata_filter.items()):
                filtered_docs.append(doc)
                filtered_ids.append(doc_id)
                filtered_metas.append(meta)

        return filtered_docs, filtered_ids, filtered_metas

    def _reciprocal_rank_fusion(
        self,
        results1: List[Dict],
        results2: List[Dict],
        k: int = 60
    ) -> List[Dict]:
        """Combine results using Reciprocal Rank Fusion"""
        rrf_scores = {}

        # Add scores from first result set
        for result in results1:
            doc_id = result['id']
            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + (1 / (k + result['rank']))

        # Add scores from second result set
        for result in results2:
            doc_id = result['id']
            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + (1 / (k + result['rank']))

        # Get full document info
        doc_info = {}
        for result in results1 + results2:
            if result['id'] not in doc_info:
                doc_info[result['id']] = result

        # Create ranked list
        ranked = []
        for rank, (doc_id, score) in enumerate(
            sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True), 1
        ):
            info = doc_info[doc_id]
            ranked.append({
                'rank': rank,
                'id': doc_id,
                'document': info['document'],
                'score': score,
                'metadata': info['metadata']
            })

        return ranked

    def search(
        self,
        query: str,
        collection_name: str,
        strategy: Optional[str] = None,
        n_results: int = 5,
        metadata_filter: Optional[Dict] = None
    ) -> Dict:
        """
        Main search interface

        Args:
            query: Search query
            collection_name: Collection to search
            strategy: Search strategy (semantic, keyword, hybrid, auto)
            n_results: Number of results
            metadata_filter: Optional metadata filter

        Returns:
            dict: Search results with metadata
        """
        start_time = time.time()

        # Use default strategy if not specified
        if strategy is None:
            strategy = self.default_strategy

        # Auto strategy: choose based on query
        if strategy == 'auto':
            strategy = self._select_strategy(query)

        # Get collection
        try:
            collection = self.client.get_collection(collection_name)
        except:
            return {
                "success": False,
                "error": f"Collection '{collection_name}' not found"
            }

        # Execute search based on strategy
        if strategy == 'semantic':
            results = self._semantic_search(
                collection, query, n_results, metadata_filter
            )
            self.stats['semantic_queries'] += 1

        elif strategy == 'keyword':
            all_docs = collection.get()
            documents = all_docs['documents']
            doc_ids = all_docs['ids']
            metadatas = all_docs['metadatas']

            if metadata_filter:
                documents, doc_ids, metadatas = self._apply_metadata_filter(
                    documents, doc_ids, metadatas, metadata_filter
                )

            results = self._keyword_search(
                documents, doc_ids, metadatas, query, n_results
            )
            self.stats['keyword_queries'] += 1

        elif strategy == 'hybrid':
            results = self._hybrid_search(
                collection, query, n_results, metadata_filter
            )
            self.stats['hybrid_queries'] += 1

        else:
            return {
                "success": False,
                "error": f"Unknown strategy: {strategy}"
            }

        # Calculate query time
        query_time = time.time() - start_time

        # Update statistics
        self.stats['total_queries'] += 1
        prev_avg = self.stats['avg_query_time']
        total = self.stats['total_queries']
        self.stats['avg_query_time'] = (prev_avg * (total - 1) + query_time) / total

        # Store in history
        self.query_history.append({
            'query': query,
            'strategy': strategy,
            'time': query_time,
            'results': len(results)
        })

        return {
            "success": True,
            "query": query,
            "strategy": strategy,
            "results": results,
            "query_time": query_time,
            "total_results": len(results)
        }

    def _select_strategy(self, query: str) -> str:
        """
        Automatically select best strategy for query

        Rules:
        - Technical terms, exact phrases ‚Üí keyword
        - Conceptual questions ‚Üí semantic
        - General queries ‚Üí hybrid
        """
        # Check for exact phrase (quoted)
        if '"' in query:
            return 'keyword'

        # Check for technical indicators
        technical_patterns = [
            r'\d+\.\d+',  # Version numbers
            r'[A-Z]{2,}',  # Acronyms
            r'\w+\.\w+',  # Module names
        ]

        for pattern in technical_patterns:
            if re.search(pattern, query):
                return 'keyword'

        # Check for question words (conceptual)
        question_words = ['what', 'why', 'how', 'when', 'where', 'who']
        if any(query.lower().startswith(word) for word in question_words):
            return 'semantic'

        # Default to hybrid
        return 'hybrid'

    def display_results(self, search_result: Dict):
        """Display search results"""
        if not search_result['success']:
            print(f"\n‚ùå Error: {search_result['error']}")
            return

        print(f"\nüîç Query: '{search_result['query']}'")
        print(f"   Strategy: {search_result['strategy']}")
        print(f"   Time: {search_result['query_time']:.4f}s")
        print(f"   Results: {search_result['total_results']}")
        print("=" * 70)

        for result in search_result['results']:
            print(f"\n#{result['rank']} - Score: {result['score']:.4f}")
            if 'distance' in result:
                print(f"   Distance: {result['distance']:.4f}")
            print(f"   Method: {result.get('method', 'N/A')}")

            # Metadata
            meta = result['metadata']
            if 'topic' in meta:
                print(f"   Topic: {meta['topic']}")
            if 'difficulty' in meta:
                print(f"   Difficulty: {meta['difficulty']}")

            # Document text
            doc_text = result['document']
            if len(doc_text) > 80:
                doc_text = doc_text[:80] + "..."
            print(f"   Text: {doc_text}")

    def get_statistics(self) -> Dict:
        """Get system statistics"""
        return self.stats.copy()


class BM25Scorer:
    """BM25 scoring algorithm"""

    def __init__(self, corpus: List[str], k1: float = 1.5, b: float = 0.75):
        self.corpus = corpus
        self.k1 = k1
        self.b = b

        self.tokenized_corpus = [self._tokenize(doc) for doc in corpus]
        self.doc_count = len(corpus)
        self.avgdl = sum(len(doc) for doc in self.tokenized_corpus) / self.doc_count
        self.idf = self._calculate_idf()

    def _tokenize(self, text: str) -> List[str]:
        return re.findall(r'\w+', text.lower())

    def _calculate_idf(self) -> Dict[str, float]:
        idf = {}
        for doc in self.tokenized_corpus:
            for term in set(doc):
                idf[term] = idf.get(term, 0) + 1

        for term, df in idf.items():
            idf[term] = math.log((self.doc_count - df + 0.5) / (df + 0.5) + 1.0)

        return idf

    def score(self, query: str, doc_idx: int) -> float:
        score = 0.0
        doc_tokens = self.tokenized_corpus[doc_idx]
        doc_len = len(doc_tokens)
        query_tokens = self._tokenize(query)
        term_freqs = Counter(doc_tokens)

        for term in query_tokens:
            if term not in term_freqs:
                continue

            tf = term_freqs[term]
            idf = self.idf.get(term, 0)
            numerator = tf * (self.k1 + 1)
            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))
            score += idf * (numerator / denominator)

        return score


# ============================================================================
# DEMO
# ============================================================================

def main():
    """Demonstrate production search system"""
    print("=" * 70)
    print("CAPSTONE: PRODUCTION SEARCH SYSTEM")
    print("=" * 70)

    # Initialize system
    search_system = ProductionSearchSystem(
        embedding_model='all-MiniLM-L6-v2',
        persist_directory="./production_search_db",
        default_strategy='hybrid'
    )

    print("\n‚úÖ Production Search System initialized")
    print("   Default strategy: hybrid")
    print("   Embedding model: all-MiniLM-L6-v2")

    # Create knowledge base
    print("\n" + "=" * 70)
    print("STEP 1: CREATE KNOWLEDGE BASE")
    print("=" * 70)

    documents = [
        "Machine learning enables computers to learn from data without explicit programming",
        "Deep learning uses neural networks with multiple layers to process information",
        "Natural language processing helps computers understand and generate human language",
        "Computer vision allows machines to interpret and analyze visual information from images",
        "Reinforcement learning trains agents through rewards and penalties in an environment",
        "Transformers are a neural network architecture that revolutionized NLP with attention mechanisms",
        "Supervised learning uses labeled training data to teach models to make predictions",
        "Unsupervised learning discovers hidden patterns and structures in unlabeled data",
        "Convolutional neural networks (CNNs) are specialized for processing grid-like data such as images",
        "Recurrent neural networks (RNNs) process sequential data like text and time series",
        "Transfer learning applies knowledge from one task to improve performance on related tasks",
        "Generative AI creates new content including text, images, and code using learned patterns"
    ]

    metadatas = [
        {"topic": "ML", "difficulty": "beginner", "category": "fundamentals"},
        {"topic": "DL", "difficulty": "intermediate", "category": "fundamentals"},
        {"topic": "NLP", "difficulty": "intermediate", "category": "application"},
        {"topic": "CV", "difficulty": "intermediate", "category": "application"},
        {"topic": "RL", "difficulty": "advanced", "category": "fundamentals"},
        {"topic": "NLP", "difficulty": "advanced", "category": "architecture"},
        {"topic": "ML", "difficulty": "beginner", "category": "fundamentals"},
        {"topic": "ML", "difficulty": "intermediate", "category": "fundamentals"},
        {"topic": "CV", "difficulty": "advanced", "category": "architecture"},
        {"topic": "NLP", "difficulty": "advanced", "category": "architecture"},
        {"topic": "ML", "difficulty": "intermediate", "category": "technique"},
        {"topic": "AI", "difficulty": "intermediate", "category": "application"}
    ]

    result = search_system.create_knowledge_base(
        documents=documents,
        metadatas=metadatas,
        collection_name="ai_knowledge"
    )

    # Test different search strategies
    print("\n" + "=" * 70)
    print("STEP 2: SEARCH WITH DIFFERENT STRATEGIES")
    print("=" * 70)

    # Semantic search
    print("\n\nüß† Semantic Search")
    print("-" * 70)
    result = search_system.search(
        query="understanding human communication",
        collection_name="ai_knowledge",
        strategy="semantic",
        n_results=3
    )
    search_system.display_results(result)

    # Keyword search
    print("\n\nüî§ Keyword Search (BM25)")
    print("-" * 70)
    result = search_system.search(
        query="neural networks layers",
        collection_name="ai_knowledge",
        strategy="keyword",
        n_results=3
    )
    search_system.display_results(result)

    # Hybrid search
    print("\n\nüîÄ Hybrid Search")
    print("-" * 70)
    result = search_system.search(
        query="learning from visual data",
        collection_name="ai_knowledge",
        strategy="hybrid",
        n_results=3
    )
    search_system.display_results(result)

    # Auto strategy selection
    print("\n" + "=" * 70)
    print("STEP 3: AUTOMATIC STRATEGY SELECTION")
    print("=" * 70)

    test_queries = [
        "What is machine learning?",  # Should use semantic
        "CNNs",  # Should use keyword
        "neural networks for images"  # Should use hybrid
    ]

    for query in test_queries:
        result = search_system.search(
            query=query,
            collection_name="ai_knowledge",
            strategy="auto",
            n_results=2
        )
        print(f"\nüìù Query: '{query}' ‚Üí Strategy: {result['strategy']}")

    # Metadata filtering
    print("\n" + "=" * 70)
    print("STEP 4: METADATA FILTERING")
    print("=" * 70)

    result = search_system.search(
        query="machine learning concepts",
        collection_name="ai_knowledge",
        strategy="hybrid",
        n_results=3,
        metadata_filter={"difficulty": "beginner"}
    )

    print("\nüéØ Search with filter: difficulty=beginner")
    search_system.display_results(result)

    # Performance statistics
    print("\n" + "=" * 70)
    print("STEP 5: PERFORMANCE STATISTICS")
    print("=" * 70)

    stats = search_system.get_statistics()
    print("\nüìä System Statistics:")
    print(f"   Total queries: {stats['total_queries']}")
    print(f"   Semantic queries: {stats['semantic_queries']}")
    print(f"   Keyword queries: {stats['keyword_queries']}")
    print(f"   Hybrid queries: {stats['hybrid_queries']}")
    print(f"   Avg query time: {stats['avg_query_time']:.4f}s")

    # Summary
    print("\n" + "=" * 70)
    print("‚úÖ CAPSTONE COMPLETE!")
    print("=" * 70)

    print("\nüéØ WHAT WE BUILT:")
    print("   ‚úÖ Multi-strategy search (semantic, keyword, hybrid)")
    print("   ‚úÖ Automatic strategy selection")
    print("   ‚úÖ Metadata filtering")
    print("   ‚úÖ Reciprocal Rank Fusion")
    print("   ‚úÖ Performance monitoring")
    print("   ‚úÖ Production-ready API")

    print("\nüí° KEY FEATURES:")
    print("   ‚Üí Semantic search for conceptual queries")
    print("   ‚Üí BM25 for exact term matching")
    print("   ‚Üí Hybrid search for best results")
    print("   ‚Üí Intelligent query routing")
    print("   ‚Üí Comprehensive analytics")

    print("\nüöÄ PRODUCTION READY:")
    print("   ‚Üí Deploy as search microservice")
    print("   ‚Üí Integrate with RAG pipelines")
    print("   ‚Üí Scale with ChromaDB")
    print("   ‚Üí Monitor with built-in statistics")


if __name__ == "__main__":
    main()
