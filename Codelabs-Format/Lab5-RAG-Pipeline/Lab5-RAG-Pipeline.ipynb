{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Complete RAG Pipeline\n",
    "\n",
    "## üõ†Ô∏è Hands-On Lab\n",
    "\n",
    "**Duration:** 75-90 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** Labs 1-4 completed, API keys for OpenAI/Claude\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this lab, you'll have:\n",
    "- ‚úÖ Complete RAG pipeline (Retrieve ‚Üí Augment ‚Üí Generate)\n",
    "- ‚úÖ Integration with multiple LLMs (OpenAI, Claude, Bedrock)\n",
    "- ‚úÖ RAG vs. non-RAG comparison tools\n",
    "- ‚úÖ Multiple prompt strategies\n",
    "- ‚úÖ RAG evaluation framework\n",
    "- ‚úÖ **Capstone**: Production SupportGenie v3.0 with full RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup\n",
    "\n",
    "### Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install openai anthropic python-dotenv sentence-transformers chromadb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setup API Keys\n",
    "\n",
    "Make sure you have a `.env` file with your API keys:\n",
    "\n",
    "```bash\n",
    "# .env\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=your-anthropic-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"‚úì OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"‚úó OpenAI API key not found\")\n",
    "\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    print(\"‚úì Anthropic API key loaded\")\n",
    "else:\n",
    "    print(\"‚úó Anthropic API key not found (optional)\")\n",
    "\n",
    "print(\"\\nNote: You need at least one API key to complete this lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: API keys loaded successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic RAG Pipeline (20 min)\n",
    "\n",
    "**Objective:** Build your first complete RAG pipeline.\n",
    "\n",
    "### Task 1A: Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize embedding model and vector database (from Lab 3)\nprint(\"Initializing components...\")\n\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nclient = chromadb.PersistentClient(path=\"./chroma_db\")\n\n# Use collection from Lab 3\ntry:\n    collection = client.get_collection(name=\"lab3_documents\")\n    print(f\"‚úì Loaded collection: {collection.count()} documents\")\nexcept Exception as e:\n    print(f\"‚úó Collection not found. Please complete Lab 3 first.\")\n    print(f\"Error: {e}\")\n    raise RuntimeError(\"ChromaDB collection 'lab3_documents' not found. Run Lab 3 first.\")\n\n# Initialize LLM client\nopenai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\nprint(\"‚úì OpenAI client initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: Build Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, n_results=3):\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks for a query\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        n_results: Number of chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        tuple: (documents, distances)\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Search vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    return results['documents'][0], results['distances'][0]\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is deep learning?\"\n",
    "docs, distances = retrieve_context(query, n_results=3)\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nRetrieved {len(docs)} chunks:\")\n",
    "for i, (doc, dist) in enumerate(zip(docs, distances)):\n",
    "    print(f\"\\n[{i+1}] Distance: {dist:.4f}\")\n",
    "    print(f\"    {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C: Create RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Create a prompt with retrieved context\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        context_chunks: List of retrieved text chunks\n",
    "\n",
    "    Returns:\n",
    "        str: Complete RAG prompt\n",
    "    \"\"\"\n",
    "    # Format context sections\n",
    "    context_text = \"\"\n",
    "    for i, chunk in enumerate(context_chunks):\n",
    "        context_text += f\"[{i+1}] {chunk}\\n\\n\"\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
    "\n",
    "CONTEXT INFORMATION:\n",
    "{context_text}\n",
    "\n",
    "USER QUESTION:\n",
    "{query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer using ONLY the information from the context above\n",
    "- Cite which context section(s) you used (e.g., [1], [2])\n",
    "- If the context doesn't contain enough information, say so clearly\n",
    "- Be concise and accurate\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Test prompt creation\n",
    "rag_prompt = create_rag_prompt(query, docs)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAG PROMPT:\")\n",
    "print(\"=\"*70)\n",
    "print(rag_prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1D: Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_openai(query, n_results=3, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline using OpenAI\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        n_results: Number of context chunks to retrieve\n",
    "        model: OpenAI model to use\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer, context_chunks, metadata)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RAG PIPELINE: {query}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Step 1: Retrieve\n",
    "    print(\"\\n[Step 1/3] Retrieving relevant context...\")\n",
    "    context_chunks, distances = retrieve_context(query, n_results)\n",
    "\n",
    "    print(f\"Retrieved {len(context_chunks)} chunks:\")\n",
    "    for i, dist in enumerate(distances):\n",
    "        print(f\"  [{i+1}] Distance: {dist:.4f}\")\n",
    "\n",
    "    # Step 2: Augment (create prompt)\n",
    "    print(\"\\n[Step 2/3] Creating RAG prompt...\")\n",
    "    prompt = create_rag_prompt(query, context_chunks)\n",
    "\n",
    "    # Step 3: Generate\n",
    "    print(f\"\\n[Step 3/3] Generating answer with {model}...\")\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,  # Low temperature for factual responses\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'model': model,\n",
    "        'n_retrieved': len(context_chunks),\n",
    "        'avg_distance': np.mean(distances),\n",
    "        'time_seconds': elapsed_time,\n",
    "        'tokens_used': response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RAG ANSWER:\")\n",
    "    print('='*70)\n",
    "    print(answer)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Metadata: {metadata}\")\n",
    "    print('='*70)\n",
    "\n",
    "    return answer, context_chunks, metadata\n",
    "\n",
    "# Test the complete pipeline\n",
    "answer, context, metadata = rag_pipeline_openai(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: You should see a complete RAG response with retrieved context and generated answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: RAG vs. Non-RAG Comparison (15 min)\n",
    "\n",
    "**Objective:** Understand the difference RAG makes.\n",
    "\n",
    "### Task 2A: Non-RAG Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_without_rag(query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generate answer WITHOUT retrieval (baseline)\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        model: OpenAI model\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer, metadata)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    metadata = {\n",
    "        'model': model,\n",
    "        'time_seconds': elapsed_time,\n",
    "        'tokens_used': response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "    return answer, metadata\n",
    "\n",
    "# Test non-RAG\n",
    "no_rag_answer, no_rag_meta = generate_without_rag(\"What is machine learning?\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NON-RAG ANSWER (No Context):\")\n",
    "print(\"=\"*70)\n",
    "print(no_rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rag_vs_no_rag(query):\n",
    "    \"\"\"\n",
    "    Compare RAG and non-RAG responses side-by-side\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"COMPARISON: RAG vs NON-RAG\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "\n",
    "    # Non-RAG\n",
    "    print(\"=\"*70)\n",
    "    print(\"WITHOUT RAG (LLM Only - No Retrieved Context)\")\n",
    "    print(\"=\"*70)\n",
    "    no_rag_answer, no_rag_meta = generate_without_rag(query)\n",
    "    print(no_rag_answer)\n",
    "    print(f\"\\nTokens used: {no_rag_meta['tokens_used']}\")\n",
    "\n",
    "    # RAG\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WITH RAG (LLM + Retrieved Context)\")\n",
    "    print(\"=\"*70)\n",
    "    rag_answer, context, rag_meta = rag_pipeline_openai(query)\n",
    "\n",
    "    # Analysis\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(\"ANALYSIS:\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    print(f\"Non-RAG tokens: {no_rag_meta['tokens_used']}\")\n",
    "    print(f\"RAG tokens: {rag_meta['tokens_used']}\")\n",
    "    print(f\"Token difference: {rag_meta['tokens_used'] - no_rag_meta['tokens_used']}\")\n",
    "    print(f\"\\nRAG retrieved {rag_meta['n_retrieved']} chunks\")\n",
    "    print(f\"Average chunk relevance: {rag_meta['avg_distance']:.4f}\")\n",
    "\n",
    "    print(\"\\nKey Differences to Observe:\")\n",
    "    print(\"  - Is RAG answer more specific to our documents?\")\n",
    "    print(\"  - Does RAG answer cite sources?\")\n",
    "    print(\"  - Is non-RAG answer more general?\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about deep learning\",\n",
    "    \"What is computer vision?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    compare_rag_vs_no_rag(query)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2C: When RAG Helps Most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_rag_value(specific_query, general_query):\n",
    "    \"\"\"\n",
    "    Show when RAG provides most value\n",
    "\n",
    "    Args:\n",
    "        specific_query: Question about specific content in your documents\n",
    "        general_query: General knowledge question\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"WHEN DOES RAG HELP MOST?\")\n",
    "    print('='*70)\n",
    "\n",
    "    print(\"\\n--- Test 1: Specific Query (RAG should excel) ---\")\n",
    "    print(f\"Query: {specific_query}\")\n",
    "    rag_answer1, _, _ = rag_pipeline_openai(specific_query, n_results=2)\n",
    "\n",
    "    print(\"\\n--- Test 2: General Query (RAG may not add much) ---\")\n",
    "    print(f\"Query: {general_query}\")\n",
    "    rag_answer2, _, _ = rag_pipeline_openai(general_query, n_results=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INSIGHT:\")\n",
    "    print(\"RAG adds most value when:\")\n",
    "    print(\"  ‚úì Question is about specific document content\")\n",
    "    print(\"  ‚úì Information is not in LLM's training data\")\n",
    "    print(\"  ‚úì Need to cite sources\")\n",
    "    print(\"  ‚úì Domain-specific or recent information\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Test\n",
    "demonstrate_rag_value(\n",
    "    specific_query=\"What specific applications of computer vision are mentioned?\",\n",
    "    general_query=\"What is Python programming?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: You should see clear differences between RAG and non-RAG responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Advanced Prompt Strategies (15 min)\n",
    "\n",
    "**Objective:** Experiment with different RAG prompt formats.\n",
    "\n",
    "### Task 3A: Multiple Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPromptTemplates:\n",
    "    \"\"\"Collection of different RAG prompt strategies\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def basic_template(query, context_chunks):\n",
    "        \"\"\"Simple, straightforward prompt\"\"\"\n",
    "        context = \"\\n\\n\".join([f\"[{i+1}] {chunk}\"\n",
    "                               for i, chunk in enumerate(context_chunks)])\n",
    "\n",
    "        return f\"\"\"Answer the question using the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def detailed_template(query, context_chunks):\n",
    "        \"\"\"Detailed with explicit instructions\"\"\"\n",
    "        context = \"\\n\\n\".join([f\"[{i+1}] {chunk}\"\n",
    "                               for i, chunk in enumerate(context_chunks)])\n",
    "\n",
    "        return f\"\"\"You are a helpful AI assistant. Answer the user's question based ONLY on the provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Use ONLY information from the context above\n",
    "2. Cite sources using [1], [2], [3] format\n",
    "3. If context is insufficient, state what's missing\n",
    "4. Be concise and accurate\n",
    "5. Do not use your general knowledge\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def structured_template(query, context_chunks):\n",
    "        \"\"\"Structured output format\"\"\"\n",
    "        context = \"\\n\\n\".join([f\"[{i+1}] {chunk}\"\n",
    "                               for i, chunk in enumerate(context_chunks)])\n",
    "\n",
    "        return f\"\"\"Answer the question using the context provided. Structure your response.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "Provide your answer in this format:\n",
    "**Summary:** [One sentence answer]\n",
    "**Details:** [Detailed explanation with citations]\n",
    "**Sources:** [List which context sections were used]\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def conversational_template(query, context_chunks):\n",
    "        \"\"\"Friendly, conversational tone\"\"\"\n",
    "        context = \"\\n\\n\".join([f\"[{i+1}] {chunk}\"\n",
    "                               for i, chunk in enumerate(context_chunks)])\n",
    "\n",
    "        return f\"\"\"You're a friendly AI assistant helping users understand information from documents.\n",
    "\n",
    "Here's what I found in our documents:\n",
    "{context}\n",
    "\n",
    "The user asked: \"{query}\"\n",
    "\n",
    "Based on what I found, here's what I can tell you:\"\"\"\n",
    "\n",
    "# Test different templates\n",
    "def compare_prompt_templates(query):\n",
    "    \"\"\"Compare different prompt templates\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT TEMPLATE COMPARISON\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Get context\n",
    "    context_chunks, _ = retrieve_context(query, n_results=3)\n",
    "\n",
    "    templates = {\n",
    "        \"Basic\": RAGPromptTemplates.basic_template,\n",
    "        \"Detailed\": RAGPromptTemplates.detailed_template,\n",
    "        \"Structured\": RAGPromptTemplates.structured_template,\n",
    "        \"Conversational\": RAGPromptTemplates.conversational_template\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, template_func in templates.items():\n",
    "        print(f\"\\n--- Template: {name} ---\")\n",
    "        prompt = template_func(query, context_chunks)\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        results[name] = answer\n",
    "\n",
    "        print(answer)\n",
    "        print()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test\n",
    "template_results = compare_prompt_templates(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3B: Context Formatting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_with_metadata(chunks, metadatas=None):\n",
    "    \"\"\"\n",
    "    Format context with rich metadata\n",
    "\n",
    "    Args:\n",
    "        chunks: List of text chunks\n",
    "        metadatas: Optional list of metadata dicts\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted context\n",
    "    \"\"\"\n",
    "    formatted = \"\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        formatted += f\"\\n{'='*60}\\n\"\n",
    "        formatted += f\"CONTEXT SECTION [{i+1}]\\n\"\n",
    "\n",
    "        if metadatas and i < len(metadatas):\n",
    "            meta = metadatas[i]\n",
    "            if 'source' in meta:\n",
    "                formatted += f\"Source: {meta['source']}\\n\"\n",
    "            if 'chunk_index' in meta:\n",
    "                formatted += f\"Section: {meta['chunk_index']}\\n\"\n",
    "\n",
    "        formatted += f\"{'='*60}\\n\"\n",
    "        formatted += chunk + \"\\n\"\n",
    "\n",
    "    return formatted\n",
    "\n",
    "# Test rich context formatting\n",
    "query = \"What is machine learning?\"\n",
    "chunks, _ = retrieve_context(query, n_results=3)\n",
    "\n",
    "# Get metadata if available\n",
    "results_with_meta = collection.query(\n",
    "    query_embeddings=embedding_model.encode([query]).tolist(),\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "rich_context = format_context_with_metadata(\n",
    "    results_with_meta['documents'][0],\n",
    "    results_with_meta['metadatas'][0]\n",
    ")\n",
    "\n",
    "print(\"Rich Context Format:\")\n",
    "print(rich_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: Different prompt templates should produce different response styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Multi-LLM Support (15 min)\n",
    "\n",
    "**Objective:** Support multiple LLM providers.\n",
    "\n",
    "### Task 4A: Claude Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_claude(query, n_results=3, model=\"claude-3-5-haiku-20241022\"):\n",
    "    \"\"\"\n",
    "    RAG pipeline using Claude (Anthropic)\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        n_results: Number of chunks to retrieve\n",
    "        model: Claude model to use\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer, context_chunks, metadata)\n",
    "    \"\"\"\n",
    "    # Check if API key is available\n",
    "    if not os.getenv('ANTHROPIC_API_KEY'):\n",
    "        print(\"Anthropic API key not found. Skipping Claude example.\")\n",
    "        return None, None, None\n",
    "\n",
    "    anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RAG with CLAUDE: {query}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Retrieve\n",
    "    print(\"\\n[Step 1/3] Retrieving context...\")\n",
    "    context_chunks, distances = retrieve_context(query, n_results)\n",
    "\n",
    "    # Create prompt\n",
    "    print(\"[Step 2/3] Creating prompt...\")\n",
    "    prompt = create_rag_prompt(query, context_chunks)\n",
    "\n",
    "    # Generate with Claude\n",
    "    print(f\"[Step 3/3] Generating with Claude {model}...\")\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=500,\n",
    "        temperature=0.3,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer = response.content[0].text\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    metadata = {\n",
    "        'model': model,\n",
    "        'n_retrieved': len(context_chunks),\n",
    "        'avg_distance': np.mean(distances),\n",
    "        'time_seconds': elapsed_time,\n",
    "        'tokens_used': response.usage.input_tokens + response.usage.output_tokens\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CLAUDE ANSWER:\")\n",
    "    print('='*70)\n",
    "    print(answer)\n",
    "    print(f\"\\nMetadata: {metadata}\")\n",
    "\n",
    "    return answer, context_chunks, metadata\n",
    "\n",
    "# Test Claude (uncomment if you have Claude API key)\n",
    "# claude_answer, claude_context, claude_meta = rag_pipeline_claude(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4B: Unified RAG Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedRAGPipeline:\n",
    "    \"\"\"\n",
    "    Unified RAG interface supporting multiple LLM providers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection, embedding_model):\n",
    "        \"\"\"Initialize with vector database and embedding model\"\"\"\n",
    "        self.collection = collection\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "        # Initialize LLM clients\n",
    "        self.openai_client = None\n",
    "        self.anthropic_client = None\n",
    "\n",
    "        if os.getenv('OPENAI_API_KEY'):\n",
    "            self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "            print(\"‚úì OpenAI client initialized\")\n",
    "\n",
    "        if os.getenv('ANTHROPIC_API_KEY'):\n",
    "            self.anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "            print(\"‚úì Claude client initialized\")\n",
    "\n",
    "    def retrieve(self, query, n_results=3, metadata_filter=None):\n",
    "        \"\"\"Retrieve relevant context\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        search_params = {\n",
    "            'query_embeddings': query_embedding.tolist(),\n",
    "            'n_results': n_results,\n",
    "            'include': ['documents', 'distances', 'metadatas']\n",
    "        }\n",
    "\n",
    "        if metadata_filter:\n",
    "            search_params['where'] = metadata_filter\n",
    "\n",
    "        results = self.collection.query(**search_params)\n",
    "\n",
    "        return {\n",
    "            'documents': results['documents'][0],\n",
    "            'distances': results['distances'][0],\n",
    "            'metadatas': results['metadatas'][0] if results['metadatas'] else []\n",
    "        }\n",
    "\n",
    "    def generate_openai(self, prompt, model=\"gpt-4o-mini\", temperature=0.3):\n",
    "        \"\"\"Generate with OpenAI\"\"\"\n",
    "        if not self.openai_client:\n",
    "            raise ValueError(\"OpenAI client not initialized\")\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'answer': response.choices[0].message.content,\n",
    "            'tokens': response.usage.total_tokens,\n",
    "            'model': model\n",
    "        }\n",
    "\n",
    "    def generate_claude(self, prompt, model=\"claude-3-5-haiku-20241022\", temperature=0.3):\n",
    "        \"\"\"Generate with Claude\"\"\"\n",
    "        if not self.anthropic_client:\n",
    "            raise ValueError(\"Claude client not initialized\")\n",
    "\n",
    "        response = self.anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=500,\n",
    "            temperature=temperature,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'answer': response.content[0].text,\n",
    "            'tokens': response.usage.input_tokens + response.usage.output_tokens,\n",
    "            'model': model\n",
    "        }\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question,\n",
    "        provider=\"openai\",\n",
    "        model=None,\n",
    "        n_results=3,\n",
    "        template=\"detailed\",\n",
    "        temperature=0.3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Complete RAG query\n",
    "\n",
    "        Args:\n",
    "            question: User's question\n",
    "            provider: 'openai' or 'claude'\n",
    "            model: Specific model (or use default)\n",
    "            n_results: Number of chunks to retrieve\n",
    "            template: Prompt template to use\n",
    "            temperature: LLM temperature\n",
    "\n",
    "        Returns:\n",
    "            dict: Complete response with answer and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Step 1: Retrieve\n",
    "        retrieval_results = self.retrieve(question, n_results)\n",
    "\n",
    "        # Step 2: Create prompt\n",
    "        template_func = getattr(RAGPromptTemplates, f\"{template}_template\")\n",
    "        prompt = template_func(question, retrieval_results['documents'])\n",
    "\n",
    "        # Step 3: Generate\n",
    "        if provider == \"openai\":\n",
    "            if model is None:\n",
    "                model = \"gpt-4o-mini\"\n",
    "            generation_result = self.generate_openai(prompt, model, temperature)\n",
    "\n",
    "        elif provider == \"claude\":\n",
    "            if model is None:\n",
    "                model = \"claude-3-5-haiku-20241022\"\n",
    "            generation_result = self.generate_claude(prompt, model, temperature)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': generation_result['answer'],\n",
    "            'provider': provider,\n",
    "            'model': generation_result['model'],\n",
    "            'retrieved_chunks': retrieval_results['documents'],\n",
    "            'chunk_distances': retrieval_results['distances'],\n",
    "            'tokens_used': generation_result['tokens'],\n",
    "            'time_seconds': elapsed_time\n",
    "        }\n",
    "\n",
    "# Initialize unified pipeline\n",
    "unified_rag = UnifiedRAGPipeline(collection, embedding_model)\n",
    "\n",
    "# Test with OpenAI\n",
    "result_openai = unified_rag.query(\n",
    "    \"What is machine learning?\",\n",
    "    provider=\"openai\",\n",
    "    template=\"detailed\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Provider: {result_openai['provider']}\")\n",
    "print(f\"Model: {result_openai['model']}\")\n",
    "print(f\"Time: {result_openai['time_seconds']:.2f}s\")\n",
    "print(f\"Tokens: {result_openai['tokens_used']}\")\n",
    "print(f\"\\nAnswer:\\n{result_openai['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: Unified interface should work with multiple LLM providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Capstone Project: SupportGenie v3.0 with RAG (30 min)\n",
    "\n",
    "**Objective:** Build a production-ready RAG-powered support system.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Your system must:\n",
    "1. ‚úÖ Support semantic search with ChromaDB\n",
    "2. ‚úÖ Integrate with multiple LLMs (OpenAI, Claude)\n",
    "3. ‚úÖ Handle conversation history\n",
    "4. ‚úÖ Provide source citations\n",
    "5. ‚úÖ Track metrics (latency, tokens, relevance)\n",
    "6. ‚úÖ Gracefully handle errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportGenieV3:\n",
    "    \"\"\"\n",
    "    Production RAG-powered support system\n",
    "    Version 3.0 - Complete with retrieval-augmented generation\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are SupportGenie, an AI customer support assistant.\n",
    "\n",
    "CAPABILITIES:\n",
    "- Answer questions using provided knowledge base context\n",
    "- Cite sources for all information\n",
    "- Admit when information is not available\n",
    "\n",
    "RESPONSE GUIDELINES:\n",
    "1. Use ONLY the provided context to answer\n",
    "2. Cite context sections used: [1], [2], etc.\n",
    "3. If context is insufficient, say so clearly\n",
    "4. Be professional, helpful, and concise\n",
    "5. Structure answers clearly\n",
    "\n",
    "TONE: Professional, helpful, and empathetic\"\"\"\n",
    "\n",
    "    def __init__(self, collection, embedding_model, llm_provider=\"openai\"):\n",
    "        \"\"\"Initialize SupportGenie v3.0\"\"\"\n",
    "        self.collection = collection\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_provider = llm_provider\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # Initialize LLM client\n",
    "        if llm_provider == \"openai\":\n",
    "            self.llm_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "            self.model = \"gpt-4o-mini\"\n",
    "        elif llm_provider == \"claude\":\n",
    "            self.llm_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "            self.model = \"claude-3-5-haiku-20241022\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {llm_provider}\")\n",
    "\n",
    "        print(f\"‚úì SupportGenie v3.0 initialized\")\n",
    "        print(f\"  LLM Provider: {llm_provider}\")\n",
    "        print(f\"  Model: {self.model}\")\n",
    "        print(f\"  Knowledge Base: {collection.count()} documents\")\n",
    "\n",
    "    def retrieve_context(self, query, n_results=3):\n",
    "        \"\"\"Retrieve relevant context from knowledge base\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas']\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'documents': results['documents'][0],\n",
    "            'distances': results['distances'][0],\n",
    "            'metadatas': results['metadatas'][0] if results['metadatas'] else []\n",
    "        }\n",
    "\n",
    "    def create_rag_prompt(self, query, context_data):\n",
    "        \"\"\"Create RAG prompt with context\"\"\"\n",
    "        # Format context\n",
    "        context_text = \"\"\n",
    "        for i, (doc, meta) in enumerate(zip(\n",
    "            context_data['documents'],\n",
    "            context_data['metadatas']\n",
    "        )):\n",
    "            context_text += f\"\\n[{i+1}] \"\n",
    "            if meta and 'source' in meta:\n",
    "                context_text += f\"(Source: {meta['source']}) \"\n",
    "            context_text += doc + \"\\n\"\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"{self.SYSTEM_PROMPT}\n",
    "\n",
    "KNOWLEDGE BASE CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "CUSTOMER QUESTION:\n",
    "{query}\n",
    "\n",
    "YOUR RESPONSE:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        \"\"\"Generate response using configured LLM\"\"\"\n",
    "        if self.llm_provider == \"openai\":\n",
    "            response = self.llm_client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.3,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return {\n",
    "                'answer': response.choices[0].message.content,\n",
    "                'tokens': response.usage.total_tokens\n",
    "            }\n",
    "\n",
    "        elif self.llm_provider == \"claude\":\n",
    "            response = self.llm_client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=500,\n",
    "                temperature=0.3,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return {\n",
    "                'answer': response.content[0].text,\n",
    "                'tokens': response.usage.input_tokens + response.usage.output_tokens\n",
    "            }\n",
    "\n",
    "    def chat(self, user_message, n_results=3):\n",
    "        \"\"\"Main chat interface with RAG\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Step 1: Retrieve context\n",
    "            context_data = self.retrieve_context(user_message, n_results)\n",
    "\n",
    "            # Step 2: Create RAG prompt\n",
    "            prompt = self.create_rag_prompt(user_message, context_data)\n",
    "\n",
    "            # Step 3: Generate response\n",
    "            generation_result = self.generate_response(prompt)\n",
    "\n",
    "            # Calculate metrics\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_relevance = np.mean(context_data['distances'])\n",
    "\n",
    "            # Store in history\n",
    "            interaction = {\n",
    "                'user_message': user_message,\n",
    "                'assistant_response': generation_result['answer'],\n",
    "                'context_used': context_data['documents'],\n",
    "                'relevance_scores': context_data['distances'],\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            self.conversation_history.append(interaction)\n",
    "\n",
    "            return {\n",
    "                'success': True,\n",
    "                'answer': generation_result['answer'],\n",
    "                'metadata': {\n",
    "                    'retrieved_chunks': len(context_data['documents']),\n",
    "                    'avg_relevance': avg_relevance,\n",
    "                    'tokens_used': generation_result['tokens'],\n",
    "                    'latency_ms': elapsed_time * 1000,\n",
    "                    'provider': self.llm_provider,\n",
    "                    'model': self.model\n",
    "                },\n",
    "                'sources': context_data['metadatas']\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'answer': \"I'm sorry, I encountered an error processing your request. Please try again.\"\n",
    "            }\n",
    "\n",
    "    def display_response(self, response):\n",
    "        \"\"\"Display response in a user-friendly format\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        if response['success']:\n",
    "            print(\"SupportGenie:\")\n",
    "            print('-'*70)\n",
    "            print(response['answer'])\n",
    "            print()\n",
    "            print(\"Metadata:\")\n",
    "            for key, value in response['metadata'].items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "            if response['sources']:\n",
    "                print(\"\\nSources Used:\")\n",
    "                for i, source in enumerate(response['sources']):\n",
    "                    if source and 'source' in source:\n",
    "                        print(f\"  [{i+1}] {source['source']}\")\n",
    "        else:\n",
    "            print(f\"Error: {response['error']}\")\n",
    "        print('='*70)\n",
    "\n",
    "# Initialize SupportGenie v3.0\n",
    "genie = SupportGenieV3(\n",
    "    collection=collection,\n",
    "    embedding_model=embedding_model,\n",
    "    llm_provider=\"openai\"  # or \"claude\"\n",
    ")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about deep learning\",\n",
    "    \"What are some applications of AI?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING SUPPORTGENIE V3.0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    response = genie.chat(query)\n",
    "    genie.display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Tasks\n",
    "\n",
    "1. **Test SupportGenie v3.0** with various queries\n",
    "2. **Add error handling** for edge cases (no API key, empty query, etc.)\n",
    "3. **Implement conversation context** - use previous messages in retrieval\n",
    "4. **Add confidence scoring** - indicate how confident the answer is\n",
    "5. **Create a feedback system** - allow users to rate responses\n",
    "\n",
    "‚úÖ **Checkpoint**: SupportGenie v3.0 should provide RAG-powered responses with source citations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Extension Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Query Rewriting\n",
    "def rewrite_query(original_query):\n",
    "    \"\"\"Rewrite query for better retrieval\"\"\"\n",
    "    rewrite_prompt = f\"\"\"Rewrite this question to be more effective for document search.\n",
    "Make it more specific and include key terms.\n",
    "\n",
    "Original: {original_query}\n",
    "Rewritten:\"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": rewrite_prompt}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# Challenge 2: Multi-Query RAG\n",
    "def multi_query_rag(original_query, n_variations=3):\n",
    "    \"\"\"Generate multiple query variations and retrieve for each\"\"\"\n",
    "    # TODO: Implement multi-query retrieval\n",
    "    pass\n",
    "\n",
    "\n",
    "# Challenge 3: RAG Evaluation Framework\n",
    "def evaluate_rag_system(test_cases):\n",
    "    \"\"\"Evaluate RAG system on test cases\"\"\"\n",
    "    results = {\n",
    "        'accuracy': [],\n",
    "        'citation_accuracy': [],\n",
    "        'latency': []\n",
    "    }\n",
    "\n",
    "    for question, expected_info, expected_sources in test_cases:\n",
    "        # Run RAG\n",
    "        answer, context, metadata = rag_pipeline_openai(question)\n",
    "\n",
    "        # Check accuracy\n",
    "        accuracy = 1.0 if expected_info.lower() in answer.lower() else 0.0\n",
    "        results['accuracy'].append(accuracy)\n",
    "\n",
    "        # Check citations\n",
    "        has_citations = '[1]' in answer or '[2]' in answer or '[3]' in answer\n",
    "        results['citation_accuracy'].append(1.0 if has_citations else 0.0)\n",
    "\n",
    "        # Track latency\n",
    "        results['latency'].append(metadata['time_seconds'])\n",
    "\n",
    "    return {\n",
    "        'avg_accuracy': np.mean(results['accuracy']),\n",
    "        'citation_rate': np.mean(results['citation_accuracy']),\n",
    "        'avg_latency': np.mean(results['latency'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "After completing this lab, you should understand:\n",
    "\n",
    "‚úÖ **RAG Pipeline** - Retrieve ‚Üí Augment ‚Üí Generate flow  \n",
    "‚úÖ **Prompt Engineering** - How to structure RAG prompts effectively  \n",
    "‚úÖ **Multi-LLM Integration** - Support OpenAI, Claude, Bedrock  \n",
    "‚úÖ **RAG vs. Non-RAG** - When RAG adds value  \n",
    "‚úÖ **Source Citations** - How to make LLMs cite sources  \n",
    "‚úÖ **Production Considerations** - Error handling, metrics, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What's Next?\n",
    "\n",
    "You've completed Lab 5! You now have:\n",
    "- ‚úÖ Complete RAG pipeline\n",
    "- ‚úÖ Multi-LLM support\n",
    "- ‚úÖ Production-ready SupportGenie v3.0\n",
    "- ‚úÖ RAG evaluation tools\n",
    "\n",
    "**Next Lab**: Lab 6 - AI Agents & Tool Calling  \n",
    "Learn how to give your RAG system the ability to take actions and use tools!\n",
    "\n",
    "---\n",
    "\n",
    "**Lab 5 Complete!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}