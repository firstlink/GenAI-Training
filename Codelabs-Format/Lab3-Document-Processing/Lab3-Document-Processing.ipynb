{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Document Processing & Embeddings\n",
    "\n",
    "## üõ†Ô∏è Hands-On Lab\n",
    "\n",
    "**Duration:** 60-90 minutes  \n",
    "**Difficulty:** Beginner to Intermediate  \n",
    "**Prerequisites:** Lab 1 & Lab 2 completed\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this lab, you'll have:\n",
    "- ‚úÖ Document loader for TXT and PDF files\n",
    "- ‚úÖ Text chunking system with multiple strategies\n",
    "- ‚úÖ Embedding generation pipeline\n",
    "- ‚úÖ Vector database (ChromaDB) with stored documents\n",
    "- ‚úÖ Similarity comparison tools\n",
    "- ‚úÖ **Capstone**: Complete document processing system for SupportGenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup\n",
    "\n",
    "### Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers chromadb pypdf langchain langchain-community scikit-learn numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Sample Document\n",
    "\n",
    "We'll create a sample document about AI to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample document\n",
    "sample_text = \"\"\"Artificial Intelligence (AI) is transforming industries worldwide. Machine learning, a subset of AI, enables computers to learn from data without explicit programming.\n",
    "\n",
    "Deep learning is a specialized form of machine learning that uses neural networks with multiple layers. These networks can process complex patterns in images, text, and audio.\n",
    "\n",
    "Natural Language Processing (NLP) is another important branch of AI that focuses on the interaction between computers and human language. Applications include chatbots, translation services, and sentiment analysis.\n",
    "\n",
    "Computer vision enables machines to interpret and understand visual information from the world. Self-driving cars and facial recognition systems rely heavily on computer vision technologies.\n",
    "\n",
    "Reinforcement learning is a type of machine learning where agents learn to make decisions by interacting with an environment. It has been successfully applied to game playing, robotics, and resource management.\n",
    "\n",
    "AI ethics is becoming increasingly important as AI systems are deployed in critical areas like healthcare, criminal justice, and employment. Issues include bias in algorithms, privacy concerns, and the need for transparency and accountability.\n",
    "\n",
    "The future of AI holds tremendous potential for solving complex problems in climate change, disease diagnosis, and scientific research. However, it also raises important questions about job displacement and the role of humans in an AI-driven world.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('sample_document.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"‚úÖ Created sample_document.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Document Loading (10 min)\n",
    "\n",
    "**Objective:** Load documents from different file formats.\n",
    "\n",
    "### Task 1A: Load Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a text file and return its content\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the text file\n",
    "\n",
    "    Returns:\n",
    "        str: File content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "document_path = 'sample_document.txt'\n",
    "document_content = load_text_file(document_path)\n",
    "\n",
    "if document_content:\n",
    "    print(\"‚úì Document loaded successfully!\")\n",
    "    print(f\"  Length: {len(document_content)} characters\")\n",
    "    print(f\"  Words: ~{len(document_content.split())} words\")\n",
    "    print(f\"\\nFirst 200 characters:\")\n",
    "    print(document_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: Load PDF File (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a PDF file and extract text from all pages\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from all pages\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text\n",
    "            print(f\"  Extracted page {page_num + 1}: {len(page_text)} characters\")\n",
    "\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file {file_path} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with your own PDF (optional)\n",
    "# pdf_content = load_pdf_file('sample.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C: Document Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(content):\n",
    "    \"\"\"Print useful statistics about a document\"\"\"\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    words = content.split()\n",
    "    paragraphs = [p for p in content.split('\\n\\n') if p.strip()]\n",
    "\n",
    "    print(\"\\n=== DOCUMENT STATISTICS ===\")\n",
    "    print(f\"Characters: {len(content):,}\")\n",
    "    print(f\"Words: {len(words):,}\")\n",
    "    print(f\"Lines: {len(lines):,}\")\n",
    "    print(f\"Paragraphs: {len(paragraphs):,}\")\n",
    "    print(f\"Avg words per paragraph: {len(words) / len(paragraphs):.1f}\")\n",
    "    print(f\"Avg characters per word: {len(content) / len(words):.1f}\")\n",
    "\n",
    "# Analyze the document\n",
    "analyze_document(document_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: You should see detailed document statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Text Chunking Strategies (20 min)\n",
    "\n",
    "**Objective:** Implement and compare different chunking strategies.\n",
    "\n",
    "### Task 2A: Fixed-Size Character Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_characters(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into fixed-size chunks with overlap\n",
    "\n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Number of overlapping characters between chunks\n",
    "\n",
    "    Returns:\n",
    "        list: Text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Move start forward by (chunk_size - overlap)\n",
    "        start += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Test character chunking\n",
    "char_chunks = chunk_by_characters(\n",
    "    document_content,\n",
    "    chunk_size=300,\n",
    "    overlap=50\n",
    ")\n",
    "\n",
    "print(\"\\n=== CHARACTER-BASED CHUNKING ===\")\n",
    "print(f\"Created {len(char_chunks)} chunks\\n\")\n",
    "\n",
    "for i, chunk in enumerate(char_chunks[:3]):  # Show first 3\n",
    "    print(f\"--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:** Some chunks might be split mid-sentence or mid-word.\n",
    "\n",
    "### Task 2B: Sentence-Based Chunking (Better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_smartly(text, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split text using LangChain's intelligent splitter\n",
    "    Tries to split on paragraphs, then sentences, then words\n",
    "\n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        chunk_size: Target size for each chunk\n",
    "        chunk_overlap: Overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        list: Text chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Test smart chunking\n",
    "smart_chunks = chunk_smartly(\n",
    "    document_content,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "print(\"\\n=== SMART (RECURSIVE) CHUNKING ===\")\n",
    "print(f\"Created {len(smart_chunks)} chunks\\n\")\n",
    "\n",
    "for i, chunk in enumerate(smart_chunks):\n",
    "    print(f\"--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:** Chunks respect sentence boundaries and maintain coherence.\n",
    "\n",
    "### Task 2C: Compare Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chunking_strategies(text):\n",
    "    \"\"\"Compare different chunking approaches\"\"\"\n",
    "\n",
    "    # Strategy 1: Fixed character\n",
    "    char_chunks = chunk_by_characters(text, chunk_size=300, overlap=50)\n",
    "\n",
    "    # Strategy 2: Smart recursive\n",
    "    smart_chunks = chunk_smartly(text, chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "    print(\"\\n=== CHUNKING STRATEGY COMPARISON ===\")\n",
    "    print(f\"\\nFixed Character Chunking:\")\n",
    "    print(f\"  Chunks created: {len(char_chunks)}\")\n",
    "    print(f\"  Avg chunk size: {sum(len(c) for c in char_chunks) / len(char_chunks):.1f}\")\n",
    "    print(f\"  Sample split: '{char_chunks[0][-30:]}'\")\n",
    "    print(f\"                '{char_chunks[1][:30]}'\")\n",
    "\n",
    "    print(f\"\\nSmart Recursive Chunking:\")\n",
    "    print(f\"  Chunks created: {len(smart_chunks)}\")\n",
    "    print(f\"  Avg chunk size: {sum(len(c) for c in smart_chunks) / len(smart_chunks):.1f}\")\n",
    "    print(f\"  Sample split: '{smart_chunks[0][-30:]}'\")\n",
    "    print(f\"                '{smart_chunks[1][:30]}'\")\n",
    "\n",
    "    # Check if splits respect sentences\n",
    "    char_breaks_sentence = not char_chunks[0].endswith(('.', '!', '?'))\n",
    "    smart_breaks_sentence = not smart_chunks[0].endswith(('.', '!', '?'))\n",
    "\n",
    "    print(f\"\\nBreaks mid-sentence?\")\n",
    "    print(f\"  Fixed: {char_breaks_sentence}\")\n",
    "    print(f\"  Smart: {smart_breaks_sentence}\")\n",
    "\n",
    "compare_chunking_strategies(document_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which strategy produces better chunks? Why?\n",
    "\n",
    "‚úÖ **Checkpoint**: You should see that smart chunking respects sentence boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Generate Embeddings (20 min)\n",
    "\n",
    "**Objective:** Convert text chunks into embeddings using HuggingFace models.\n",
    "\n",
    "### Task 3A: Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model (this will download ~80MB on first run)\n",
    "print(\"Loading embedding model...\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"‚úì Loaded: {model_name}\")\n",
    "print(f\"  Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3B: Generate Embeddings for Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks, model):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text chunks\n",
    "\n",
    "    Args:\n",
    "        chunks: List of text strings\n",
    "        model: SentenceTransformer model\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Embeddings (num_chunks √ó embedding_dim)\n",
    "    \"\"\"\n",
    "    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    print(f\"‚úì Generated {len(embeddings)} embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for our smart chunks\n",
    "embeddings = generate_embeddings(smart_chunks, embedding_model)\n",
    "\n",
    "print(f\"\\nEmbedding shape: {embeddings.shape}\")\n",
    "print(f\"  {embeddings.shape[0]} chunks\")\n",
    "print(f\"  {embeddings.shape[1]} dimensions each\")\n",
    "print(f\"\\nFirst embedding (first 10 values):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C: Visualize Embedding Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embedding_stats(embeddings):\n",
    "    \"\"\"Print statistics about embeddings\"\"\"\n",
    "\n",
    "    embeddings_array = np.array(embeddings)\n",
    "\n",
    "    print(\"\\n=== EMBEDDING STATISTICS ===\")\n",
    "    print(f\"Shape: {embeddings_array.shape}\")\n",
    "    print(f\"  {embeddings_array.shape[0]} vectors\")\n",
    "    print(f\"  {embeddings_array.shape[1]} dimensions\")\n",
    "    print(f\"\\nValue ranges:\")\n",
    "    print(f\"  Min: {embeddings_array.min():.4f}\")\n",
    "    print(f\"  Max: {embeddings_array.max():.4f}\")\n",
    "    print(f\"  Mean: {embeddings_array.mean():.4f}\")\n",
    "    print(f\"  Std Dev: {embeddings_array.std():.4f}\")\n",
    "\n",
    "    # Check if normalized (unit length)\n",
    "    norms = np.linalg.norm(embeddings_array, axis=1)\n",
    "    print(f\"\\nVector norms (should be ~1.0 if normalized):\")\n",
    "    print(f\"  Min norm: {norms.min():.4f}\")\n",
    "    print(f\"  Max norm: {norms.max():.4f}\")\n",
    "    print(f\"  Mean norm: {norms.mean():.4f}\")\n",
    "\n",
    "visualize_embedding_stats(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3D: Visualize Individual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_embedding_distribution(embedding, chunk_text, max_chars=50):\n",
    "    \"\"\"Visualize a single embedding as a histogram\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(embedding, bins=50, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Embedding Value Distribution\\n\"{chunk_text[:max_chars]}...\"')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(embedding, linewidth=0.5)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Embedding Values (384 dimensions)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first embedding\n",
    "plot_embedding_distribution(embeddings[0], smart_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: You should have embeddings with 384 dimensions, values roughly between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Semantic Similarity (15 min)\n",
    "\n",
    "**Objective:** Understand how embeddings capture semantic meaning.\n",
    "\n",
    "### Task 4A: Compare Chunk Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_chunks(embeddings, chunks, idx1, idx2):\n",
    "    \"\"\"\n",
    "    Calculate and display similarity between two chunks\n",
    "\n",
    "    Args:\n",
    "        embeddings: Array of embeddings\n",
    "        chunks: List of text chunks\n",
    "        idx1, idx2: Indices of chunks to compare\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score\n",
    "    \"\"\"\n",
    "    similarity = cosine_similarity(\n",
    "        [embeddings[idx1]],\n",
    "        [embeddings[idx2]]\n",
    "    )[0][0]\n",
    "\n",
    "    print(f\"\\n=== COMPARING CHUNKS {idx1} and {idx2} ===\")\n",
    "    print(f\"\\nChunk {idx1} ({len(chunks[idx1])} chars):\")\n",
    "    print(f'\"{chunks[idx1]}\"')\n",
    "    print(f\"\\nChunk {idx2} ({len(chunks[idx2])} chars):\")\n",
    "    print(f'\"{chunks[idx2]}\"')\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "\n",
    "    # Interpret the score\n",
    "    if similarity > 0.9:\n",
    "        interpretation = \"Extremely similar (nearly identical)\"\n",
    "    elif similarity > 0.8:\n",
    "        interpretation = \"Very similar\"\n",
    "    elif similarity > 0.7:\n",
    "        interpretation = \"Similar\"\n",
    "    elif similarity > 0.5:\n",
    "        interpretation = \"Somewhat related\"\n",
    "    else:\n",
    "        interpretation = \"Not very related\"\n",
    "\n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    print('='*50)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# Compare adjacent chunks (should be similar due to overlap)\n",
    "similarity_adjacent = compare_chunks(embeddings, smart_chunks, 0, 1)\n",
    "\n",
    "# Compare distant chunks (should be less similar)\n",
    "similarity_distant = compare_chunks(\n",
    "    embeddings,\n",
    "    smart_chunks,\n",
    "    0,\n",
    "    len(smart_chunks) - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4B: Create Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Create a matrix showing similarity between all chunks\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Similarity matrix\n",
    "    \"\"\"\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def visualize_similarity_matrix(similarity_matrix):\n",
    "    \"\"\"Visualize the similarity matrix as a heatmap\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.xlabel('Chunk Index')\n",
    "    plt.ylabel('Chunk Index')\n",
    "    plt.title('Chunk Similarity Matrix\\n(Lighter = More Similar)')\n",
    "\n",
    "    # Add value annotations for small matrices\n",
    "    if len(similarity_matrix) <= 10:\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(len(similarity_matrix)):\n",
    "                plt.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                        ha='center', va='center',\n",
    "                        color='black' if similarity_matrix[i, j] < 0.7 else 'white',\n",
    "                        fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create and visualize similarity matrix\n",
    "sim_matrix = create_similarity_matrix(embeddings)\n",
    "print(f\"\\nSimilarity Matrix Shape: {sim_matrix.shape}\")\n",
    "visualize_similarity_matrix(sim_matrix)\n",
    "\n",
    "# Find most similar chunk pairs\n",
    "print(\"\\n=== MOST SIMILAR CHUNK PAIRS ===\")\n",
    "for i in range(len(sim_matrix)):\n",
    "    for j in range(i + 1, len(sim_matrix)):\n",
    "        if sim_matrix[i, j] > 0.8:  # High similarity threshold\n",
    "            print(f\"Chunks {i} and {j}: {sim_matrix[i, j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4C: Test Semantic Search (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_chunks(query, chunks, embeddings, model, top_k=3):\n",
    "    \"\"\"\n",
    "    Find chunks most similar to a query (manual semantic search)\n",
    "\n",
    "    Args:\n",
    "        query: Search query text\n",
    "        chunks: List of text chunks\n",
    "        embeddings: Pre-computed chunk embeddings\n",
    "        model: SentenceTransformer model\n",
    "        top_k: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        list: Top-k most similar chunks with scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Calculate similarity with all chunks\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk_index': idx,\n",
    "            'similarity': similarities[idx],\n",
    "            'text': chunks[idx]\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "test_queries = [\n",
    "    \"What is deep learning?\",\n",
    "    \"Tell me about computer vision\",\n",
    "    \"What are AI ethics concerns?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print('='*60)\n",
    "\n",
    "    results = find_similar_chunks(\n",
    "        query,\n",
    "        smart_chunks,\n",
    "        embeddings,\n",
    "        embedding_model,\n",
    "        top_k=2\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i} (Similarity: {result['similarity']:.4f}):\")\n",
    "        print(f\"Chunk {result['chunk_index']}: {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:** The most similar chunks should actually relate to the query, even if they don't contain the exact words!\n",
    "\n",
    "‚úÖ **Checkpoint**: Semantic search should retrieve relevant chunks based on meaning, not just keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Vector Database with ChromaDB (20 min)\n",
    "\n",
    "**Objective:** Store embeddings in a persistent vector database.\n",
    "\n",
    "### Task 5A: Initialize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import chromadb\n\ndef initialize_chromadb(persist_directory=\"./chroma_db\"):\n    \"\"\"\n    Initialize ChromaDB with persistent storage\n\n    Args:\n        persist_directory: Where to save the database\n\n    Returns:\n        tuple: (client, collection)\n    \"\"\"\n    # Create persistent client\n    client = chromadb.PersistentClient(path=persist_directory)\n\n    # Delete existing collection if it exists (for fresh start)\n    try:\n        client.delete_collection(\"lab3_documents\")\n        print(\"Deleted existing collection\")\n    except:\n        pass\n\n    # Create new collection\n    collection = client.create_collection(\n        name=\"lab3_documents\",\n        metadata={\"description\": \"Lab 3 - Document Processing & Embeddings\"}\n    )\n\n    print(f\"‚úì Initialized ChromaDB at '{persist_directory}'\")\n    print(f\"‚úì Created collection: 'lab3_documents'\")\n\n    return client, collection\n\n# Initialize the database\nchroma_client, collection = initialize_chromadb()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5B: Add Documents to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_chroma(collection, chunks, embeddings, source_file=\"sample_document.txt\"):\n",
    "    \"\"\"\n",
    "    Add chunks and embeddings to ChromaDB collection\n",
    "\n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        chunks: List of text chunks\n",
    "        embeddings: Numpy array of embeddings\n",
    "        source_file: Source filename for metadata\n",
    "    \"\"\"\n",
    "    # Convert numpy embeddings to list\n",
    "    embeddings_list = embeddings.tolist()\n",
    "\n",
    "    # Create unique IDs\n",
    "    ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "    # Create metadata for each chunk\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"chunk_index\": i,\n",
    "            \"source\": source_file,\n",
    "            \"chunk_size\": len(chunk),\n",
    "            \"word_count\": len(chunk.split())\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings_list,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úì Added {len(chunks)} documents to ChromaDB\")\n",
    "    print(f\"  Collection now contains: {collection.count()} items\")\n",
    "\n",
    "# Add our documents\n",
    "add_documents_to_chroma(collection, smart_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5C: Verify Storage and Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_chroma_storage(collection, chunk_id=\"chunk_0\"):\n",
    "    \"\"\"Verify that data was stored correctly\"\"\"\n",
    "\n",
    "    # Get a specific chunk\n",
    "    result = collection.get(\n",
    "        ids=[chunk_id],\n",
    "        include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== VERIFYING STORAGE (chunk_id: {chunk_id}) ===\")\n",
    "    print(f\"\\nDocument text:\")\n",
    "    print(f'\"{result[\"documents\"][0]}\"')\n",
    "    print(f\"\\nMetadata:\")\n",
    "    for key, value in result[\"metadatas\"][0].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"\\nEmbedding (first 10 values):\")\n",
    "    print(result[\"embeddings\"][0][:10])\n",
    "    print(f\"  ... ({len(result['embeddings'][0])} values total)\")\n",
    "\n",
    "verify_chroma_storage(collection, \"chunk_0\")\n",
    "\n",
    "# Query the database\n",
    "def query_chroma(collection, query_text, n_results=3):\n",
    "    \"\"\"\n",
    "    Query ChromaDB using natural language\n",
    "\n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        query_text: Search query\n",
    "        n_results: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        dict: Query results\n",
    "    \"\"\"\n",
    "    # Generate embedding for query\n",
    "    query_embedding = embedding_model.encode([query_text])\n",
    "\n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test querying\n",
    "test_query = \"What is machine learning?\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"QUERY: {test_query}\")\n",
    "print('='*60)\n",
    "\n",
    "results = query_chroma(collection, test_query, n_results=3)\n",
    "\n",
    "for i in range(len(results['documents'][0])):\n",
    "    distance = results['distances'][0][i]\n",
    "    similarity = 1 - distance  # ChromaDB uses distance, convert to similarity\n",
    "\n",
    "    print(f\"\\nResult {i+1} (Similarity: {similarity:.4f}):\")\n",
    "    print(f\"Metadata: {results['metadatas'][0][i]}\")\n",
    "    print(f\"Text: {results['documents'][0][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5D: Persistence Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_persistence():\n",
    "    \"\"\"Test that ChromaDB persists data across sessions\"\"\"\n",
    "\n",
    "    print(\"\\n=== TESTING PERSISTENCE ===\")\n",
    "\n",
    "    # Create a new client (simulates restarting the program)\n",
    "    new_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "    # Get existing collection\n",
    "    reloaded_collection = new_client.get_collection(\"lab3_documents\")\n",
    "\n",
    "    print(f\"‚úì Reloaded collection: {reloaded_collection.name}\")\n",
    "    print(f\"  Count: {reloaded_collection.count()} documents\")\n",
    "\n",
    "    # Verify data is still there\n",
    "    result = reloaded_collection.get(ids=[\"chunk_0\"], include=[\"documents\"])\n",
    "    print(f\"  First chunk still accessible: {result['documents'][0][:50]}...\")\n",
    "\n",
    "    return reloaded_collection\n",
    "\n",
    "persisted_collection = test_persistence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Checkpoint**: ChromaDB should persist data to disk. You can close and reopen the collection without losing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Capstone Project: Complete Document Processing Pipeline (30 min)\n",
    "\n",
    "**Objective:** Build a complete, reusable document processing system.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Your system must:\n",
    "1. ‚úÖ Load documents from files\n",
    "2. ‚úÖ Chunk text intelligently\n",
    "3. ‚úÖ Generate embeddings\n",
    "4. ‚úÖ Store in ChromaDB\n",
    "5. ‚úÖ Support querying\n",
    "6. ‚úÖ Handle errors gracefully\n",
    "7. ‚úÖ Be reusable for different documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Document Processor Class\n",
    "\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Complete document processing pipeline for RAG systems\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        chroma_db_path='./chroma_db',\n",
    "        collection_name='documents'\n",
    "    ):\n",
    "        print(\"Initializing Document Processor...\")\n",
    "\n",
    "        # Load embedding model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"‚úì Loaded embedding model: {embedding_model_name}\")\n",
    "\n",
    "        # Initialize ChromaDB\n",
    "        self.chroma_client = chromadb.PersistentClient(path=chroma_db_path)\n",
    "\n",
    "        # Get or create collection\n",
    "        try:\n",
    "            self.collection = self.chroma_client.get_collection(collection_name)\n",
    "            print(f\"‚úì Loaded existing collection: {collection_name}\")\n",
    "        except:\n",
    "            self.collection = self.chroma_client.create_collection(collection_name)\n",
    "            print(f\"‚úì Created new collection: {collection_name}\")\n",
    "\n",
    "        # Text splitter configuration\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        print(f\"‚úì Document Processor ready!\")\n",
    "        print(f\"  Embedding dimension: {self.embedding_model.get_sentence_embedding_dimension()}\")\n",
    "        print(f\"  Current collection size: {self.collection.count()}\")\n",
    "\n",
    "    def load_text_file(self, file_path):\n",
    "        \"\"\"Load a text file\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def load_pdf_file(self, file_path):\n",
    "        \"\"\"Load a PDF file\"\"\"\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "    def load_document(self, file_path):\n",
    "        \"\"\"Load a document (auto-detects file type)\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if ext == '.pdf':\n",
    "            content = self.load_pdf_file(file_path)\n",
    "        elif ext in ['.txt', '.md']:\n",
    "            content = self.load_text_file(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "        print(f\"‚úì Loaded {file_path} ({len(content)} characters)\")\n",
    "        return content\n",
    "\n",
    "    def chunk_document(self, content):\n",
    "        \"\"\"Split document into chunks\"\"\"\n",
    "        chunks = self.text_splitter.split_text(content)\n",
    "        print(f\"‚úì Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "\n",
    "    def generate_embeddings(self, chunks):\n",
    "        \"\"\"Generate embeddings for chunks\"\"\"\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            chunks,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        print(f\"‚úì Generated {len(embeddings)} embeddings\")\n",
    "        return embeddings\n",
    "\n",
    "    def add_to_database(self, chunks, embeddings, source_file):\n",
    "        \"\"\"Add chunks to ChromaDB\"\"\"\n",
    "        current_count = self.collection.count()\n",
    "        ids = [f\"doc_{current_count + i}\" for i in range(len(chunks))]\n",
    "\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": source_file,\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_size\": len(chunk)\n",
    "            }\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            embeddings=embeddings.tolist(),\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "        print(f\"‚úì Added {len(chunks)} chunks to database\")\n",
    "        print(f\"  Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "    def process_document(self, file_path):\n",
    "        \"\"\"Complete pipeline: load ‚Üí chunk ‚Üí embed ‚Üí store\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING: {file_path}\")\n",
    "        print('='*60)\n",
    "\n",
    "        content = self.load_document(file_path)\n",
    "        chunks = self.chunk_document(content)\n",
    "        embeddings = self.generate_embeddings(chunks)\n",
    "        self.add_to_database(chunks, embeddings, file_path)\n",
    "\n",
    "        stats = {\n",
    "            'file': file_path,\n",
    "            'characters': len(content),\n",
    "            'chunks': len(chunks),\n",
    "            'embeddings_shape': embeddings.shape\n",
    "        }\n",
    "\n",
    "        print(f\"\\n‚úì PROCESSING COMPLETE!\")\n",
    "        print(f\"  {stats['chunks']} chunks stored in database\")\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def search(self, query, n_results=5):\n",
    "        \"\"\"Search for relevant chunks\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_and_display(self, query, n_results=3):\n",
    "        \"\"\"Search and display results in readable format\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SEARCH: {query}\")\n",
    "        print('='*60)\n",
    "\n",
    "        results = self.search(query, n_results)\n",
    "\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            distance = results['distances'][0][i]\n",
    "            similarity = 1 - distance\n",
    "\n",
    "            print(f\"\\nResult {i+1} (Similarity: {similarity:.4f})\")\n",
    "            print(f\"Source: {results['metadatas'][0][i]['source']}\")\n",
    "            print(f\"Chunk: {results['metadatas'][0][i]['chunk_index']}\")\n",
    "            print(f\"Text: {results['documents'][0][i]}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get database statistics\"\"\"\n",
    "        return {\n",
    "            'total_documents': self.collection.count(),\n",
    "            'collection_name': self.collection.name,\n",
    "            'embedding_dimension': self.embedding_model.get_sentence_embedding_dimension()\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ DocumentProcessor class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Complete Document Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = DocumentProcessor(\n",
    "    collection_name='lab3_complete',\n",
    "    chroma_db_path='./chroma_db_complete'\n",
    ")\n",
    "\n",
    "# Process sample document\n",
    "stats = processor.process_document('sample_document.txt')\n",
    "\n",
    "# Test searches\n",
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about AI ethics\",\n",
    "    \"How does computer vision work?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    processor.search_and_display(query, n_results=2)\n",
    "\n",
    "# Show stats\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATABASE STATISTICS\")\n",
    "print('='*60)\n",
    "stats = processor.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Tasks\n",
    "\n",
    "1. **Run the complete pipeline** and verify it works\n",
    "2. **Add error handling** for edge cases\n",
    "3. **Process multiple documents** (create another text file)\n",
    "4. **Add a method** to delete documents by source file\n",
    "5. **Add a method** to get all unique sources in the database\n",
    "\n",
    "‚úÖ **Checkpoint**: Your document processor should successfully load, chunk, embed, and store documents, then allow searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Extension Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Add support for DOCX files\n",
    "def load_docx_file(self, file_path):\n",
    "    \"\"\"Load a DOCX file\"\"\"\n",
    "    # Hint: pip install python-docx\n",
    "    pass\n",
    "\n",
    "# TODO 2: Add duplicate detection\n",
    "def is_duplicate(self, chunk):\n",
    "    \"\"\"Check if chunk already exists in database\"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO 3: Add batch processing\n",
    "def process_directory(self, directory_path):\n",
    "    \"\"\"Process all documents in a directory\"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO 4: Add re-ranking\n",
    "def rerank_results(self, query, results):\n",
    "    \"\"\"Re-rank results using a different model\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "After completing this lab, you should understand:\n",
    "\n",
    "‚úÖ **Document Loading** - How to load TXT, PDF, and other formats  \n",
    "‚úÖ **Text Chunking** - Why and how to split documents intelligently  \n",
    "‚úÖ **Embeddings** - Converting text to numerical vectors that capture meaning  \n",
    "‚úÖ **Vector Databases** - Storing and querying embeddings efficiently  \n",
    "‚úÖ **Semantic Search** - Finding relevant content by meaning, not keywords  \n",
    "‚úÖ **Complete Pipeline** - All steps from document to searchable database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Troubleshooting\n",
    "\n",
    "**Issue**: \"Model download is slow\"  \n",
    "**Solution**: First download is ~80MB and may take time. It's cached for future use.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: \"ChromaDB not persisting\"  \n",
    "**Solution**: Ensure you're using `PersistentClient`, not `Client`:\n",
    "```python\n",
    "# ‚úì Correct\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# ‚úó Wrong (in-memory only)\n",
    "client = chromadb.Client()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: \"Embeddings dimension mismatch\"  \n",
    "**Solution**: Ensure same model for embedding and querying\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: \"Out of memory with large documents\"  \n",
    "**Solution**: Process in batches of 100 chunks at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What's Next?\n",
    "\n",
    "You've completed Lab 3! You now have:\n",
    "- ‚úÖ Working document processing pipeline\n",
    "- ‚úÖ Vector database with semantic search\n",
    "- ‚úÖ Foundation for building RAG systems\n",
    "\n",
    "**Next Lab**: Lab 4 - Semantic Search & Retrieval  \n",
    "Learn advanced querying techniques and build a production-ready search system!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [Understanding Embeddings](https://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "---\n",
    "\n",
    "**Lab 3 Complete!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}