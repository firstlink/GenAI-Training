# Lab 3: Document Processing & Embeddings

## ğŸ“š Learning Material

**Duration:** 35 minutes
**Difficulty:** Beginner to Intermediate
**Prerequisites:** Lab 1 & Lab 2 completed

---

## ğŸ¯ Learning Objectives

By the end of this learning module, you will understand:
- âœ… Why we need to chunk documents for RAG systems
- âœ… Different text chunking strategies and their tradeoffs
- âœ… What embeddings are and how they work
- âœ… How to choose embedding models
- âœ… Vector databases and semantic similarity
- âœ… The complete document processing pipeline

---

## ğŸ“– Table of Contents

1. [Introduction: The RAG Pipeline](#1-introduction-the-rag-pipeline)
2. [Why Chunk Documents?](#2-why-chunk-documents)
3. [Text Chunking Strategies](#3-text-chunking-strategies)
4. [Understanding Embeddings](#4-understanding-embeddings)
5. [Embedding Models](#5-embedding-models)
6. [Vector Databases](#6-vector-databases)
7. [Semantic Similarity](#7-semantic-similarity)
8. [The Complete Pipeline](#8-the-complete-pipeline)
9. [Review & Key Takeaways](#9-review--key-takeaways)

---

## 1. Introduction: The RAG Pipeline

### What is RAG?

**RAG** = **Retrieval-Augmented Generation**

It's a technique to give LLMs access to external knowledge by:
1. **Storing** documents in a searchable format
2. **Retrieving** relevant content based on user questions
3. **Augmenting** the LLM prompt with that content
4. **Generating** answers using both the LLM's knowledge and your documents

### The Problem RAG Solves

```
âŒ WITHOUT RAG:
User: "What's our return policy for laptops?"
LLM: "I don't have information about your specific return policy."

âœ… WITH RAG:
User: "What's our return policy for laptops?"
System: [Retrieves company policy document]
LLM: "According to your policy, laptops can be returned within 30
     days if unopened, or 14 days if opened, with original packaging..."
```

### The RAG Pipeline (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  OFFLINE (Once):                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚Documents â”‚ â†’ â”‚  Chunk   â”‚ â†’ â”‚ Embeddingsâ”‚          â”‚
â”‚  â”‚          â”‚   â”‚   Text   â”‚   â”‚  + Store  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                       â†“                 â”‚
â”‚                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                                 â”‚  Vector  â”‚           â”‚
â”‚                                 â”‚    DB    â”‚           â”‚
â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                       â†‘                 â”‚
â”‚  ONLINE (Every Query):                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   User   â”‚ â†’ â”‚ Search   â”‚ â†’ â”‚ Generate â”‚          â”‚
â”‚  â”‚ Question â”‚   â”‚  Vector  â”‚   â”‚  Answer  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    DB    â”‚   â”‚with LLM  â”‚          â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Lab 3 Focus**: The OFFLINE part - processing documents and creating embeddings.

**â±ï¸ Duration so far:** 5 minutes

---

## 2. Why Chunk Documents?

### The Challenge

Imagine you have a 100-page company handbook. You can't send the entire document to the LLM because:

1. **Context Limits**: LLMs have maximum context windows (e.g., 4K, 16K, 128K tokens)
2. **Cost**: More tokens = more money
3. **Precision**: LLMs perform better with focused, relevant context
4. **Noise**: Irrelevant information can confuse the model

### The Solution: Chunking

**Chunking** = Breaking large documents into smaller, meaningful pieces.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ORIGINAL DOCUMENT (10,000 words)     â”‚
â”‚  "Our company was founded in 1995...        â”‚
â”‚   ...return policy states that...           â”‚
â”‚   ...warranty covers manufacturing..."      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ CHUNK
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHUNK 1: Company History (500 words)        â”‚
â”‚ CHUNK 2: Return Policy (300 words)          â”‚
â”‚ CHUNK 3: Warranty Information (400 words)   â”‚
â”‚ CHUNK 4: Contact Information (200 words)    â”‚
â”‚ ...                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits of Chunking

âœ… **Relevance**: Retrieve only the chunks that matter for the question
âœ… **Efficiency**: Send less data to the LLM
âœ… **Cost Savings**: Fewer tokens = lower API costs
âœ… **Better Answers**: Focused context = more accurate responses

### Real-World Example

```
Question: "What's your laptop return policy?"

Without Chunking:
- Send entire 100-page handbook to LLM
- Cost: ~$0.50 per query
- Risk: LLM might miss the relevant section

With Chunking:
- Retrieve only "Return Policy" chunk (300 words)
- Cost: ~$0.05 per query
- Result: LLM focuses on exactly what matters
```

**â±ï¸ Duration so far:** 10 minutes

---

## 3. Text Chunking Strategies

### Strategy 1: Fixed-Size Character Chunking

**Simple approach**: Split text every N characters.

```python
# Example: 200 characters per chunk
text = "Artificial Intelligence is transforming industries..."
chunks = [text[0:200], text[200:400], text[400:600], ...]
```

**Pros:**
âœ… Simple to implement
âœ… Predictable chunk sizes
âœ… Fast

**Cons:**
âŒ Might split mid-sentence
âŒ Might split mid-word
âŒ Loses semantic boundaries

**Example of Bad Split:**
```
Chunk 1: "Our return policy allows customers to return produc"
Chunk 2: "ts within 30 days of purchase with original packaging."
```

### Strategy 2: Sentence-Based Chunking

**Better approach**: Split on sentence boundaries.

```
Chunk 1: "Our return policy is customer-friendly. Items can
          be returned within 30 days."

Chunk 2: "To initiate a return, contact customer service.
          Refunds are processed within 5-7 business days."
```

**Pros:**
âœ… Maintains sentence integrity
âœ… More semantic coherence
âœ… Better for reading comprehension

**Cons:**
âŒ Variable chunk sizes
âŒ Sentences might be very long or very short

### Strategy 3: Paragraph-Based Chunking

**Even better**: Split on paragraph boundaries.

```
Chunk 1: [Entire paragraph about return policy]
Chunk 2: [Entire paragraph about shipping]
Chunk 3: [Entire paragraph about warranty]
```

**Pros:**
âœ… Maintains topical coherence
âœ… Natural semantic boundaries
âœ… Good for documents with clear structure

**Cons:**
âŒ Paragraphs can vary greatly in size
âŒ Might exceed optimal chunk size

### Strategy 4: Recursive Character Text Splitting (BEST)

**LangChain's approach**: Try to split on paragraphs, then sentences, then words.

```python
Separators (in order of preference):
1. "\n\n"    # Paragraph breaks (best)
2. "\n"      # Line breaks (good)
3. ". "      # Sentence breaks (okay)
4. " "       # Word breaks (acceptable)
5. ""        # Character breaks (last resort)
```

**How it works:**
```
1. Try to split on "\n\n" (paragraphs)
   - If chunk is still too big â†’ continue

2. Try to split on "\n" (lines)
   - If chunk is still too big â†’ continue

3. Try to split on ". " (sentences)
   - If chunk is still too big â†’ continue

4. Split on " " (words)
```

**Pros:**
âœ… Maintains semantic meaning
âœ… Respects document structure
âœ… Configurable chunk size
âœ… Industry standard

**This is what we'll use in the lab!**

### Chunk Overlap: The Secret Sauce

**Problem**: Information at chunk boundaries might get lost.

```
Chunk 1: "...the warranty covers parts and labor."
Chunk 2: "Warranty claims must be filed within 90 days..."
```

If someone asks "How long is the warranty coverage?", information is split across chunks.

**Solution**: Overlap chunks

```
Chunk 1: "...the warranty covers parts and labor.
          Warranty claims must be filed..."

Chunk 2: "Warranty claims must be filed within 90 days.
          To file a claim, contact support..."
```

**Typical Settings:**
- **Chunk Size**: 500-1000 characters (or 100-200 tokens)
- **Overlap**: 10-20% of chunk size (50-200 characters)

### Chunking Strategy Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Strategy       â”‚  Speed  â”‚  Quality    â”‚  Use Case    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Fixed-size       â”‚  â˜…â˜…â˜…â˜…â˜…  â”‚  â˜…â˜†â˜†â˜†â˜†      â”‚  Quick tests â”‚
â”‚ Sentence-based   â”‚  â˜…â˜…â˜…â˜…â˜†  â”‚  â˜…â˜…â˜…â˜†â˜†      â”‚  Simple docs â”‚
â”‚ Paragraph-based  â”‚  â˜…â˜…â˜…â˜†â˜†  â”‚  â˜…â˜…â˜…â˜…â˜†      â”‚  Structured  â”‚
â”‚ Recursive (Best) â”‚  â˜…â˜…â˜…â˜†â˜†  â”‚  â˜…â˜…â˜…â˜…â˜…      â”‚  Production  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**â±ï¸ Duration so far:** 18 minutes

---

## 4. Understanding Embeddings

### What Are Embeddings?

**Embeddings** = Numerical representations of text that capture semantic meaning.

Instead of storing text as words, we convert it to numbers (vectors) that represent its *meaning*.

```
Text: "I love pizza"
Embedding: [0.23, -0.45, 0.78, 0.12, -0.34, ...]
           (typically 384 to 1536 dimensions)
```

### Why Numbers?

Because computers can:
- **Compare** numbers (which texts are similar?)
- **Search** numbers efficiently (find relevant documents)
- **Calculate** distances (how related are two concepts?)

### Visual Intuition (2D Simplification)

Imagine text mapped to a 2D space:

```
                 Animals
                    â†‘
            cat â€¢   |   â€¢ dog
                    |
        pizza â€¢     |     â€¢ burger
                    |
    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
       Food         |        Sports
                    |
         â€¢ apple    |    â€¢ basketball
                    |
                    â†“
                 Health
```

**Key Insight**: Similar meanings are close together in vector space!

### Real Embeddings (384D Example)

In reality, embeddings have hundreds of dimensions:

```
"The cat sat on the mat"
â†’ [0.23, -0.45, 0.12, 0.89, -0.34, 0.56, ...]
   (384 numbers total)

"A feline rested on the rug"
â†’ [0.25, -0.43, 0.15, 0.87, -0.32, 0.54, ...]
   (Very similar numbers because similar meaning!)

"I love pizza"
â†’ [-0.78, 0.92, -0.45, 0.23, 0.67, -0.12, ...]
   (Very different numbers because different meaning)
```

### How Embeddings Capture Meaning

**Example 1: Synonyms**
```
"happy"     â†’ [0.5, 0.8, -0.2, ...]
"joyful"    â†’ [0.5, 0.8, -0.2, ...]  (very similar!)
"sad"       â†’ [-0.5, -0.7, 0.3, ...]  (very different!)
```

**Example 2: Context**
```
"bank" (financial institution)
â†’ [0.2, 0.5, -0.3, 0.1, ...]

"bank" (river bank)
â†’ [-0.3, 0.1, 0.4, -0.2, ...]
```

The embedding model uses context to determine meaning!

### Embeddings vs. Keywords

**Old Way (Keyword Search):**
```
User: "How do I get my money back?"
Keyword Search: Looks for documents with "money" and "back"
Misses: "refund policy" (no keywords match!)
```

**New Way (Semantic Search with Embeddings):**
```
User: "How do I get my money back?"
Embedding: [0.5, -0.3, 0.7, ...]

Document: "Our refund policy allows returns..."
Embedding: [0.5, -0.3, 0.7, ...]  (very similar!)

Match found! âœ“
```

### Properties of Good Embeddings

âœ… **Semantic Similarity**: Similar meanings â†’ similar vectors
âœ… **Dimensionality**: Enough dimensions to capture nuance (384-1536)
âœ… **Normalized**: Typically unit length for easy comparison
âœ… **Dense**: Every dimension contributes to meaning
âœ… **Context-Aware**: Same word in different contexts â†’ different embeddings

**â±ï¸ Duration so far:** 25 minutes

---

## 5. Embedding Models

### What is an Embedding Model?

A **neural network** trained to convert text into meaningful vectors.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        EMBEDDING MODEL                 â”‚
â”‚                                        â”‚
â”‚  Input: "I love pizza"                â”‚
â”‚         â†“                              â”‚
â”‚  [Neural Network Processing]          â”‚
â”‚         â†“                              â”‚
â”‚  Output: [0.23, -0.45, 0.78, ...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Popular Embedding Models

#### 1. **sentence-transformers/all-MiniLM-L6-v2**
```
Dimensions: 384
Speed: â˜…â˜…â˜…â˜…â˜… (Very Fast)
Quality: â˜…â˜…â˜…â˜…â˜† (Good)
Size: 80 MB
Use Case: Beginner-friendly, production-ready
```

**Best for:** Lab exercises, small to medium applications

#### 2. **sentence-transformers/all-mpnet-base-v2**
```
Dimensions: 768
Speed: â˜…â˜…â˜…â˜†â˜† (Medium)
Quality: â˜…â˜…â˜…â˜…â˜… (Excellent)
Size: 420 MB
Use Case: Higher quality requirements
```

**Best for:** Production systems where quality matters more than speed

#### 3. **text-embedding-3-small** (OpenAI)
```
Dimensions: 1536
Speed: â˜…â˜…â˜…â˜…â˜† (Fast via API)
Quality: â˜…â˜…â˜…â˜…â˜… (Excellent)
Cost: $0.02 per 1M tokens
Use Case: Enterprise applications
```

**Best for:** Production with budget for API calls

#### 4. **text-embedding-3-large** (OpenAI)
```
Dimensions: 3072
Speed: â˜…â˜…â˜…â˜†â˜† (Medium via API)
Quality: â˜…â˜…â˜…â˜…â˜… (State-of-the-art)
Cost: $0.13 per 1M tokens
Use Case: Highest quality needs
```

**Best for:** Maximum accuracy requirements

### Model Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                    â”‚ Dims  â”‚ Speed  â”‚ Quality  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ all-MiniLM-L6-v2        â”‚  384  â”‚ Fastestâ”‚ Good     â”‚
â”‚ all-mpnet-base-v2       â”‚  768  â”‚ Medium â”‚ Better   â”‚
â”‚ text-embedding-3-small  â”‚ 1536  â”‚ Fast*  â”‚ Best     â”‚
â”‚ text-embedding-3-large  â”‚ 3072  â”‚ Medium*â”‚ Best++   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* Requires API call
```

### Choosing the Right Model

**Start with:** `all-MiniLM-L6-v2`
- Free
- Fast
- Good enough for most use cases
- Easy to run locally
- **This is what we use in Lab 3!**

**Upgrade to:** `all-mpnet-base-v2`
- When you need better quality
- Still free and local
- 2x slower, 2x larger

**Upgrade to:** OpenAI embeddings
- When quality is critical
- When you're already using OpenAI for generation
- Budget for API costs

### Key Consideration: Consistency

âš ï¸ **IMPORTANT**: Use the **same embedding model** for:
1. Creating embeddings (offline)
2. Searching embeddings (online)

```
âŒ BAD:
Documents embedded with: all-MiniLM-L6-v2 (384D)
Query embedded with: all-mpnet-base-v2 (768D)
â†’ Dimension mismatch! Won't work!

âœ… GOOD:
Documents embedded with: all-MiniLM-L6-v2 (384D)
Query embedded with: all-MiniLM-L6-v2 (384D)
â†’ Perfect match! Works great!
```

**â±ï¸ Duration so far:** 30 minutes

---

## 6. Vector Databases

### What is a Vector Database?

A **specialized database** designed to:
1. Store high-dimensional vectors (embeddings)
2. Perform fast similarity searches
3. Handle millions of vectors efficiently

```
Traditional Database:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID   â”‚ Name     â”‚ Email  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1    â”‚ John     â”‚ j@...  â”‚
â”‚ 2    â”‚ Sarah    â”‚ s@...  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Vector Database:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID   â”‚ Text                 â”‚ Embedding (384D)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1    â”‚ "Return policy..."   â”‚ [0.2, -0.5, 0.7, ...]   â”‚
â”‚ 2    â”‚ "Shipping info..."   â”‚ [-0.3, 0.8, -0.1, ...]  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Not Use Regular Databases?

**Problem**: Finding similar vectors in regular databases is SLOW.

```
Finding similar text in PostgreSQL:
SELECT * FROM documents
WHERE CONTAINS(text, 'return policy');
â†’ Only finds exact keyword matches

Finding similar vectors in Vector DB:
query_vector = [0.2, -0.5, 0.7, ...]
results = db.search(query_vector, top_k=5)
â†’ Finds semantically similar documents (fast!)
```

### Popular Vector Databases

#### 1. **ChromaDB** (What we use in Lab 3)
```
Type: Embedded / Server
Setup: pip install chromadb
Speed: â˜…â˜…â˜…â˜…â˜…
Ease: â˜…â˜…â˜…â˜…â˜… (Easiest!)
Scale: Up to millions of vectors
Best for: Development, small-medium production
```

#### 2. **Pinecone**
```
Type: Cloud-only
Setup: API key required
Speed: â˜…â˜…â˜…â˜…â˜…
Ease: â˜…â˜…â˜…â˜…â˜†
Scale: Billions of vectors
Best for: Large-scale production
```

#### 3. **Weaviate**
```
Type: Self-hosted / Cloud
Setup: Docker required
Speed: â˜…â˜…â˜…â˜…â˜†
Ease: â˜…â˜…â˜…â˜†â˜†
Scale: Hundreds of millions
Best for: Enterprise, self-hosted
```

#### 4. **FAISS** (Facebook AI)
```
Type: Library (not a database)
Setup: pip install faiss-cpu
Speed: â˜…â˜…â˜…â˜…â˜…
Ease: â˜…â˜…â˜†â˜†â˜†
Scale: Billions of vectors
Best for: Research, advanced users
```

### ChromaDB: What We'll Use

**Why ChromaDB?**
âœ… **Easy**: Works out of the box, no setup
âœ… **Persistent**: Saves to disk automatically
âœ… **Fast**: Optimized for similarity search
âœ… **Metadata**: Can store metadata with each vector
âœ… **Free**: Open source, no API costs

**Basic ChromaDB Operations:**

```python
# 1. Create/connect
client = chromadb.PersistentClient(path="./my_db")
collection = client.get_or_create_collection("my_docs")

# 2. Add documents
collection.add(
    documents=["text chunk 1", "text chunk 2"],
    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],
    ids=["id1", "id2"],
    metadatas=[{"source": "doc1.pdf"}, {"source": "doc2.pdf"}]
)

# 3. Search (we'll learn this in Lab 4)
results = collection.query(
    query_embeddings=[[0.1, 0.2, ...]],
    n_results=5
)
```

**â±ï¸ Duration so far:** 33 minutes

---

## 7. Semantic Similarity

### How Do We Measure Similarity?

**Cosine Similarity** = The standard way to compare embeddings.

**Formula:**
```
similarity = (A Â· B) / (||A|| Ã— ||B||)

Where:
Â· = dot product
||A|| = magnitude of vector A
```

**Don't worry about the math!** Libraries calculate this for you.

### Cosine Similarity Range

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Similarity Score Interpretation       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1.0       â”‚ Identical (same text)     â”‚
â”‚  0.9-1.0   â”‚ Extremely similar         â”‚
â”‚  0.8-0.9   â”‚ Very similar              â”‚
â”‚  0.7-0.8   â”‚ Similar                   â”‚
â”‚  0.5-0.7   â”‚ Somewhat related          â”‚
â”‚  0.0-0.5   â”‚ Barely related            â”‚
â”‚  < 0.0     â”‚ Opposite meanings         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real Example

```python
from sklearn.metrics.pairwise import cosine_similarity

text1 = "I love programming in Python"
text2 = "Python is my favorite coding language"
text3 = "I enjoy eating pizza"

# After embedding:
embedding1 = [0.5, 0.8, -0.2, 0.3, ...]
embedding2 = [0.5, 0.8, -0.1, 0.3, ...]
embedding3 = [-0.3, 0.1, 0.7, -0.5, ...]

similarity_1_2 = cosine_similarity([embedding1], [embedding2])
# Result: 0.92 (very similar!)

similarity_1_3 = cosine_similarity([embedding1], [embedding3])
# Result: 0.15 (not related)
```

### Why Cosine Similarity?

**Advantages:**
âœ… Normalized (-1 to 1 range)
âœ… Works well for high dimensions
âœ… Fast to compute
âœ… Industry standard

**Alternatives:**
- **Euclidean Distance**: Measures straight-line distance
- **Dot Product**: Simple but not normalized
- **Manhattan Distance**: Sum of absolute differences

**For RAG systems, use Cosine Similarity** (it's the default in ChromaDB).

**â±ï¸ Duration so far:** 35 minutes

---

## 8. The Complete Pipeline

### End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Load Document                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ PDF / TXT /  â”‚ â†’ Read file into memory               â”‚
â”‚  â”‚ DOCX / HTML  â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Chunk Text                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ "Lorem ipsum dolor sit amet...       â”‚              â”‚
â”‚  â”‚  consectetur adipiscing elit..."     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                    â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Chunk 1  â”‚  â”‚ Chunk 2  â”‚  â”‚ Chunk 3  â”‚            â”‚
â”‚  â”‚ (500 chr)â”‚  â”‚ (500 chr)â”‚  â”‚ (500 chr)â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Generate Embeddings                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚    Embedding Model (384D)            â”‚              â”‚
â”‚  â”‚    all-MiniLM-L6-v2                  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                    â†“                                    â”‚
â”‚  Chunk 1 â†’ [0.2, -0.5, 0.7, 0.1, ...]                  â”‚
â”‚  Chunk 2 â†’ [-0.3, 0.8, -0.1, 0.4, ...]                 â”‚
â”‚  Chunk 3 â†’ [0.6, -0.2, 0.9, -0.3, ...]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Store in Vector Database                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚        ChromaDB Collection          â”‚               â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚  â”‚ ID   â”‚ Text         â”‚ Embedding    â”‚               â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚  â”‚ ch_1 â”‚ "Lorem..."   â”‚ [0.2, -0.5...]â”‚              â”‚
â”‚  â”‚ ch_2 â”‚ "Ipsum..."   â”‚ [-0.3, 0.8...]â”‚              â”‚
â”‚  â”‚ ch_3 â”‚ "Dolor..."   â”‚ [0.6, -0.2...]â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Query (Lab 4)                                  â”‚
â”‚  User Question â†’ Embedding â†’ Search â†’ Retrieve Chunks   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code Example (Complete Pipeline)

```python
# Complete document processing pipeline

from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb

# STEP 1: Load document
with open('document.txt', 'r') as f:
    document = f.read()

# STEP 2: Chunk text
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""]
)
chunks = splitter.split_text(document)

# STEP 3: Generate embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(chunks)

# STEP 4: Store in vector database
client = chromadb.PersistentClient(path="./db")
collection = client.get_or_create_collection("docs")

collection.add(
    documents=chunks,
    embeddings=embeddings.tolist(),
    ids=[f"chunk_{i}" for i in range(len(chunks))],
    metadatas=[{"index": i} for i in range(len(chunks))]
)

print(f"âœ“ Processed {len(chunks)} chunks and stored in ChromaDB")
```

**That's it!** In just ~15 lines of code, you have a working document processing pipeline.

---

## 9. Review & Key Takeaways

### ğŸ¯ What You Learned

âœ… **RAG Pipeline**: Retrieval-Augmented Generation gives LLMs external knowledge
âœ… **Chunking**: Breaking documents into smaller pieces for better retrieval
âœ… **Embeddings**: Numerical representations that capture semantic meaning
âœ… **Vector Databases**: Specialized storage for fast similarity search
âœ… **Similarity**: Cosine similarity measures how related chunks are

### ğŸ’¡ Key Concepts

**1. Why Chunk?**
- LLMs have context limits
- Focused context = better answers
- Lower costs

**2. Best Chunking Strategy**
- Recursive Character Text Splitter
- Chunk size: 500-1000 characters
- Overlap: 10-20%

**3. Embeddings Capture Meaning**
- Similar meaning â†’ similar vectors
- "refund" and "money back" will be close in vector space
- Enables semantic search (not just keyword matching)

**4. Vector Databases Are Essential**
- Regular databases can't efficiently search vectors
- ChromaDB makes it easy
- Stores documents, embeddings, and metadata

**5. The Complete Pipeline**
```
Load â†’ Chunk â†’ Embed â†’ Store â†’ Query (next lab!)
```

### ğŸ§  Knowledge Check

<details>
<summary><strong>Question 1:</strong> Why do we chunk documents instead of storing them whole?</summary>

**Answer:**
1. LLMs have context window limits
2. Focused context produces better answers
3. Reduces token costs
4. Improves retrieval precision
</details>

<details>
<summary><strong>Question 2:</strong> What is an embedding?</summary>

**Answer:**
A numerical (vector) representation of text that captures semantic meaning. Similar meanings produce similar vectors, enabling semantic search.
</details>

<details>
<summary><strong>Question 3:</strong> What's the difference between keyword search and semantic search?</summary>

**Answer:**
- **Keyword search**: Matches exact words ("refund" won't match "money back")
- **Semantic search**: Matches meaning (finds "refund policy" when you ask about "getting money back")
</details>

<details>
<summary><strong>Question 4:</strong> What is cosine similarity and what does a score of 0.9 mean?</summary>

**Answer:**
Cosine similarity measures how similar two embeddings are, ranging from -1 to 1. A score of 0.9 means the texts are extremely similar in meaning.
</details>

<details>
<summary><strong>Question 5:</strong> Why must you use the same embedding model for documents and queries?</summary>

**Answer:**
Different models produce embeddings with different dimensions and meanings. Using different models would be like trying to compare temperatures in Celsius and Fahrenheit without conversion - the numbers won't match up correctly.
</details>

### ğŸš€ Ready for Hands-On Practice?

You now understand:
- âœ… The theory behind document processing
- âœ… Why and how to chunk text
- âœ… What embeddings are and how they work
- âœ… How vector databases enable semantic search

**Next step**: [Hands-On Lab â†’](lab.md)

In the lab, you'll:
1. Load and chunk real documents
2. Generate embeddings using HuggingFace models
3. Store them in ChromaDB
4. Visualize and compare embeddings
5. Build the complete document processing pipeline

---

### ğŸ“š Additional Resources

**Want to dive deeper?**
- [Sentence Transformers Documentation](https://www.sbert.net/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [Understanding Embeddings (Visual)](https://jalammar.github.io/illustrated-word2vec/)

---

**Learning Material Complete!** âœ…
[â† Back to README](../README.md) | [Start Hands-On Lab â†’](lab.md)
